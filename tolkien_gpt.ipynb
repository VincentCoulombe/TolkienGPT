{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprendre l'architecture Transformer avec TolkienGPT 📚\n",
    "\n",
    "Bienvenue 😀 \n",
    "\n",
    "Ce notebook n'a qu'un seul objectif. VOUS apprendre quelque chose de nouveau. Plus particulièrement:  \n",
    "\n",
    "- Comprendre l'architecture Transformer\n",
    "- Implémenter un Transformer decoder (l'algorithme derrière ChatGPT) avec seulement Pytorch\n",
    "- Entraîner un Transformer decoder à générer du texte\n",
    "\n",
    "## About TolkienGPT 🧙‍♂️\n",
    "\n",
    "TolkienGPT est un modèle de langue qu'on va coder et entraîner dans le but d'imiter l'écriture de mon auteur préféré, J.R.R. Tolkien (Le seigneur des anneaux, Le hobbit, Le silmarillon...). Ce modèle est basé sur l'architecture Transformer introduite dans la publication 📄 [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf). Aussi, la pédagogie 📖 est inspirée du [tutoriel par Andrej Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY) de OpenAI 🦾. \n",
    "\n",
    "## Les prérequis 🔍\n",
    "\n",
    "Pour être à l'aise, une compréhension de Python, Pytorch et des concepts de Deep Learning sont nécessaires. Je fais de mon mieux pour vulgariser, mais je ne peux pas faire un vidéo de 3 heures non plus.\n",
    "\n",
    "\n",
    "Si vous êtes prêts, je vos propose de commencer notre aventure vers les secrets du Transformer Decoder 📜 et la magie de TolkienGPT 🪄!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger les données 🚚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import re\n",
    "import os\n",
    "\n",
    "pdf_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"Tolkien-J.-The-lord-of-the-rings-HarperCollins-ebooks-2010.pdf\")\n",
    "pdf_reader = PdfReader(pdf_path)\n",
    "lotr = \"\"\n",
    "for i, page in enumerate(pdf_reader.pages):\n",
    "    if i >= 24 and i < 1163:\n",
    "        page_str = \"\".join(page.extract_text())\n",
    "        nlp_page = nlp(page_str)\n",
    "        for sentence in nlp_page.sents:\n",
    "            sentence_text = sentence.text.replace(\"\\n\", \" \").strip()\n",
    "            if re.search(r\"^[A-Z]\", sentence_text):\n",
    "                # print(sentence_text)\n",
    "                # print(\"-----\")\n",
    "                lotr += f\"{sentence_text.lower()}\\n\"\n",
    "  \n",
    "\n",
    "replacements = {\"-\", \"  \"}\n",
    "pattern = \"|\".join(map(re.escape, replacements))\n",
    "lotr = re.sub(pattern, \" \", lotr)       \n",
    "lotr = re.sub(r\"ﬁ\", \"fi\", lotr) \n",
    "lotr = re.sub(r\"ﬂ\", \"fl\", lotr) \n",
    "lotr = re.sub(r\"ﬀ\", \"ff\", lotr)\n",
    "lotr = re.sub(r\"æ\", \"ae\", lotr)\n",
    "lotr = re.sub(r\"’\", \"'\", lotr)\n",
    "with open(\"lotr.txt\", \"w\") as f:\n",
    "    f.write(lotr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorer les données 🕵️‍♂️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 2191110 lettres dans le livre\n",
      "Il y a 417877 mots dans le livre\n",
      "Il y a 26276 phrases dans le livre\n"
     ]
    }
   ],
   "source": [
    "print(f\"Il y a {len(lotr)} lettres dans le livre\")\n",
    "print(f\"Il y a {len(lotr.split())} mots dans le livre\")\n",
    "print(f\"Il y a {len(lotr.splitlines())} phrases dans le livre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici la première phrase du livre:\n",
      " prologue  1 concerning hobbits  this book is largely concerned with hobbits, and from its pages a reader may discover much of their character and a little of their  history.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Voici la première phrase du livre:\\n {lotr.splitlines()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer les données 🏭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spoiler Alert ⚠️ les ordinateurs ne travaillent jamais directement sur les lettres (ils ne savent pas c'est quoi...). Donc, absolument toutes les applications encode les textes d'une manière ou d'une autre.\n",
    "\n",
    "En Deep Learning, on les tokenize. C'est-à-dire qu'on donne un ID 🪪 soit à chaque lettre, à chaque bout de mot ou bien directement à chaque mot. \n",
    "\n",
    "Ici, on va tokenizer les mots directement (ce n'est pas un tutoriel sur la tokenization), mais les vrais modèles de langue tokenize sur des bouts de mots avec des algorithmes comme [SentencePiece](https://github.com/google/sentencepiece) de Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le mot 'gandalf' est le 1270ème token du corpus\n",
      "\n",
      "Voici une phrase aléatoire : \n",
      " great orcs, who also bore the white hand of isengard: that kind is stronger and more fell than all others.\n",
      "\n",
      "Voici les tokens de cette phrase : \n",
      " [115, 762, 22, 607, 63, 2303, 67, 769, 178, 55, 6672, 23, 71, 233, 42, 3286, 46, 125, 478, 153, 114, 511, 14]\n",
      "\n",
      "Voici la phrase reconstruite : \n",
      " great orcs , who also bore the white hand of isengard : that kind is stronger and more fell than all others .\n",
      "\n",
      "Le seigneur des anneaux contient 489427 tokens\n",
      "\n",
      "Le seigneur des anneaux contient 13451 tokens différents\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class lotrWordTokenizer:\n",
    "    def __init__(self, lotr):\n",
    "        self.special_chars = re.findall(r\"[^a-z\\s]\", \"\".join(set(lotr)))\n",
    "        self.unique_words = [] + self.special_chars\n",
    "        self.word2idx = {w: i for i, w in enumerate(self.unique_words)}\n",
    "        self.idx2word = {i: w for i, w in enumerate(self.unique_words)}\n",
    "        self.pattern = '|'.join(re.escape(special_char) for special_char in self.special_chars) + r\"|\\s\"\n",
    "\n",
    "    def tokenize_a_sentence(self, sentence):\n",
    "        matches = {match.start(): match.group() for match in re.finditer(self.pattern, sentence)}\n",
    "        matches\n",
    "        running_word = \"\"\n",
    "        tokens = []\n",
    "        for i, char in enumerate(sentence):\n",
    "            match = matches.get(i)\n",
    "            if match is not None:\n",
    "                if running_word != \"\":\n",
    "                    running_word = self.process_running_word(running_word, tokens)\n",
    "                if match != \" \":\n",
    "                    tokens.append(self.word2idx[match])\n",
    "            else:\n",
    "                running_word += char\n",
    "\n",
    "        if running_word != \"\":\n",
    "            running_word = self.process_running_word(running_word, tokens)\n",
    "        return tokens\n",
    "    \n",
    "    def process_running_word(self, running_word, tokens):\n",
    "        if running_word not in self.unique_words:\n",
    "            self.add_to_lexicon(running_word)\n",
    "        tokens.append(self.word2idx[running_word])\n",
    "        return \"\"\n",
    "\n",
    "    def add_to_lexicon(self, word):\n",
    "        self.unique_words.append(word)\n",
    "        self.word2idx[word] = len(self.unique_words) - 1\n",
    "        self.idx2word[len(self.unique_words) - 1] = word\n",
    "        \n",
    "tokenizer = lotrWordTokenizer(lotr)\n",
    "tokenized_lotr = []\n",
    "for sentence in lotr.splitlines():\n",
    "    tokenized_lotr.extend(tokenizer.tokenize_a_sentence(sentence))\n",
    "    \n",
    "    \n",
    "print(f\"Le mot 'gandalf' est le {tokenizer.word2idx['gandalf']}ème token du corpus\\n\")\n",
    "random_sentence = \" \".join(lotr.splitlines()[9860:9861])\n",
    "print(f\"Voici une phrase aléatoire : \\n {random_sentence}\")\n",
    "tokens = tokenizer.tokenize_a_sentence(random_sentence)\n",
    "print(f\"\\nVoici les tokens de cette phrase : \\n {tokens}\")\n",
    "print(f\"\\nVoici la phrase reconstruite : \\n {' '.join([tokenizer.idx2word[token] for token in tokens])}\\n\")\n",
    "\n",
    "tokenized_lotr = torch.tensor(tokenized_lotr)\n",
    "print(f\"Le seigneur des anneaux contient {len(tokenized_lotr)} tokens\")\n",
    "\n",
    "print(f\"\\nLe seigneur des anneaux contient {len(tokenizer.unique_words)} tokens différents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_words = \"\"\n",
    "for word in tokenizer.unique_words:\n",
    "    lotr_words += f\"{word}\\n\"\n",
    "    \n",
    "with open(\"lotr_words.txt\", \"w\") as f:\n",
    "    f.write(lotr_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Important** ⚠️\n",
    "\n",
    "On  un modèle qui écrit du Tolkien. C'est-à-dire qui est capable d'aligner des mots, un après l'autre, comme Tolkien. Donc, il nous faut un modèle qui est capable de prédire le prochain mot à écrire en fonction de ce qu'il a déjà écrit.\n",
    "\n",
    "Voici ce que je veux dire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TolkienGPT devra apprendre à écrire le mot 'great' dans un contexte où il a déjà écrit : \n",
      "\n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'orcs,' dans un contexte où il a déjà écrit : \n",
      "great \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'who' dans un contexte où il a déjà écrit : \n",
      "great orcs, \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'also' dans un contexte où il a déjà écrit : \n",
      "great orcs, who \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'bore' dans un contexte où il a déjà écrit : \n",
      "great orcs, who also \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'the' dans un contexte où il a déjà écrit : \n",
      "great orcs, who also bore \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'white' dans un contexte où il a déjà écrit : \n",
      "great orcs, who also bore the \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'hand' dans un contexte où il a déjà écrit : \n",
      "great orcs, who also bore the white \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'of' dans un contexte où il a déjà écrit : \n",
      "great orcs, who also bore the white hand \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'isengard:' dans un contexte où il a déjà écrit : \n",
      "great orcs, who also bore the white hand of \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'that' dans un contexte où il a déjà écrit : \n",
      "great orcs, who also bore the white hand of isengard: \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'kind' dans un contexte où il a déjà écrit : \n",
      "great orcs, who also bore the white hand of isengard: that \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'is' dans un contexte où il a déjà écrit : \n",
      "great orcs, who also bore the white hand of isengard: that kind \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'stronger' dans un contexte où il a déjà écrit : \n",
      "great orcs, who also bore the white hand of isengard: that kind is \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'and' dans un contexte où il a déjà écrit : \n",
      "great orcs, who also bore the white hand of isengard: that kind is stronger \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'more' dans un contexte où il a déjà écrit : \n",
      "great orcs, who also bore the white hand of isengard: that kind is stronger and \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'fell' dans un contexte où il a déjà écrit : \n",
      "great orcs, who also bore the white hand of isengard: that kind is stronger and more \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'than' dans un contexte où il a déjà écrit : \n",
      "great orcs, who also bore the white hand of isengard: that kind is stronger and more fell \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'all' dans un contexte où il a déjà écrit : \n",
      "great orcs, who also bore the white hand of isengard: that kind is stronger and more fell than \n",
      "\n",
      "TolkienGPT devra apprendre à écrire le mot 'others.' dans un contexte où il a déjà écrit : \n",
      "great orcs, who also bore the white hand of isengard: that kind is stronger and more fell than all \n",
      "\n"
     ]
    }
   ],
   "source": [
    "wrote_words = \"\"\n",
    "for word in random_sentence.split():\n",
    "    print(f\"TolkienGPT devra apprendre à écrire le mot '{word}' dans un contexte où il a déjà écrit : \\n{wrote_words}\\n\")\n",
    "    wrote_words += f\"{word} \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, comment procéder dans un contexte matriciel 🤔?\n",
    "\n",
    "Je propose d'utiliser la méthode suivante:\n",
    "- Définir jusqu'à combien de mots en arrière ça vaut la peine de regarder pour écrire le prochain (context_length) \n",
    "- Piger aléatoirement un mot dans le text (idx)\n",
    "- Tronquer les (context_length) mots suivants (idx)\n",
    "- Garder aussi en mémoire le (context_length + 1)e mot après (idx).\n",
    "- Apprendre au modèle à prédire chacunes des combinaisons comprises dans l'exemple d'en haut ⬆️:\n",
    "    - Le (idx + 1)e mot est celui qui devrait suivre le (idx)e mot\n",
    "    - Le (idx + 2)e mot est celui qui devrait suivre les (idx)e et (idx + 1)e mot\n",
    "    - ...\n",
    "    - Le (idx + context_length + 1)e mot est celui qui devrait suivre tous les mots du contexte\n",
    "\n",
    "Bref, ont initialise deux matrices. L'une contient le contexte et l'autre contient les mots à écrire. Soit:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici la matrice des inputs : \n",
      " ['all', 'that', 'was', 'necessary', ')']\n",
      "Voici la matrice des labels : \n",
      " ['that', 'was', 'necessary', ')', ',']\n"
     ]
    }
   ],
   "source": [
    "context_length = 5\n",
    "idx = 12346\n",
    "inputs = [tokenizer.idx2word[tokenized_lotr[i].item()] for i in range(idx, idx + context_length)]\n",
    "print(f\"Voici la matrice des inputs : \\n {inputs}\")\n",
    "\n",
    "labels = [tokenizer.idx2word[tokenized_lotr[i].item()] for i in range(idx + 1, idx + context_length + 1)]\n",
    "print(f\"Voici la matrice des labels : \\n {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On créer un [Dataset Pytorch](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) pour pouvoir facilement loader les phrases ainsi que le prochain mot à prédire.\n",
    "\n",
    "⚠️ **Important** ⚠️\n",
    "\n",
    "Les batches sont traitées de manière **indépendantes**. Elles sont utiles parce que le GPU peut les traitées en parrallèle (elles accélère l'entrainement). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LotrDataset(Dataset):\n",
    "    def __init__(self, data, context_length, length):\n",
    "        self.data = data\n",
    "        self.context_length = context_length\n",
    "        self.length = length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, batch_idx):\n",
    "        input_tensor = torch.stack([self.data[idx:idx + self.context_length] for idx in batch_idx])\n",
    "        labels = torch.stack([self.data[idx+1:idx + self.context_length+1] for idx in batch_idx])\n",
    "        return input_tensor, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 32]), torch.Size([3, 32]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 32\n",
    "dataset = LotrDataset(tokenized_lotr, context_length, 1)\n",
    "batch = dataset[torch.tensor([1, 1234, 12345])]\n",
    "batch[0].shape, batch[1].shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construire TolkienGPT 🏗️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le mécansime d'attention**\n",
    "\n",
    "En théorie: [Blogue sur le mécansime d'attention](https://www.syntell.com/blogue/nlp-3e-partie-chatgpt/)\n",
    "\n",
    "En pratique: [Implémentation Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n",
    "\n",
    "Travaillons ensemble pour comprendre what's going on 🕵️‍♂️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment masquer l'attention via calculs matriciels 🤔? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5666,  1.1510, -0.8756,  1.1321],\n",
       "        [ 1.0021, -0.1516, -1.4235, -1.5469],\n",
       "        [ 0.1155, -0.0129,  0.6754,  0.4297],\n",
       "        [-1.6479, -0.2052,  0.1547, -0.4327]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_table = nn.Embedding(len(tokenizer.unique_words), 4)\n",
    "x = embedding_table(torch.tensor([tokenizer.word2idx[x] for x in [\"gandalf\", \"the\", \"grey\", \"is\"]]))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones((4, 4)))\n",
    "mask /= torch.sum(mask, dim=1, keepdim=True)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5666,  1.1510, -0.8756,  1.1321],\n",
       "        [-0.2822,  0.4997, -1.1496, -0.2074],\n",
       "        [-0.1497,  0.3289, -0.5412,  0.0050],\n",
       "        [-0.5242,  0.1954, -0.3673, -0.1044]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = mask @ x\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_bias = torch.zeros((4, 4))\n",
    "mas = torch.tril(torch.ones((4, 4)))\n",
    "attn_bias.masked_fill_(mas == 0, float(\"-inf\"))\n",
    "# Communication entre les mots\n",
    "# attn_bias = torch.softmax(attn_bias, dim=1)\n",
    "attn_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment masquer les embeddings font pour se parler 🤔? \n",
    "\n",
    "- Query = Voici ce que je cherche comme information\n",
    "- Key = Voici ce que je peux te donner comme information (marketing)\n",
    "- value = Voici l'information que je te donne réellement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communication = nn.Linear(4, 4*3)\n",
    "key, query, value = communication(x).chunk(3, dim=-1)\n",
    "attn_scores = query @ key.transpose(-1, -2) / math.sqrt(query.size(-1))\n",
    "attn_scores += attn_bias\n",
    "attn_scores = torch.softmax(attn_scores, dim=-1)\n",
    "attn_scores = attn_scores @ value\n",
    "attn_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(key, query, value):\n",
    "    B,_, T, E = key.size()\n",
    "    attn_bias = torch.zeros((T, T), device=key.device)\n",
    "    mask = torch.tril(torch.ones((T, T), device=key.device))\n",
    "    attn_bias.masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "    attn_scores = query @ key.transpose(-1, -2) / math.sqrt(query.size(-1))\n",
    "    attn_scores += attn_bias\n",
    "    attn_scores = torch.softmax(attn_scores, dim=-1)\n",
    "    attn_scores = attn_scores @ value\n",
    "    return attn_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, nb_heads, embedding_size, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.communication = nn.Linear(embedding_size, embedding_size*3, bias=False)\n",
    "        self.nb_heads = nb_heads\n",
    "        self.projection = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, E = x.shape\n",
    "        key, query, value = self.communication(x).chunk(3, dim=-1)\n",
    "        key = key.view(B, T, self.nb_heads, E // self.nb_heads).transpose(1, 2) # shape: (batch size, nb heads, sequence length , embedding size per head)\n",
    "        query = query.view(B, T, self.nb_heads, E // self.nb_heads).transpose(1, 2)\n",
    "        value = value.view(B, T, self.nb_heads, E // self.nb_heads).transpose(1, 2)\n",
    "        attn_scores = scaled_dot_product_attention(key, query, value)\n",
    "        return self.dropout(self.projection(attn_scores.transpose(1, 2).contiguous().view(B, T, E)))\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embedding_size, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embedding_size, embedding_size)\n",
    "        self.fc2 = nn.Linear(embedding_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.fc2(F.gelu(self.fc1(x))))\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, nb_heads, embedding_size) -> None:\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(nb_heads, embedding_size, 0.2)\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_size)\n",
    "        self.mlp = MLP(embedding_size, 0.2)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.multi_head_attention(self.layer_norm1(x))\n",
    "        x = x + self.mlp(self.layer_norm2(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L'architecture Transformer**\n",
    "\n",
    "Implémentation selon le paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TolkienGPT(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, embedding_dim, context_length, nb_blocks=6, nb_heads=8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_embeddings = nn.Embedding(context_length, embedding_dim)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(nb_heads, embedding_dim) for _ in range(nb_blocks)]\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_length = context_length\n",
    "        self.nb_blocks = nb_blocks\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "        embeddings = self.embedding_table(x) + self.positional_embeddings(\n",
    "            torch.arange(T, device=x.device)\n",
    "        )\n",
    "        E = embeddings.shape[-1]\n",
    "\n",
    "        x = self.dropout(embeddings)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        logits = self.linear(self.ln(x))\n",
    "\n",
    "        logits = logits.view(B * T, self.vocab_size)  # Concaténer les batchs\n",
    "        if targets is not None:\n",
    "            targets = targets.view(B * T)\n",
    "            loss = self.loss_function(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits.view(B, T, self.vocab_size), loss\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(\n",
    "                module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.nb_blocks)\n",
    "            )\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def generate(self, idx, length):\n",
    "        with torch.no_grad():\n",
    "            for _ in range(length):\n",
    "                # On prend les derniers context_length embeddings comme contexte d'inférence\n",
    "                context = idx[:, -self.context_length :]\n",
    "                x_emb, _ = self.forward(context, None)\n",
    "\n",
    "                # On utilise les derniers logits pour obtenir notre distribution de probabilité pour le prochain mot\n",
    "                x_emb = x_emb[:, -1, :] / 0.5\n",
    "                normalized_x_emb = F.softmax(x_emb, dim=-1)\n",
    "\n",
    "                # On pige un mot suivant cette distribution de probabilité\n",
    "                next_idx = torch.multinomial(normalized_x_emb, num_samples=1)\n",
    "                idx = torch.cat([idx, next_idx], dim=-1)\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraîner TolkienGPT 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 64\n",
    "batch_size = 128\n",
    "embeddind_dim = 768\n",
    "nb_heads = 16\n",
    "nb_blocks = 16\n",
    "model = TolkienGPT(len(tokenizer.unique_words), embeddind_dim, context_length, nb_blocks=nb_blocks, nb_heads=nb_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle a 77421707 paramètres\n"
     ]
    }
   ],
   "source": [
    "print(f\"Le modèle a {sum(p.numel() for p in model.parameters() if p.requires_grad)} paramètres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12889/4194449351.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = LotrDataset(torch.tensor(tokenized_lotr), context_length, length=1)\n"
     ]
    }
   ],
   "source": [
    "train_val_split_idx = int(len(tokenized_lotr) * 0.8)\n",
    "dataset = LotrDataset(torch.tensor(tokenized_lotr), context_length, length=1)\n",
    "random_idx = torch.randint(low=0, high=train_val_split_idx-context_length, size=(batch_size,))\n",
    "x, targets = dataset[random_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: \n",
      " torch.Size([128, 64]) (batch_size, context_length) \n",
      "\n",
      "Output size: \n",
      " torch.Size([128, 64, 13451]) (batch_size, context_length, embedding_dim)\n",
      "Théoriquement, une loss initialisée random devrait être de 9.51, la loss est de 9.51.\n"
     ]
    }
   ],
   "source": [
    "batch_output = model(x, targets)\n",
    "print(f\"Input size: \\n {x.shape} (batch_size, context_length) \\n\\nOutput size: \\n {batch_output[0].shape} (batch_size, context_length, embedding_dim)\")\n",
    "print(f\"Théoriquement, une loss initialisée random devrait être de {-math.log(1/batch_output[0].size(-1)):.2f}, la loss est de {batch_output[1]:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12889/1252902887.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = LotrDataset(torch.tensor(tokenized_lotr), context_length, length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with val loss: 9.51\n",
      "Epoch 0 | Train loss: 9.51 | Val loss: 9.51\n",
      "New best model saved with val loss: 9.51\n",
      "New best model saved with val loss: 9.51\n",
      "New best model saved with val loss: 9.51\n",
      "New best model saved with val loss: 9.51\n",
      "New best model saved with val loss: 9.50\n",
      "New best model saved with val loss: 9.50\n",
      "New best model saved with val loss: 9.49\n",
      "New best model saved with val loss: 9.49\n",
      "New best model saved with val loss: 9.48\n",
      "Epoch 10 | Train loss: 9.49 | Val loss: 9.48\n",
      "New best model saved with val loss: 9.47\n",
      "New best model saved with val loss: 9.47\n",
      "New best model saved with val loss: 9.46\n",
      "New best model saved with val loss: 9.45\n",
      "New best model saved with val loss: 9.44\n",
      "New best model saved with val loss: 9.42\n",
      "New best model saved with val loss: 9.41\n",
      "New best model saved with val loss: 9.40\n",
      "New best model saved with val loss: 9.39\n",
      "New best model saved with val loss: 9.37\n",
      "Epoch 20 | Train loss: 9.42 | Val loss: 9.37\n",
      "New best model saved with val loss: 9.36\n",
      "New best model saved with val loss: 9.35\n",
      "New best model saved with val loss: 9.33\n",
      "New best model saved with val loss: 9.32\n",
      "New best model saved with val loss: 9.31\n",
      "New best model saved with val loss: 9.30\n",
      "New best model saved with val loss: 9.29\n",
      "New best model saved with val loss: 9.27\n",
      "New best model saved with val loss: 9.26\n",
      "New best model saved with val loss: 9.26\n",
      "Epoch 30 | Train loss: 9.29 | Val loss: 9.26\n",
      "New best model saved with val loss: 9.25\n",
      "New best model saved with val loss: 9.24\n",
      "New best model saved with val loss: 9.22\n",
      "New best model saved with val loss: 9.21\n",
      "New best model saved with val loss: 9.21\n",
      "New best model saved with val loss: 9.20\n",
      "New best model saved with val loss: 9.19\n",
      "New best model saved with val loss: 9.19\n",
      "New best model saved with val loss: 9.18\n",
      "New best model saved with val loss: 9.18\n",
      "Epoch 40 | Train loss: 9.20 | Val loss: 9.18\n",
      "New best model saved with val loss: 9.17\n",
      "New best model saved with val loss: 9.15\n",
      "New best model saved with val loss: 9.14\n",
      "New best model saved with val loss: 9.13\n",
      "New best model saved with val loss: 9.12\n",
      "New best model saved with val loss: 9.11\n",
      "New best model saved with val loss: 9.11\n",
      "Epoch 50 | Train loss: 9.13 | Val loss: 9.11\n",
      "New best model saved with val loss: 9.11\n",
      "New best model saved with val loss: 9.10\n",
      "New best model saved with val loss: 9.09\n",
      "New best model saved with val loss: 9.09\n",
      "New best model saved with val loss: 9.08\n",
      "New best model saved with val loss: 9.07\n",
      "New best model saved with val loss: 9.05\n",
      "New best model saved with val loss: 9.05\n",
      "New best model saved with val loss: 9.05\n",
      "Epoch 60 | Train loss: 9.06 | Val loss: 9.05\n",
      "New best model saved with val loss: 9.03\n",
      "New best model saved with val loss: 9.03\n",
      "New best model saved with val loss: 9.02\n",
      "New best model saved with val loss: 9.01\n",
      "New best model saved with val loss: 8.99\n",
      "New best model saved with val loss: 8.98\n",
      "New best model saved with val loss: 8.97\n",
      "Epoch 70 | Train loss: 8.98 | Val loss: 8.97\n",
      "New best model saved with val loss: 8.97\n",
      "New best model saved with val loss: 8.96\n",
      "New best model saved with val loss: 8.96\n",
      "New best model saved with val loss: 8.94\n",
      "New best model saved with val loss: 8.93\n",
      "New best model saved with val loss: 8.92\n",
      "New best model saved with val loss: 8.91\n",
      "New best model saved with val loss: 8.90\n",
      "New best model saved with val loss: 8.89\n",
      "Epoch 80 | Train loss: 8.88 | Val loss: 8.89\n",
      "New best model saved with val loss: 8.86\n",
      "New best model saved with val loss: 8.85\n",
      "New best model saved with val loss: 8.84\n",
      "New best model saved with val loss: 8.83\n",
      "New best model saved with val loss: 8.81\n",
      "New best model saved with val loss: 8.79\n",
      "Epoch 90 | Train loss: 8.79 | Val loss: 8.79\n",
      "New best model saved with val loss: 8.79\n",
      "New best model saved with val loss: 8.78\n",
      "New best model saved with val loss: 8.78\n",
      "New best model saved with val loss: 8.77\n",
      "New best model saved with val loss: 8.74\n",
      "New best model saved with val loss: 8.73\n",
      "New best model saved with val loss: 8.71\n",
      "New best model saved with val loss: 8.69\n",
      "Epoch 100 | Train loss: 8.68 | Val loss: 8.70\n",
      "New best model saved with val loss: 8.69\n",
      "New best model saved with val loss: 8.67\n",
      "New best model saved with val loss: 8.66\n",
      "New best model saved with val loss: 8.66\n",
      "New best model saved with val loss: 8.62\n",
      "New best model saved with val loss: 8.62\n",
      "New best model saved with val loss: 8.61\n",
      "New best model saved with val loss: 8.59\n",
      "New best model saved with val loss: 8.59\n",
      "Epoch 110 | Train loss: 8.57 | Val loss: 8.59\n",
      "New best model saved with val loss: 8.58\n",
      "New best model saved with val loss: 8.56\n",
      "New best model saved with val loss: 8.54\n",
      "New best model saved with val loss: 8.54\n",
      "New best model saved with val loss: 8.52\n",
      "New best model saved with val loss: 8.49\n",
      "Epoch 120 | Train loss: 8.46 | Val loss: 8.51\n",
      "New best model saved with val loss: 8.47\n",
      "New best model saved with val loss: 8.45\n",
      "New best model saved with val loss: 8.44\n",
      "New best model saved with val loss: 8.43\n",
      "New best model saved with val loss: 8.42\n",
      "New best model saved with val loss: 8.41\n",
      "New best model saved with val loss: 8.38\n",
      "New best model saved with val loss: 8.37\n",
      "Epoch 130 | Train loss: 8.34 | Val loss: 8.37\n",
      "New best model saved with val loss: 8.36\n",
      "New best model saved with val loss: 8.36\n",
      "New best model saved with val loss: 8.35\n",
      "New best model saved with val loss: 8.33\n",
      "New best model saved with val loss: 8.32\n",
      "New best model saved with val loss: 8.30\n",
      "New best model saved with val loss: 8.28\n",
      "Epoch 140 | Train loss: 8.23 | Val loss: 8.28\n",
      "New best model saved with val loss: 8.27\n",
      "New best model saved with val loss: 8.25\n",
      "New best model saved with val loss: 8.24\n",
      "New best model saved with val loss: 8.21\n",
      "New best model saved with val loss: 8.20\n",
      "Epoch 150 | Train loss: 8.13 | Val loss: 8.21\n",
      "New best model saved with val loss: 8.19\n",
      "New best model saved with val loss: 8.15\n",
      "New best model saved with val loss: 8.13\n",
      "New best model saved with val loss: 8.12\n",
      "New best model saved with val loss: 8.08\n",
      "Epoch 160 | Train loss: 8.02 | Val loss: 8.09\n",
      "New best model saved with val loss: 8.06\n",
      "New best model saved with val loss: 8.05\n",
      "New best model saved with val loss: 8.02\n",
      "New best model saved with val loss: 8.01\n",
      "New best model saved with val loss: 8.01\n",
      "New best model saved with val loss: 8.00\n",
      "New best model saved with val loss: 8.00\n",
      "Epoch 170 | Train loss: 7.93 | Val loss: 8.00\n",
      "New best model saved with val loss: 7.99\n",
      "New best model saved with val loss: 7.98\n",
      "New best model saved with val loss: 7.94\n",
      "New best model saved with val loss: 7.94\n",
      "New best model saved with val loss: 7.90\n",
      "New best model saved with val loss: 7.90\n",
      "Epoch 180 | Train loss: 7.83 | Val loss: 7.90\n",
      "New best model saved with val loss: 7.89\n",
      "New best model saved with val loss: 7.87\n",
      "New best model saved with val loss: 7.86\n",
      "New best model saved with val loss: 7.84\n",
      "New best model saved with val loss: 7.83\n",
      "New best model saved with val loss: 7.81\n",
      "Epoch 190 | Train loss: 7.73 | Val loss: 7.81\n",
      "New best model saved with val loss: 7.78\n",
      "New best model saved with val loss: 7.76\n",
      "New best model saved with val loss: 7.74\n",
      "Epoch 200 | Train loss: 7.64 | Val loss: 7.74\n",
      "New best model saved with val loss: 7.71\n",
      "New best model saved with val loss: 7.70\n",
      "New best model saved with val loss: 7.68\n",
      "New best model saved with val loss: 7.66\n",
      "Epoch 210 | Train loss: 7.57 | Val loss: 7.69\n",
      "New best model saved with val loss: 7.61\n",
      "New best model saved with val loss: 7.61\n",
      "New best model saved with val loss: 7.57\n",
      "Epoch 220 | Train loss: 7.48 | Val loss: 7.57\n",
      "New best model saved with val loss: 7.56\n",
      "New best model saved with val loss: 7.56\n",
      "New best model saved with val loss: 7.54\n",
      "New best model saved with val loss: 7.52\n",
      "Epoch 230 | Train loss: 7.40 | Val loss: 7.53\n",
      "New best model saved with val loss: 7.49\n",
      "New best model saved with val loss: 7.47\n",
      "New best model saved with val loss: 7.45\n",
      "Epoch 240 | Train loss: 7.35 | Val loss: 7.45\n",
      "New best model saved with val loss: 7.43\n",
      "New best model saved with val loss: 7.39\n",
      "New best model saved with val loss: 7.37\n",
      "Epoch 250 | Train loss: 7.26 | Val loss: 7.40\n",
      "New best model saved with val loss: 7.35\n",
      "New best model saved with val loss: 7.34\n",
      "New best model saved with val loss: 7.34\n",
      "New best model saved with val loss: 7.31\n",
      "Epoch 260 | Train loss: 7.18 | Val loss: 7.31\n",
      "New best model saved with val loss: 7.30\n",
      "New best model saved with val loss: 7.28\n",
      "New best model saved with val loss: 7.27\n",
      "New best model saved with val loss: 7.25\n",
      "New best model saved with val loss: 7.24\n",
      "Epoch 270 | Train loss: 7.11 | Val loss: 7.24\n",
      "New best model saved with val loss: 7.18\n",
      "Epoch 280 | Train loss: 7.09 | Val loss: 7.20\n",
      "New best model saved with val loss: 7.18\n",
      "New best model saved with val loss: 7.17\n",
      "Epoch 290 | Train loss: 7.02 | Val loss: 7.18\n",
      "New best model saved with val loss: 7.14\n",
      "New best model saved with val loss: 7.14\n",
      "New best model saved with val loss: 7.13\n",
      "New best model saved with val loss: 7.13\n",
      "New best model saved with val loss: 7.10\n",
      "Epoch 300 | Train loss: 6.96 | Val loss: 7.10\n",
      "New best model saved with val loss: 7.08\n",
      "Epoch 310 | Train loss: 6.90 | Val loss: 7.14\n",
      "New best model saved with val loss: 7.04\n",
      "New best model saved with val loss: 7.01\n",
      "New best model saved with val loss: 7.01\n",
      "Epoch 320 | Train loss: 6.85 | Val loss: 7.05\n",
      "New best model saved with val loss: 7.00\n",
      "New best model saved with val loss: 6.99\n",
      "New best model saved with val loss: 6.99\n",
      "New best model saved with val loss: 6.96\n",
      "Epoch 330 | Train loss: 6.80 | Val loss: 6.96\n",
      "New best model saved with val loss: 6.94\n",
      "New best model saved with val loss: 6.93\n",
      "Epoch 340 | Train loss: 6.76 | Val loss: 6.96\n",
      "New best model saved with val loss: 6.91\n",
      "New best model saved with val loss: 6.89\n",
      "Epoch 350 | Train loss: 6.73 | Val loss: 6.90\n",
      "New best model saved with val loss: 6.88\n",
      "New best model saved with val loss: 6.85\n",
      "New best model saved with val loss: 6.83\n",
      "Epoch 360 | Train loss: 6.71 | Val loss: 6.86\n",
      "New best model saved with val loss: 6.82\n",
      "Epoch 370 | Train loss: 6.65 | Val loss: 6.86\n",
      "New best model saved with val loss: 6.78\n",
      "New best model saved with val loss: 6.76\n",
      "Epoch 380 | Train loss: 6.61 | Val loss: 6.89\n",
      "New best model saved with val loss: 6.74\n",
      "Epoch 390 | Train loss: 6.59 | Val loss: 6.78\n",
      "New best model saved with val loss: 6.74\n",
      "New best model saved with val loss: 6.73\n",
      "New best model saved with val loss: 6.71\n",
      "Epoch 400 | Train loss: 6.57 | Val loss: 6.76\n",
      "New best model saved with val loss: 6.69\n",
      "Epoch 410 | Train loss: 6.51 | Val loss: 6.75\n",
      "New best model saved with val loss: 6.67\n",
      "New best model saved with val loss: 6.64\n",
      "Epoch 420 | Train loss: 6.49 | Val loss: 6.75\n",
      "New best model saved with val loss: 6.59\n",
      "Epoch 430 | Train loss: 6.47 | Val loss: 6.70\n",
      "New best model saved with val loss: 6.55\n",
      "Epoch 440 | Train loss: 6.45 | Val loss: 6.70\n",
      "Epoch 450 | Train loss: 6.41 | Val loss: 6.65\n",
      "Epoch 460 | Train loss: 6.37 | Val loss: 6.64\n",
      "Epoch 470 | Train loss: 6.38 | Val loss: 6.57\n",
      "New best model saved with val loss: 6.51\n",
      "New best model saved with val loss: 6.50\n",
      "Epoch 480 | Train loss: 6.34 | Val loss: 6.50\n",
      "New best model saved with val loss: 6.50\n",
      "Epoch 490 | Train loss: 6.33 | Val loss: 6.50\n",
      "New best model saved with val loss: 6.46\n",
      "New best model saved with val loss: 6.45\n",
      "New best model saved with val loss: 6.44\n",
      "Epoch 500 | Train loss: 6.28 | Val loss: 6.59\n",
      "New best model saved with val loss: 6.41\n",
      "Epoch 510 | Train loss: 6.26 | Val loss: 6.50\n",
      "Epoch 520 | Train loss: 6.24 | Val loss: 6.41\n",
      "Epoch 530 | Train loss: 6.29 | Val loss: 6.48\n",
      "New best model saved with val loss: 6.40\n",
      "New best model saved with val loss: 6.35\n",
      "Epoch 540 | Train loss: 6.25 | Val loss: 6.35\n",
      "Epoch 550 | Train loss: 6.18 | Val loss: 6.46\n",
      "Epoch 560 | Train loss: 6.18 | Val loss: 6.44\n",
      "Epoch 570 | Train loss: 6.20 | Val loss: 6.36\n",
      "New best model saved with val loss: 6.34\n",
      "New best model saved with val loss: 6.30\n",
      "Epoch 580 | Train loss: 6.17 | Val loss: 6.40\n",
      "New best model saved with val loss: 6.29\n",
      "Epoch 590 | Train loss: 6.13 | Val loss: 6.35\n",
      "New best model saved with val loss: 6.29\n",
      "Epoch 600 | Train loss: 6.15 | Val loss: 6.42\n",
      "Epoch 610 | Train loss: 6.11 | Val loss: 6.32\n",
      "New best model saved with val loss: 6.29\n",
      "Epoch 620 | Train loss: 6.07 | Val loss: 6.29\n",
      "New best model saved with val loss: 6.28\n",
      "Epoch 630 | Train loss: 6.08 | Val loss: 6.34\n",
      "New best model saved with val loss: 6.26\n",
      "New best model saved with val loss: 6.25\n",
      "Epoch 640 | Train loss: 6.00 | Val loss: 6.33\n",
      "Epoch 650 | Train loss: 6.06 | Val loss: 6.30\n",
      "New best model saved with val loss: 6.19\n",
      "Epoch 660 | Train loss: 6.05 | Val loss: 6.26\n",
      "Epoch 670 | Train loss: 5.99 | Val loss: 6.19\n",
      "Epoch 680 | Train loss: 6.00 | Val loss: 6.30\n",
      "Epoch 690 | Train loss: 5.96 | Val loss: 6.26\n",
      "Epoch 700 | Train loss: 5.99 | Val loss: 6.28\n",
      "Epoch 710 | Train loss: 5.97 | Val loss: 6.26\n",
      "New best model saved with val loss: 6.16\n",
      "Epoch 720 | Train loss: 5.96 | Val loss: 6.16\n",
      "Epoch 730 | Train loss: 5.93 | Val loss: 6.20\n",
      "New best model saved with val loss: 6.12\n",
      "Epoch 740 | Train loss: 5.92 | Val loss: 6.31\n",
      "Epoch 750 | Train loss: 5.90 | Val loss: 6.24\n",
      "New best model saved with val loss: 6.10\n",
      "New best model saved with val loss: 6.04\n",
      "Epoch 760 | Train loss: 5.90 | Val loss: 6.04\n",
      "Epoch 770 | Train loss: 5.89 | Val loss: 6.14\n",
      "Epoch 780 | Train loss: 5.88 | Val loss: 6.25\n",
      "Epoch 790 | Train loss: 5.83 | Val loss: 6.25\n",
      "Epoch 800 | Train loss: 5.85 | Val loss: 6.09\n",
      "Epoch 810 | Train loss: 5.81 | Val loss: 6.21\n",
      "New best model saved with val loss: 6.02\n",
      "Epoch 820 | Train loss: 5.86 | Val loss: 6.07\n",
      "Epoch 830 | Train loss: 5.86 | Val loss: 6.19\n",
      "Epoch 840 | Train loss: 5.88 | Val loss: 6.15\n",
      "New best model saved with val loss: 5.96\n",
      "Epoch 850 | Train loss: 5.81 | Val loss: 5.96\n",
      "Epoch 860 | Train loss: 5.83 | Val loss: 6.12\n",
      "Epoch 870 | Train loss: 5.78 | Val loss: 6.03\n",
      "Epoch 880 | Train loss: 5.75 | Val loss: 6.05\n",
      "Epoch 890 | Train loss: 5.74 | Val loss: 6.00\n",
      "Epoch 900 | Train loss: 5.76 | Val loss: 6.09\n",
      "Epoch 910 | Train loss: 5.75 | Val loss: 6.01\n",
      "New best model saved with val loss: 5.94\n",
      "Epoch 920 | Train loss: 5.75 | Val loss: 6.07\n",
      "New best model saved with val loss: 5.94\n",
      "Epoch 930 | Train loss: 5.72 | Val loss: 6.04\n",
      "New best model saved with val loss: 5.93\n",
      "Epoch 940 | Train loss: 5.72 | Val loss: 6.14\n",
      "Epoch 950 | Train loss: 5.72 | Val loss: 6.12\n",
      "Epoch 960 | Train loss: 5.69 | Val loss: 6.08\n",
      "New best model saved with val loss: 5.93\n",
      "Epoch 970 | Train loss: 5.64 | Val loss: 6.06\n",
      "Epoch 980 | Train loss: 5.64 | Val loss: 5.95\n",
      "New best model saved with val loss: 5.92\n",
      "Epoch 990 | Train loss: 5.65 | Val loss: 6.01\n",
      "New best model saved with val loss: 5.88\n",
      "Epoch 1000 | Train loss: 5.63 | Val loss: 5.95\n",
      "Epoch 1010 | Train loss: 5.62 | Val loss: 6.03\n",
      "Epoch 1020 | Train loss: 5.67 | Val loss: 6.03\n",
      "Epoch 1030 | Train loss: 5.63 | Val loss: 5.97\n",
      "Epoch 1040 | Train loss: 5.64 | Val loss: 6.05\n",
      "Epoch 1050 | Train loss: 5.61 | Val loss: 5.90\n",
      "Epoch 1060 | Train loss: 5.55 | Val loss: 6.08\n",
      "Epoch 1070 | Train loss: 5.56 | Val loss: 5.94\n",
      "New best model saved with val loss: 5.86\n",
      "Epoch 1080 | Train loss: 5.56 | Val loss: 5.94\n",
      "New best model saved with val loss: 5.83\n",
      "Epoch 1090 | Train loss: 5.57 | Val loss: 5.91\n",
      "Epoch 1100 | Train loss: 5.52 | Val loss: 5.90\n",
      "Epoch 1110 | Train loss: 5.56 | Val loss: 5.95\n",
      "Epoch 1120 | Train loss: 5.53 | Val loss: 5.87\n",
      "Epoch 1130 | Train loss: 5.53 | Val loss: 6.06\n",
      "Epoch 1140 | Train loss: 5.53 | Val loss: 5.95\n",
      "Epoch 1150 | Train loss: 5.49 | Val loss: 5.94\n",
      "New best model saved with val loss: 5.83\n",
      "Epoch 1160 | Train loss: 5.62 | Val loss: 5.83\n",
      "New best model saved with val loss: 5.80\n",
      "Epoch 1170 | Train loss: 5.47 | Val loss: 5.87\n",
      "New best model saved with val loss: 5.77\n",
      "Epoch 1180 | Train loss: 5.50 | Val loss: 5.79\n",
      "New best model saved with val loss: 5.71\n",
      "Epoch 1190 | Train loss: 5.47 | Val loss: 5.88\n",
      "Epoch 1200 | Train loss: 5.47 | Val loss: 5.97\n",
      "Epoch 1210 | Train loss: 5.48 | Val loss: 5.81\n",
      "Epoch 1220 | Train loss: 5.51 | Val loss: 5.80\n",
      "Epoch 1230 | Train loss: 5.50 | Val loss: 5.76\n",
      "Epoch 1240 | Train loss: 5.47 | Val loss: 5.81\n",
      "Epoch 1250 | Train loss: 5.46 | Val loss: 5.96\n",
      "Epoch 1260 | Train loss: 5.45 | Val loss: 5.91\n",
      "Epoch 1270 | Train loss: 5.43 | Val loss: 5.83\n",
      "Epoch 1280 | Train loss: 5.35 | Val loss: 5.88\n",
      "Epoch 1290 | Train loss: 5.39 | Val loss: 5.93\n",
      "Epoch 1300 | Train loss: 5.43 | Val loss: 5.80\n",
      "New best model saved with val loss: 5.71\n",
      "Epoch 1310 | Train loss: 5.43 | Val loss: 5.93\n",
      "Epoch 1320 | Train loss: 5.46 | Val loss: 5.75\n",
      "New best model saved with val loss: 5.69\n",
      "Epoch 1330 | Train loss: 5.34 | Val loss: 5.73\n",
      "Epoch 1340 | Train loss: 5.38 | Val loss: 5.74\n",
      "Epoch 1350 | Train loss: 5.39 | Val loss: 5.77\n",
      "Epoch 1360 | Train loss: 5.39 | Val loss: 5.85\n",
      "Epoch 1370 | Train loss: 5.33 | Val loss: 5.80\n",
      "Epoch 1380 | Train loss: 5.38 | Val loss: 5.72\n",
      "Epoch 1390 | Train loss: 5.38 | Val loss: 5.86\n",
      "New best model saved with val loss: 5.68\n",
      "Epoch 1400 | Train loss: 5.38 | Val loss: 5.75\n",
      "New best model saved with val loss: 5.68\n",
      "Epoch 1410 | Train loss: 5.36 | Val loss: 5.87\n",
      "New best model saved with val loss: 5.64\n",
      "Epoch 1420 | Train loss: 5.32 | Val loss: 5.79\n",
      "New best model saved with val loss: 5.63\n",
      "Epoch 1430 | Train loss: 5.30 | Val loss: 5.78\n",
      "New best model saved with val loss: 5.62\n",
      "Epoch 1440 | Train loss: 5.33 | Val loss: 5.62\n",
      "New best model saved with val loss: 5.61\n",
      "Epoch 1450 | Train loss: 5.32 | Val loss: 5.65\n",
      "Epoch 1460 | Train loss: 5.28 | Val loss: 5.72\n",
      "Epoch 1470 | Train loss: 5.30 | Val loss: 5.74\n",
      "New best model saved with val loss: 5.60\n",
      "Epoch 1480 | Train loss: 5.28 | Val loss: 5.70\n",
      "Epoch 1490 | Train loss: 5.28 | Val loss: 5.83\n",
      "Epoch 1500 | Train loss: 5.23 | Val loss: 5.77\n",
      "New best model saved with val loss: 5.60\n",
      "New best model saved with val loss: 5.57\n",
      "Epoch 1510 | Train loss: 5.26 | Val loss: 5.69\n",
      "Epoch 1520 | Train loss: 5.25 | Val loss: 5.66\n",
      "Epoch 1530 | Train loss: 5.33 | Val loss: 5.72\n",
      "Epoch 1540 | Train loss: 5.25 | Val loss: 5.79\n",
      "New best model saved with val loss: 5.55\n",
      "Epoch 1550 | Train loss: 5.29 | Val loss: 5.55\n",
      "Epoch 1560 | Train loss: 5.28 | Val loss: 5.64\n",
      "Epoch 1570 | Train loss: 5.27 | Val loss: 5.75\n",
      "Epoch 1580 | Train loss: 5.21 | Val loss: 5.64\n",
      "Epoch 1590 | Train loss: 5.27 | Val loss: 5.71\n",
      "Epoch 1600 | Train loss: 5.24 | Val loss: 5.68\n",
      "Epoch 1610 | Train loss: 5.23 | Val loss: 5.68\n",
      "Epoch 1620 | Train loss: 5.28 | Val loss: 5.80\n",
      "Epoch 1630 | Train loss: 5.29 | Val loss: 5.74\n",
      "Epoch 1640 | Train loss: 5.25 | Val loss: 5.73\n",
      "Epoch 1650 | Train loss: 5.24 | Val loss: 5.68\n",
      "New best model saved with val loss: 5.55\n",
      "New best model saved with val loss: 5.54\n",
      "Epoch 1660 | Train loss: 5.23 | Val loss: 5.65\n",
      "Epoch 1670 | Train loss: 5.23 | Val loss: 5.63\n",
      "Epoch 1680 | Train loss: 5.18 | Val loss: 5.66\n",
      "Epoch 1690 | Train loss: 5.19 | Val loss: 5.61\n",
      "Epoch 1700 | Train loss: 5.23 | Val loss: 5.62\n",
      "Epoch 1710 | Train loss: 5.21 | Val loss: 5.57\n",
      "Epoch 1720 | Train loss: 5.14 | Val loss: 5.61\n",
      "Epoch 1730 | Train loss: 5.19 | Val loss: 5.75\n",
      "Epoch 1740 | Train loss: 5.14 | Val loss: 5.64\n",
      "Epoch 1750 | Train loss: 5.19 | Val loss: 5.70\n",
      "Epoch 1760 | Train loss: 5.17 | Val loss: 5.68\n",
      "New best model saved with val loss: 5.54\n",
      "Epoch 1770 | Train loss: 5.14 | Val loss: 5.58\n",
      "Epoch 1780 | Train loss: 5.11 | Val loss: 5.72\n",
      "Epoch 1790 | Train loss: 5.12 | Val loss: 5.66\n",
      "New best model saved with val loss: 5.49\n",
      "Epoch 1800 | Train loss: 5.13 | Val loss: 5.63\n",
      "Epoch 1810 | Train loss: 5.17 | Val loss: 5.66\n",
      "Epoch 1820 | Train loss: 5.13 | Val loss: 5.62\n",
      "Epoch 1830 | Train loss: 5.13 | Val loss: 5.65\n",
      "Epoch 1840 | Train loss: 5.10 | Val loss: 5.51\n",
      "Epoch 1850 | Train loss: 5.12 | Val loss: 5.59\n",
      "Epoch 1860 | Train loss: 5.12 | Val loss: 5.63\n",
      "Epoch 1870 | Train loss: 5.12 | Val loss: 5.51\n",
      "Epoch 1880 | Train loss: 5.14 | Val loss: 5.65\n",
      "Epoch 1890 | Train loss: 5.06 | Val loss: 5.68\n",
      "Epoch 1900 | Train loss: 5.06 | Val loss: 5.67\n",
      "Epoch 1910 | Train loss: 5.09 | Val loss: 5.50\n",
      "Epoch 1920 | Train loss: 5.10 | Val loss: 5.74\n",
      "Epoch 1930 | Train loss: 5.11 | Val loss: 5.58\n",
      "Epoch 1940 | Train loss: 5.04 | Val loss: 5.56\n",
      "Epoch 1950 | Train loss: 5.08 | Val loss: 5.56\n",
      "New best model saved with val loss: 5.47\n",
      "Epoch 1960 | Train loss: 5.11 | Val loss: 5.58\n",
      "Epoch 1970 | Train loss: 5.09 | Val loss: 5.60\n",
      "Epoch 1980 | Train loss: 5.08 | Val loss: 5.59\n",
      "Epoch 1990 | Train loss: 5.07 | Val loss: 5.60\n",
      "Epoch 2000 | Train loss: 5.08 | Val loss: 5.61\n",
      "Epoch 2010 | Train loss: 5.06 | Val loss: 5.72\n",
      "Epoch 2020 | Train loss: 5.07 | Val loss: 5.55\n",
      "Epoch 2030 | Train loss: 5.07 | Val loss: 5.65\n",
      "Epoch 2040 | Train loss: 5.07 | Val loss: 5.65\n",
      "Epoch 2050 | Train loss: 5.03 | Val loss: 5.58\n",
      "Epoch 2060 | Train loss: 5.04 | Val loss: 5.52\n",
      "Epoch 2070 | Train loss: 5.01 | Val loss: 5.75\n",
      "Epoch 2080 | Train loss: 4.99 | Val loss: 5.67\n",
      "Epoch 2090 | Train loss: 5.02 | Val loss: 5.59\n",
      "Epoch 2100 | Train loss: 5.06 | Val loss: 5.61\n",
      "Epoch 2110 | Train loss: 5.01 | Val loss: 5.59\n",
      "Epoch 2120 | Train loss: 5.04 | Val loss: 5.56\n",
      "New best model saved with val loss: 5.45\n",
      "Epoch 2130 | Train loss: 5.03 | Val loss: 5.54\n",
      "New best model saved with val loss: 5.44\n",
      "Epoch 2140 | Train loss: 5.01 | Val loss: 5.58\n",
      "Epoch 2150 | Train loss: 5.03 | Val loss: 5.62\n",
      "New best model saved with val loss: 5.40\n",
      "Epoch 2160 | Train loss: 4.98 | Val loss: 5.59\n",
      "Epoch 2170 | Train loss: 5.04 | Val loss: 5.64\n",
      "Epoch 2180 | Train loss: 4.99 | Val loss: 5.53\n",
      "Epoch 2190 | Train loss: 5.03 | Val loss: 5.56\n",
      "Epoch 2200 | Train loss: 4.98 | Val loss: 5.56\n",
      "Epoch 2210 | Train loss: 5.05 | Val loss: 5.58\n",
      "Epoch 2220 | Train loss: 4.91 | Val loss: 5.46\n",
      "Epoch 2230 | Train loss: 4.97 | Val loss: 5.73\n",
      "Epoch 2240 | Train loss: 5.00 | Val loss: 5.53\n",
      "Epoch 2250 | Train loss: 5.02 | Val loss: 5.64\n",
      "Epoch 2260 | Train loss: 4.95 | Val loss: 5.50\n",
      "Epoch 2270 | Train loss: 4.94 | Val loss: 5.51\n",
      "Epoch 2280 | Train loss: 4.96 | Val loss: 5.59\n",
      "New best model saved with val loss: 5.37\n",
      "Epoch 2290 | Train loss: 4.98 | Val loss: 5.53\n",
      "Epoch 2300 | Train loss: 4.92 | Val loss: 5.47\n",
      "Epoch 2310 | Train loss: 4.97 | Val loss: 5.77\n",
      "Epoch 2320 | Train loss: 4.88 | Val loss: 5.47\n",
      "Epoch 2330 | Train loss: 5.01 | Val loss: 5.50\n",
      "Epoch 2340 | Train loss: 4.97 | Val loss: 5.42\n",
      "Epoch 2350 | Train loss: 4.95 | Val loss: 5.62\n",
      "Epoch 2360 | Train loss: 4.94 | Val loss: 5.58\n",
      "Epoch 2370 | Train loss: 4.97 | Val loss: 5.53\n",
      "Epoch 2380 | Train loss: 4.95 | Val loss: 5.63\n",
      "Epoch 2390 | Train loss: 4.93 | Val loss: 5.52\n",
      "Epoch 2400 | Train loss: 4.92 | Val loss: 5.56\n",
      "Epoch 2410 | Train loss: 4.99 | Val loss: 5.38\n",
      "Epoch 2420 | Train loss: 4.96 | Val loss: 5.48\n",
      "Epoch 2430 | Train loss: 4.87 | Val loss: 5.53\n",
      "Epoch 2440 | Train loss: 4.90 | Val loss: 5.51\n",
      "Epoch 2450 | Train loss: 4.96 | Val loss: 5.51\n",
      "New best model saved with val loss: 5.35\n",
      "Epoch 2460 | Train loss: 4.97 | Val loss: 5.41\n",
      "Epoch 2470 | Train loss: 4.93 | Val loss: 5.53\n",
      "Epoch 2480 | Train loss: 4.98 | Val loss: 5.42\n",
      "Epoch 2490 | Train loss: 4.93 | Val loss: 5.44\n",
      "Epoch 2500 | Train loss: 4.93 | Val loss: 5.44\n",
      "Epoch 2510 | Train loss: 4.88 | Val loss: 5.52\n",
      "Epoch 2520 | Train loss: 4.91 | Val loss: 5.38\n",
      "New best model saved with val loss: 5.31\n",
      "Epoch 2530 | Train loss: 4.92 | Val loss: 5.36\n",
      "Epoch 2540 | Train loss: 4.91 | Val loss: 5.50\n",
      "Epoch 2550 | Train loss: 4.92 | Val loss: 5.55\n",
      "Epoch 2560 | Train loss: 4.87 | Val loss: 5.50\n",
      "Epoch 2570 | Train loss: 4.87 | Val loss: 5.45\n",
      "Epoch 2580 | Train loss: 4.94 | Val loss: 5.52\n",
      "Epoch 2590 | Train loss: 4.88 | Val loss: 5.53\n",
      "Epoch 2600 | Train loss: 4.88 | Val loss: 5.60\n",
      "Epoch 2610 | Train loss: 4.90 | Val loss: 5.48\n",
      "Epoch 2620 | Train loss: 4.95 | Val loss: 5.38\n",
      "New best model saved with val loss: 5.26\n",
      "Epoch 2630 | Train loss: 4.88 | Val loss: 5.47\n",
      "Epoch 2640 | Train loss: 4.84 | Val loss: 5.40\n",
      "Epoch 2650 | Train loss: 4.89 | Val loss: 5.57\n",
      "Epoch 2660 | Train loss: 4.87 | Val loss: 5.45\n",
      "Epoch 2670 | Train loss: 4.95 | Val loss: 5.51\n",
      "Epoch 2680 | Train loss: 4.86 | Val loss: 5.47\n",
      "Epoch 2690 | Train loss: 4.86 | Val loss: 5.59\n",
      "Epoch 2700 | Train loss: 4.92 | Val loss: 5.41\n",
      "Epoch 2710 | Train loss: 4.84 | Val loss: 5.58\n",
      "Epoch 2720 | Train loss: 4.90 | Val loss: 5.51\n",
      "Epoch 2730 | Train loss: 4.84 | Val loss: 5.52\n",
      "Epoch 2740 | Train loss: 4.87 | Val loss: 5.53\n",
      "Epoch 2750 | Train loss: 4.80 | Val loss: 5.44\n",
      "Epoch 2760 | Train loss: 4.86 | Val loss: 5.38\n",
      "Epoch 2770 | Train loss: 4.84 | Val loss: 5.48\n",
      "Epoch 2780 | Train loss: 4.85 | Val loss: 5.40\n",
      "Epoch 2790 | Train loss: 4.78 | Val loss: 5.43\n",
      "Epoch 2800 | Train loss: 4.85 | Val loss: 5.35\n",
      "Epoch 2810 | Train loss: 4.79 | Val loss: 5.43\n",
      "Epoch 2820 | Train loss: 4.82 | Val loss: 5.45\n",
      "Epoch 2830 | Train loss: 4.85 | Val loss: 5.52\n",
      "Epoch 2840 | Train loss: 4.92 | Val loss: 5.40\n",
      "Epoch 2850 | Train loss: 4.83 | Val loss: 5.49\n",
      "Epoch 2860 | Train loss: 4.82 | Val loss: 5.48\n",
      "Epoch 2870 | Train loss: 4.85 | Val loss: 5.42\n",
      "Epoch 2880 | Train loss: 4.76 | Val loss: 5.60\n",
      "Epoch 2890 | Train loss: 4.83 | Val loss: 5.36\n",
      "Epoch 2900 | Train loss: 4.80 | Val loss: 5.50\n",
      "Epoch 2910 | Train loss: 4.85 | Val loss: 5.43\n",
      "Epoch 2920 | Train loss: 4.82 | Val loss: 5.50\n",
      "Epoch 2930 | Train loss: 4.78 | Val loss: 5.59\n",
      "Epoch 2940 | Train loss: 4.85 | Val loss: 5.43\n",
      "Epoch 2950 | Train loss: 4.82 | Val loss: 5.39\n",
      "Epoch 2960 | Train loss: 4.81 | Val loss: 5.38\n",
      "Epoch 2970 | Train loss: 4.78 | Val loss: 5.45\n",
      "Epoch 2980 | Train loss: 4.89 | Val loss: 5.43\n",
      "Epoch 2990 | Train loss: 4.83 | Val loss: 5.49\n",
      "Epoch 3000 | Train loss: 4.77 | Val loss: 5.39\n",
      "Epoch 3010 | Train loss: 4.84 | Val loss: 5.46\n",
      "Epoch 3020 | Train loss: 4.89 | Val loss: 5.41\n",
      "Epoch 3030 | Train loss: 4.75 | Val loss: 5.43\n",
      "Epoch 3040 | Train loss: 4.89 | Val loss: 5.42\n",
      "Epoch 3050 | Train loss: 4.79 | Val loss: 5.38\n",
      "Epoch 3060 | Train loss: 4.79 | Val loss: 5.35\n",
      "Epoch 3070 | Train loss: 4.80 | Val loss: 5.41\n",
      "Epoch 3080 | Train loss: 4.80 | Val loss: 5.49\n",
      "Epoch 3090 | Train loss: 4.80 | Val loss: 5.48\n",
      "Epoch 3100 | Train loss: 4.75 | Val loss: 5.44\n",
      "Epoch 3110 | Train loss: 4.84 | Val loss: 5.56\n",
      "Epoch 3120 | Train loss: 4.79 | Val loss: 5.38\n",
      "Epoch 3130 | Train loss: 4.84 | Val loss: 5.37\n",
      "New best model saved with val loss: 5.24\n",
      "Epoch 3140 | Train loss: 4.76 | Val loss: 5.40\n",
      "Epoch 3150 | Train loss: 4.79 | Val loss: 5.32\n",
      "Epoch 3160 | Train loss: 4.77 | Val loss: 5.40\n",
      "Epoch 3170 | Train loss: 4.86 | Val loss: 5.39\n",
      "New best model saved with val loss: 5.22\n",
      "Epoch 3180 | Train loss: 4.78 | Val loss: 5.46\n",
      "Epoch 3190 | Train loss: 4.74 | Val loss: 5.40\n",
      "Epoch 3200 | Train loss: 4.78 | Val loss: 5.53\n",
      "Epoch 3210 | Train loss: 4.74 | Val loss: 5.52\n",
      "Epoch 3220 | Train loss: 4.77 | Val loss: 5.42\n",
      "Epoch 3230 | Train loss: 4.76 | Val loss: 5.42\n",
      "Epoch 3240 | Train loss: 4.79 | Val loss: 5.48\n",
      "Epoch 3250 | Train loss: 4.77 | Val loss: 5.37\n",
      "Epoch 3260 | Train loss: 4.74 | Val loss: 5.52\n",
      "Epoch 3270 | Train loss: 4.72 | Val loss: 5.38\n",
      "Epoch 3280 | Train loss: 4.73 | Val loss: 5.45\n",
      "Epoch 3290 | Train loss: 4.79 | Val loss: 5.57\n",
      "Epoch 3300 | Train loss: 4.74 | Val loss: 5.50\n",
      "Epoch 3310 | Train loss: 4.72 | Val loss: 5.37\n",
      "New best model saved with val loss: 5.21\n",
      "Epoch 3320 | Train loss: 4.75 | Val loss: 5.21\n",
      "Epoch 3330 | Train loss: 4.75 | Val loss: 5.38\n",
      "Epoch 3340 | Train loss: 4.75 | Val loss: 5.32\n",
      "Epoch 3350 | Train loss: 4.74 | Val loss: 5.48\n",
      "Epoch 3360 | Train loss: 4.71 | Val loss: 5.38\n",
      "Epoch 3370 | Train loss: 4.71 | Val loss: 5.41\n",
      "Epoch 3380 | Train loss: 4.74 | Val loss: 5.43\n",
      "Epoch 3390 | Train loss: 4.79 | Val loss: 5.48\n",
      "Epoch 3400 | Train loss: 4.68 | Val loss: 5.34\n",
      "Epoch 3410 | Train loss: 4.73 | Val loss: 5.48\n",
      "Epoch 3420 | Train loss: 4.73 | Val loss: 5.43\n",
      "Epoch 3430 | Train loss: 4.72 | Val loss: 5.35\n",
      "Epoch 3440 | Train loss: 4.73 | Val loss: 5.36\n",
      "New best model saved with val loss: 5.20\n",
      "Epoch 3450 | Train loss: 4.69 | Val loss: 5.36\n",
      "Epoch 3460 | Train loss: 4.69 | Val loss: 5.34\n",
      "Epoch 3470 | Train loss: 4.77 | Val loss: 5.49\n",
      "Epoch 3480 | Train loss: 4.71 | Val loss: 5.54\n",
      "Epoch 3490 | Train loss: 4.71 | Val loss: 5.46\n",
      "Epoch 3500 | Train loss: 4.69 | Val loss: 5.35\n",
      "Epoch 3510 | Train loss: 4.68 | Val loss: 5.38\n",
      "Epoch 3520 | Train loss: 4.73 | Val loss: 5.41\n",
      "Epoch 3530 | Train loss: 4.72 | Val loss: 5.35\n",
      "Epoch 3540 | Train loss: 4.74 | Val loss: 5.46\n",
      "Epoch 3550 | Train loss: 4.73 | Val loss: 5.32\n",
      "New best model saved with val loss: 5.17\n",
      "Epoch 3560 | Train loss: 4.78 | Val loss: 5.33\n",
      "Epoch 3570 | Train loss: 4.72 | Val loss: 5.48\n",
      "Epoch 3580 | Train loss: 4.74 | Val loss: 5.38\n",
      "Epoch 3590 | Train loss: 4.66 | Val loss: 5.45\n",
      "Epoch 3600 | Train loss: 4.64 | Val loss: 5.32\n",
      "Epoch 3610 | Train loss: 4.76 | Val loss: 5.27\n",
      "Epoch 3620 | Train loss: 4.62 | Val loss: 5.46\n",
      "Epoch 3630 | Train loss: 4.68 | Val loss: 5.44\n",
      "Epoch 3640 | Train loss: 4.71 | Val loss: 5.25\n",
      "Epoch 3650 | Train loss: 4.65 | Val loss: 5.37\n",
      "Epoch 3660 | Train loss: 4.70 | Val loss: 5.48\n",
      "Epoch 3670 | Train loss: 4.68 | Val loss: 5.33\n",
      "Epoch 3680 | Train loss: 4.70 | Val loss: 5.43\n",
      "Epoch 3690 | Train loss: 4.68 | Val loss: 5.45\n",
      "Epoch 3700 | Train loss: 4.75 | Val loss: 5.33\n",
      "Epoch 3710 | Train loss: 4.59 | Val loss: 5.41\n",
      "Epoch 3720 | Train loss: 4.64 | Val loss: 5.36\n",
      "Epoch 3730 | Train loss: 4.62 | Val loss: 5.37\n",
      "Epoch 3740 | Train loss: 4.67 | Val loss: 5.38\n",
      "Epoch 3750 | Train loss: 4.67 | Val loss: 5.43\n",
      "Epoch 3760 | Train loss: 4.64 | Val loss: 5.31\n",
      "Epoch 3770 | Train loss: 4.65 | Val loss: 5.45\n",
      "Epoch 3780 | Train loss: 4.67 | Val loss: 5.33\n",
      "Epoch 3790 | Train loss: 4.65 | Val loss: 5.56\n",
      "Epoch 3800 | Train loss: 4.66 | Val loss: 5.41\n",
      "New best model saved with val loss: 5.15\n",
      "Epoch 3810 | Train loss: 4.67 | Val loss: 5.15\n",
      "Epoch 3820 | Train loss: 4.68 | Val loss: 5.25\n",
      "Epoch 3830 | Train loss: 4.66 | Val loss: 5.40\n",
      "Epoch 3840 | Train loss: 4.66 | Val loss: 5.42\n",
      "Epoch 3850 | Train loss: 4.65 | Val loss: 5.50\n",
      "Epoch 3860 | Train loss: 4.65 | Val loss: 5.42\n",
      "Epoch 3870 | Train loss: 4.66 | Val loss: 5.42\n",
      "Epoch 3880 | Train loss: 4.63 | Val loss: 5.37\n",
      "Epoch 3890 | Train loss: 4.61 | Val loss: 5.29\n",
      "Epoch 3900 | Train loss: 4.64 | Val loss: 5.46\n",
      "Epoch 3910 | Train loss: 4.64 | Val loss: 5.39\n",
      "Epoch 3920 | Train loss: 4.63 | Val loss: 5.37\n",
      "Epoch 3930 | Train loss: 4.66 | Val loss: 5.34\n",
      "Epoch 3940 | Train loss: 4.60 | Val loss: 5.33\n",
      "Epoch 3950 | Train loss: 4.67 | Val loss: 5.35\n",
      "Epoch 3960 | Train loss: 4.63 | Val loss: 5.37\n",
      "Epoch 3970 | Train loss: 4.66 | Val loss: 5.27\n",
      "Epoch 3980 | Train loss: 4.66 | Val loss: 5.32\n",
      "Epoch 3990 | Train loss: 4.61 | Val loss: 5.34\n",
      "Epoch 4000 | Train loss: 4.62 | Val loss: 5.44\n",
      "Epoch 4010 | Train loss: 4.61 | Val loss: 5.33\n",
      "Epoch 4020 | Train loss: 4.68 | Val loss: 5.38\n",
      "Epoch 4030 | Train loss: 4.60 | Val loss: 5.40\n",
      "Epoch 4040 | Train loss: 4.66 | Val loss: 5.40\n",
      "Epoch 4050 | Train loss: 4.56 | Val loss: 5.24\n",
      "Epoch 4060 | Train loss: 4.69 | Val loss: 5.43\n",
      "Epoch 4070 | Train loss: 4.62 | Val loss: 5.31\n",
      "Epoch 4080 | Train loss: 4.62 | Val loss: 5.47\n",
      "Epoch 4090 | Train loss: 4.59 | Val loss: 5.42\n",
      "Epoch 4100 | Train loss: 4.60 | Val loss: 5.51\n",
      "Epoch 4110 | Train loss: 4.63 | Val loss: 5.36\n",
      "Epoch 4120 | Train loss: 4.59 | Val loss: 5.39\n",
      "Epoch 4130 | Train loss: 4.63 | Val loss: 5.36\n",
      "Epoch 4140 | Train loss: 4.61 | Val loss: 5.41\n",
      "Epoch 4150 | Train loss: 4.63 | Val loss: 5.46\n",
      "Epoch 4160 | Train loss: 4.63 | Val loss: 5.38\n",
      "Epoch 4170 | Train loss: 4.63 | Val loss: 5.33\n",
      "Epoch 4180 | Train loss: 4.64 | Val loss: 5.26\n",
      "Epoch 4190 | Train loss: 4.67 | Val loss: 5.35\n",
      "Epoch 4200 | Train loss: 4.65 | Val loss: 5.21\n",
      "Epoch 4210 | Train loss: 4.62 | Val loss: 5.44\n",
      "Epoch 4220 | Train loss: 4.58 | Val loss: 5.34\n",
      "Epoch 4230 | Train loss: 4.61 | Val loss: 5.38\n",
      "Epoch 4240 | Train loss: 4.61 | Val loss: 5.42\n",
      "Epoch 4250 | Train loss: 4.62 | Val loss: 5.36\n",
      "Epoch 4260 | Train loss: 4.59 | Val loss: 5.38\n",
      "Epoch 4270 | Train loss: 4.62 | Val loss: 5.40\n",
      "Epoch 4280 | Train loss: 4.67 | Val loss: 5.48\n",
      "Epoch 4290 | Train loss: 4.61 | Val loss: 5.33\n",
      "Epoch 4300 | Train loss: 4.58 | Val loss: 5.36\n",
      "Epoch 4310 | Train loss: 4.56 | Val loss: 5.42\n",
      "Epoch 4320 | Train loss: 4.59 | Val loss: 5.35\n",
      "Epoch 4330 | Train loss: 4.57 | Val loss: 5.53\n",
      "Epoch 4340 | Train loss: 4.64 | Val loss: 5.34\n",
      "Epoch 4350 | Train loss: 4.61 | Val loss: 5.31\n",
      "Epoch 4360 | Train loss: 4.53 | Val loss: 5.41\n",
      "Epoch 4370 | Train loss: 4.57 | Val loss: 5.43\n",
      "New best model saved with val loss: 5.14\n",
      "Epoch 4380 | Train loss: 4.61 | Val loss: 5.31\n",
      "Epoch 4390 | Train loss: 4.57 | Val loss: 5.37\n",
      "Epoch 4400 | Train loss: 4.56 | Val loss: 5.39\n",
      "Epoch 4410 | Train loss: 4.58 | Val loss: 5.33\n",
      "Epoch 4420 | Train loss: 4.58 | Val loss: 5.44\n",
      "Epoch 4430 | Train loss: 4.58 | Val loss: 5.30\n",
      "Epoch 4440 | Train loss: 4.58 | Val loss: 5.32\n",
      "Epoch 4450 | Train loss: 4.52 | Val loss: 5.27\n",
      "Epoch 4460 | Train loss: 4.55 | Val loss: 5.32\n",
      "Epoch 4470 | Train loss: 4.56 | Val loss: 5.25\n",
      "New best model saved with val loss: 5.12\n",
      "Epoch 4480 | Train loss: 4.54 | Val loss: 5.22\n",
      "Epoch 4490 | Train loss: 4.60 | Val loss: 5.34\n",
      "Epoch 4500 | Train loss: 4.63 | Val loss: 5.37\n",
      "Epoch 4510 | Train loss: 4.56 | Val loss: 5.33\n",
      "Epoch 4520 | Train loss: 4.55 | Val loss: 5.33\n",
      "Epoch 4530 | Train loss: 4.65 | Val loss: 5.35\n",
      "Epoch 4540 | Train loss: 4.58 | Val loss: 5.38\n",
      "Epoch 4550 | Train loss: 4.62 | Val loss: 5.29\n",
      "Epoch 4560 | Train loss: 4.57 | Val loss: 5.27\n",
      "Epoch 4570 | Train loss: 4.57 | Val loss: 5.37\n",
      "Epoch 4580 | Train loss: 4.50 | Val loss: 5.32\n",
      "Epoch 4590 | Train loss: 4.57 | Val loss: 5.35\n",
      "Epoch 4600 | Train loss: 4.57 | Val loss: 5.38\n",
      "Epoch 4610 | Train loss: 4.55 | Val loss: 5.14\n",
      "Epoch 4620 | Train loss: 4.57 | Val loss: 5.29\n",
      "Epoch 4630 | Train loss: 4.54 | Val loss: 5.32\n",
      "Epoch 4640 | Train loss: 4.57 | Val loss: 5.49\n",
      "Epoch 4650 | Train loss: 4.55 | Val loss: 5.35\n",
      "Epoch 4660 | Train loss: 4.56 | Val loss: 5.31\n",
      "Epoch 4670 | Train loss: 4.54 | Val loss: 5.39\n",
      "Epoch 4680 | Train loss: 4.56 | Val loss: 5.48\n",
      "Epoch 4690 | Train loss: 4.59 | Val loss: 5.21\n",
      "Epoch 4700 | Train loss: 4.53 | Val loss: 5.35\n",
      "Epoch 4710 | Train loss: 4.55 | Val loss: 5.27\n",
      "Epoch 4720 | Train loss: 4.56 | Val loss: 5.24\n",
      "Epoch 4730 | Train loss: 4.50 | Val loss: 5.39\n",
      "Epoch 4740 | Train loss: 4.55 | Val loss: 5.24\n",
      "Epoch 4750 | Train loss: 4.53 | Val loss: 5.30\n",
      "Epoch 4760 | Train loss: 4.51 | Val loss: 5.32\n",
      "Epoch 4770 | Train loss: 4.55 | Val loss: 5.40\n",
      "Epoch 4780 | Train loss: 4.53 | Val loss: 5.31\n",
      "Epoch 4790 | Train loss: 4.57 | Val loss: 5.29\n",
      "Epoch 4800 | Train loss: 4.53 | Val loss: 5.29\n",
      "Epoch 4810 | Train loss: 4.52 | Val loss: 5.36\n",
      "Epoch 4820 | Train loss: 4.56 | Val loss: 5.28\n",
      "Epoch 4830 | Train loss: 4.52 | Val loss: 5.28\n",
      "Epoch 4840 | Train loss: 4.45 | Val loss: 5.37\n",
      "Epoch 4850 | Train loss: 4.52 | Val loss: 5.30\n",
      "Epoch 4860 | Train loss: 4.52 | Val loss: 5.26\n",
      "Epoch 4870 | Train loss: 4.52 | Val loss: 5.21\n",
      "Epoch 4880 | Train loss: 4.50 | Val loss: 5.21\n",
      "Epoch 4890 | Train loss: 4.52 | Val loss: 5.18\n",
      "Epoch 4900 | Train loss: 4.57 | Val loss: 5.35\n",
      "Epoch 4910 | Train loss: 4.54 | Val loss: 5.49\n",
      "Epoch 4920 | Train loss: 4.62 | Val loss: 5.38\n",
      "Epoch 4930 | Train loss: 4.51 | Val loss: 5.36\n",
      "Epoch 4940 | Train loss: 4.55 | Val loss: 5.39\n",
      "Epoch 4950 | Train loss: 4.54 | Val loss: 5.32\n",
      "Epoch 4960 | Train loss: 4.53 | Val loss: 5.31\n",
      "Epoch 4970 | Train loss: 4.53 | Val loss: 5.39\n",
      "Epoch 4980 | Train loss: 4.55 | Val loss: 5.41\n",
      "Epoch 4990 | Train loss: 4.56 | Val loss: 5.24\n",
      "Epoch 5000 | Train loss: 4.51 | Val loss: 5.21\n",
      "Epoch 5010 | Train loss: 4.55 | Val loss: 5.32\n",
      "New best model saved with val loss: 5.11\n",
      "Epoch 5020 | Train loss: 4.49 | Val loss: 5.33\n",
      "Epoch 5030 | Train loss: 4.56 | Val loss: 5.35\n",
      "Epoch 5040 | Train loss: 4.51 | Val loss: 5.35\n",
      "Epoch 5050 | Train loss: 4.50 | Val loss: 5.31\n",
      "Epoch 5060 | Train loss: 4.50 | Val loss: 5.30\n",
      "Epoch 5070 | Train loss: 4.59 | Val loss: 5.37\n",
      "Epoch 5080 | Train loss: 4.53 | Val loss: 5.23\n",
      "New best model saved with val loss: 5.08\n",
      "Epoch 5090 | Train loss: 4.44 | Val loss: 5.32\n",
      "Epoch 5100 | Train loss: 4.48 | Val loss: 5.26\n",
      "Epoch 5110 | Train loss: 4.49 | Val loss: 5.32\n",
      "Epoch 5120 | Train loss: 4.51 | Val loss: 5.47\n",
      "Epoch 5130 | Train loss: 4.49 | Val loss: 5.52\n",
      "Epoch 5140 | Train loss: 4.56 | Val loss: 5.18\n",
      "Epoch 5150 | Train loss: 4.50 | Val loss: 5.35\n",
      "Epoch 5160 | Train loss: 4.53 | Val loss: 5.36\n",
      "Epoch 5170 | Train loss: 4.52 | Val loss: 5.17\n",
      "Epoch 5180 | Train loss: 4.46 | Val loss: 5.43\n",
      "Epoch 5190 | Train loss: 4.51 | Val loss: 5.32\n",
      "Epoch 5200 | Train loss: 4.54 | Val loss: 5.28\n",
      "Epoch 5210 | Train loss: 4.41 | Val loss: 5.29\n",
      "Epoch 5220 | Train loss: 4.49 | Val loss: 5.24\n",
      "Epoch 5230 | Train loss: 4.48 | Val loss: 5.29\n",
      "Epoch 5240 | Train loss: 4.54 | Val loss: 5.31\n",
      "Epoch 5250 | Train loss: 4.51 | Val loss: 5.30\n",
      "Epoch 5260 | Train loss: 4.52 | Val loss: 5.17\n",
      "Epoch 5270 | Train loss: 4.48 | Val loss: 5.32\n",
      "Epoch 5280 | Train loss: 4.48 | Val loss: 5.39\n",
      "Epoch 5290 | Train loss: 4.51 | Val loss: 5.24\n",
      "Epoch 5300 | Train loss: 4.52 | Val loss: 5.38\n",
      "Epoch 5310 | Train loss: 4.50 | Val loss: 5.22\n",
      "Epoch 5320 | Train loss: 4.48 | Val loss: 5.33\n",
      "Epoch 5330 | Train loss: 4.52 | Val loss: 5.38\n",
      "Epoch 5340 | Train loss: 4.55 | Val loss: 5.32\n",
      "Epoch 5350 | Train loss: 4.45 | Val loss: 5.53\n",
      "Epoch 5360 | Train loss: 4.47 | Val loss: 5.41\n",
      "Epoch 5370 | Train loss: 4.49 | Val loss: 5.31\n",
      "Epoch 5380 | Train loss: 4.51 | Val loss: 5.29\n",
      "Epoch 5390 | Train loss: 4.49 | Val loss: 5.37\n",
      "Epoch 5400 | Train loss: 4.48 | Val loss: 5.27\n",
      "Epoch 5410 | Train loss: 4.44 | Val loss: 5.32\n",
      "Epoch 5420 | Train loss: 4.55 | Val loss: 5.35\n",
      "Epoch 5430 | Train loss: 4.45 | Val loss: 5.36\n",
      "Epoch 5440 | Train loss: 4.49 | Val loss: 5.23\n",
      "Epoch 5450 | Train loss: 4.45 | Val loss: 5.21\n",
      "Epoch 5460 | Train loss: 4.49 | Val loss: 5.18\n",
      "Epoch 5470 | Train loss: 4.43 | Val loss: 5.31\n",
      "Epoch 5480 | Train loss: 4.53 | Val loss: 5.41\n",
      "Epoch 5490 | Train loss: 4.50 | Val loss: 5.30\n",
      "Epoch 5500 | Train loss: 4.46 | Val loss: 5.35\n",
      "Epoch 5510 | Train loss: 4.50 | Val loss: 5.28\n",
      "Epoch 5520 | Train loss: 4.50 | Val loss: 5.37\n",
      "Epoch 5530 | Train loss: 4.38 | Val loss: 5.33\n",
      "Epoch 5540 | Train loss: 4.46 | Val loss: 5.26\n",
      "Epoch 5550 | Train loss: 4.45 | Val loss: 5.38\n",
      "Epoch 5560 | Train loss: 4.43 | Val loss: 5.29\n",
      "Epoch 5570 | Train loss: 4.45 | Val loss: 5.24\n",
      "Epoch 5580 | Train loss: 4.46 | Val loss: 5.31\n",
      "Epoch 5590 | Train loss: 4.44 | Val loss: 5.29\n",
      "Epoch 5600 | Train loss: 4.48 | Val loss: 5.31\n",
      "Epoch 5610 | Train loss: 4.44 | Val loss: 5.29\n",
      "Epoch 5620 | Train loss: 4.47 | Val loss: 5.32\n",
      "Epoch 5630 | Train loss: 4.54 | Val loss: 5.24\n",
      "Epoch 5640 | Train loss: 4.46 | Val loss: 5.28\n",
      "Epoch 5650 | Train loss: 4.50 | Val loss: 5.17\n",
      "Epoch 5660 | Train loss: 4.46 | Val loss: 5.27\n",
      "Epoch 5670 | Train loss: 4.50 | Val loss: 5.36\n",
      "Epoch 5680 | Train loss: 4.42 | Val loss: 5.36\n",
      "Epoch 5690 | Train loss: 4.42 | Val loss: 5.31\n",
      "Epoch 5700 | Train loss: 4.52 | Val loss: 5.34\n",
      "Epoch 5710 | Train loss: 4.42 | Val loss: 5.23\n",
      "Epoch 5720 | Train loss: 4.45 | Val loss: 5.28\n",
      "Epoch 5730 | Train loss: 4.47 | Val loss: 5.33\n",
      "Epoch 5740 | Train loss: 4.46 | Val loss: 5.29\n",
      "Epoch 5750 | Train loss: 4.42 | Val loss: 5.19\n",
      "Epoch 5760 | Train loss: 4.49 | Val loss: 5.38\n",
      "Epoch 5770 | Train loss: 4.50 | Val loss: 5.25\n",
      "Epoch 5780 | Train loss: 4.41 | Val loss: 5.31\n",
      "Epoch 5790 | Train loss: 4.44 | Val loss: 5.37\n",
      "Epoch 5800 | Train loss: 4.50 | Val loss: 5.29\n",
      "Epoch 5810 | Train loss: 4.46 | Val loss: 5.29\n",
      "Epoch 5820 | Train loss: 4.49 | Val loss: 5.35\n",
      "Epoch 5830 | Train loss: 4.44 | Val loss: 5.32\n",
      "Epoch 5840 | Train loss: 4.47 | Val loss: 5.33\n",
      "Epoch 5850 | Train loss: 4.45 | Val loss: 5.23\n",
      "Epoch 5860 | Train loss: 4.43 | Val loss: 5.35\n",
      "Epoch 5870 | Train loss: 4.45 | Val loss: 5.30\n",
      "Epoch 5880 | Train loss: 4.45 | Val loss: 5.24\n",
      "Epoch 5890 | Train loss: 4.45 | Val loss: 5.34\n",
      "Epoch 5900 | Train loss: 4.42 | Val loss: 5.32\n",
      "Epoch 5910 | Train loss: 4.40 | Val loss: 5.19\n",
      "Epoch 5920 | Train loss: 4.44 | Val loss: 5.20\n",
      "Epoch 5930 | Train loss: 4.45 | Val loss: 5.42\n",
      "Epoch 5940 | Train loss: 4.49 | Val loss: 5.39\n",
      "Epoch 5950 | Train loss: 4.48 | Val loss: 5.32\n",
      "Epoch 5960 | Train loss: 4.43 | Val loss: 5.38\n",
      "Epoch 5970 | Train loss: 4.47 | Val loss: 5.40\n",
      "Epoch 5980 | Train loss: 4.48 | Val loss: 5.29\n",
      "Epoch 5990 | Train loss: 4.50 | Val loss: 5.26\n",
      "Epoch 6000 | Train loss: 4.43 | Val loss: 5.25\n",
      "Epoch 6010 | Train loss: 4.47 | Val loss: 5.27\n",
      "Epoch 6020 | Train loss: 4.45 | Val loss: 5.42\n",
      "Epoch 6030 | Train loss: 4.47 | Val loss: 5.25\n",
      "Epoch 6040 | Train loss: 4.49 | Val loss: 5.29\n",
      "Epoch 6050 | Train loss: 4.43 | Val loss: 5.21\n",
      "Epoch 6060 | Train loss: 4.42 | Val loss: 5.30\n",
      "Epoch 6070 | Train loss: 4.43 | Val loss: 5.31\n",
      "Epoch 6080 | Train loss: 4.42 | Val loss: 5.30\n",
      "Epoch 6090 | Train loss: 4.42 | Val loss: 5.22\n",
      "Epoch 6100 | Train loss: 4.47 | Val loss: 5.36\n",
      "Epoch 6110 | Train loss: 4.42 | Val loss: 5.11\n",
      "Epoch 6120 | Train loss: 4.47 | Val loss: 5.28\n",
      "Epoch 6130 | Train loss: 4.41 | Val loss: 5.13\n",
      "Epoch 6140 | Train loss: 4.43 | Val loss: 5.43\n",
      "Epoch 6150 | Train loss: 4.37 | Val loss: 5.28\n",
      "Epoch 6160 | Train loss: 4.46 | Val loss: 5.41\n",
      "Epoch 6170 | Train loss: 4.43 | Val loss: 5.16\n",
      "Epoch 6180 | Train loss: 4.45 | Val loss: 5.42\n",
      "Epoch 6190 | Train loss: 4.41 | Val loss: 5.23\n",
      "Epoch 6200 | Train loss: 4.37 | Val loss: 5.34\n",
      "Epoch 6210 | Train loss: 4.44 | Val loss: 5.40\n",
      "Epoch 6220 | Train loss: 4.40 | Val loss: 5.32\n",
      "Epoch 6230 | Train loss: 4.44 | Val loss: 5.34\n",
      "Epoch 6240 | Train loss: 4.41 | Val loss: 5.29\n",
      "Epoch 6250 | Train loss: 4.43 | Val loss: 5.28\n",
      "Epoch 6260 | Train loss: 4.42 | Val loss: 5.23\n",
      "Epoch 6270 | Train loss: 4.49 | Val loss: 5.41\n",
      "Epoch 6280 | Train loss: 4.37 | Val loss: 5.22\n",
      "Epoch 6290 | Train loss: 4.47 | Val loss: 5.26\n",
      "Epoch 6300 | Train loss: 4.41 | Val loss: 5.31\n",
      "Epoch 6310 | Train loss: 4.43 | Val loss: 5.42\n",
      "Epoch 6320 | Train loss: 4.45 | Val loss: 5.37\n",
      "Epoch 6330 | Train loss: 4.42 | Val loss: 5.32\n",
      "Epoch 6340 | Train loss: 4.41 | Val loss: 5.40\n",
      "Epoch 6350 | Train loss: 4.42 | Val loss: 5.28\n",
      "Epoch 6360 | Train loss: 4.43 | Val loss: 5.10\n",
      "Epoch 6370 | Train loss: 4.36 | Val loss: 5.32\n",
      "Epoch 6380 | Train loss: 4.41 | Val loss: 5.27\n",
      "Epoch 6390 | Train loss: 4.41 | Val loss: 5.28\n",
      "Epoch 6400 | Train loss: 4.47 | Val loss: 5.19\n",
      "Epoch 6410 | Train loss: 4.45 | Val loss: 5.30\n",
      "New best model saved with val loss: 5.06\n",
      "Epoch 6420 | Train loss: 4.37 | Val loss: 5.20\n",
      "Epoch 6430 | Train loss: 4.43 | Val loss: 5.32\n",
      "Epoch 6440 | Train loss: 4.41 | Val loss: 5.16\n",
      "Epoch 6450 | Train loss: 4.42 | Val loss: 5.33\n",
      "Epoch 6460 | Train loss: 4.44 | Val loss: 5.35\n",
      "Epoch 6470 | Train loss: 4.45 | Val loss: 5.27\n",
      "Epoch 6480 | Train loss: 4.40 | Val loss: 5.33\n",
      "Epoch 6490 | Train loss: 4.45 | Val loss: 5.39\n",
      "Epoch 6500 | Train loss: 4.42 | Val loss: 5.40\n",
      "Epoch 6510 | Train loss: 4.36 | Val loss: 5.33\n",
      "Epoch 6520 | Train loss: 4.46 | Val loss: 5.31\n",
      "Epoch 6530 | Train loss: 4.41 | Val loss: 5.32\n",
      "Epoch 6540 | Train loss: 4.42 | Val loss: 5.35\n",
      "Epoch 6550 | Train loss: 4.39 | Val loss: 5.36\n",
      "Epoch 6560 | Train loss: 4.42 | Val loss: 5.30\n",
      "Epoch 6570 | Train loss: 4.42 | Val loss: 5.38\n",
      "Epoch 6580 | Train loss: 4.39 | Val loss: 5.41\n",
      "Epoch 6590 | Train loss: 4.37 | Val loss: 5.38\n",
      "Epoch 6600 | Train loss: 4.43 | Val loss: 5.15\n",
      "Epoch 6610 | Train loss: 4.39 | Val loss: 5.17\n",
      "Epoch 6620 | Train loss: 4.41 | Val loss: 5.39\n",
      "Epoch 6630 | Train loss: 4.41 | Val loss: 5.45\n",
      "Epoch 6640 | Train loss: 4.41 | Val loss: 5.35\n",
      "Epoch 6650 | Train loss: 4.38 | Val loss: 5.28\n",
      "Epoch 6660 | Train loss: 4.35 | Val loss: 5.31\n",
      "Epoch 6670 | Train loss: 4.40 | Val loss: 5.27\n",
      "Epoch 6680 | Train loss: 4.41 | Val loss: 5.34\n",
      "Epoch 6690 | Train loss: 4.32 | Val loss: 5.42\n",
      "Epoch 6700 | Train loss: 4.38 | Val loss: 5.33\n",
      "Epoch 6710 | Train loss: 4.43 | Val loss: 5.35\n",
      "Epoch 6720 | Train loss: 4.36 | Val loss: 5.26\n",
      "Epoch 6730 | Train loss: 4.41 | Val loss: 5.20\n",
      "Epoch 6740 | Train loss: 4.41 | Val loss: 5.29\n",
      "Epoch 6750 | Train loss: 4.37 | Val loss: 5.32\n",
      "Epoch 6760 | Train loss: 4.33 | Val loss: 5.20\n",
      "Epoch 6770 | Train loss: 4.42 | Val loss: 5.23\n",
      "Epoch 6780 | Train loss: 4.39 | Val loss: 5.47\n",
      "Epoch 6790 | Train loss: 4.37 | Val loss: 5.33\n",
      "Epoch 6800 | Train loss: 4.37 | Val loss: 5.30\n",
      "Epoch 6810 | Train loss: 4.37 | Val loss: 5.39\n",
      "Epoch 6820 | Train loss: 4.36 | Val loss: 5.34\n",
      "Epoch 6830 | Train loss: 4.42 | Val loss: 5.26\n",
      "Epoch 6840 | Train loss: 4.39 | Val loss: 5.23\n",
      "Epoch 6850 | Train loss: 4.37 | Val loss: 5.41\n",
      "Epoch 6860 | Train loss: 4.40 | Val loss: 5.19\n",
      "Epoch 6870 | Train loss: 4.39 | Val loss: 5.17\n",
      "Epoch 6880 | Train loss: 4.40 | Val loss: 5.35\n",
      "Epoch 6890 | Train loss: 4.45 | Val loss: 5.22\n",
      "Epoch 6900 | Train loss: 4.42 | Val loss: 5.31\n",
      "Epoch 6910 | Train loss: 4.36 | Val loss: 5.34\n",
      "Epoch 6920 | Train loss: 4.39 | Val loss: 5.35\n",
      "Epoch 6930 | Train loss: 4.44 | Val loss: 5.41\n",
      "Epoch 6940 | Train loss: 4.41 | Val loss: 5.25\n",
      "Epoch 6950 | Train loss: 4.41 | Val loss: 5.28\n",
      "Epoch 6960 | Train loss: 4.33 | Val loss: 5.36\n",
      "Epoch 6970 | Train loss: 4.38 | Val loss: 5.27\n",
      "Epoch 6980 | Train loss: 4.42 | Val loss: 5.26\n",
      "Epoch 6990 | Train loss: 4.42 | Val loss: 5.29\n",
      "Epoch 7000 | Train loss: 4.43 | Val loss: 5.31\n",
      "Epoch 7010 | Train loss: 4.39 | Val loss: 5.30\n",
      "Epoch 7020 | Train loss: 4.44 | Val loss: 5.24\n",
      "Epoch 7030 | Train loss: 4.38 | Val loss: 5.24\n",
      "Epoch 7040 | Train loss: 4.40 | Val loss: 5.28\n",
      "Epoch 7050 | Train loss: 4.34 | Val loss: 5.31\n",
      "Epoch 7060 | Train loss: 4.36 | Val loss: 5.36\n",
      "Epoch 7070 | Train loss: 4.37 | Val loss: 5.32\n",
      "Epoch 7080 | Train loss: 4.38 | Val loss: 5.32\n",
      "Epoch 7090 | Train loss: 4.35 | Val loss: 5.42\n",
      "Epoch 7100 | Train loss: 4.43 | Val loss: 5.39\n",
      "Epoch 7110 | Train loss: 4.39 | Val loss: 5.26\n",
      "Epoch 7120 | Train loss: 4.41 | Val loss: 5.30\n",
      "Epoch 7130 | Train loss: 4.38 | Val loss: 5.32\n",
      "Epoch 7140 | Train loss: 4.39 | Val loss: 5.44\n",
      "Epoch 7150 | Train loss: 4.38 | Val loss: 5.25\n",
      "Epoch 7160 | Train loss: 4.36 | Val loss: 5.32\n",
      "Epoch 7170 | Train loss: 4.39 | Val loss: 5.11\n",
      "Epoch 7180 | Train loss: 4.40 | Val loss: 5.41\n",
      "Epoch 7190 | Train loss: 4.37 | Val loss: 5.36\n",
      "Epoch 7200 | Train loss: 4.39 | Val loss: 5.43\n",
      "Epoch 7210 | Train loss: 4.38 | Val loss: 5.24\n",
      "Epoch 7220 | Train loss: 4.40 | Val loss: 5.21\n",
      "Epoch 7230 | Train loss: 4.38 | Val loss: 5.30\n",
      "Epoch 7240 | Train loss: 4.39 | Val loss: 5.28\n",
      "Epoch 7250 | Train loss: 4.38 | Val loss: 5.35\n",
      "Epoch 7260 | Train loss: 4.37 | Val loss: 5.20\n",
      "Epoch 7270 | Train loss: 4.38 | Val loss: 5.34\n",
      "Epoch 7280 | Train loss: 4.39 | Val loss: 5.31\n",
      "Epoch 7290 | Train loss: 4.39 | Val loss: 5.38\n",
      "Epoch 7300 | Train loss: 4.38 | Val loss: 5.21\n",
      "Epoch 7310 | Train loss: 4.42 | Val loss: 5.27\n",
      "Epoch 7320 | Train loss: 4.41 | Val loss: 5.32\n",
      "Epoch 7330 | Train loss: 4.42 | Val loss: 5.28\n",
      "Epoch 7340 | Train loss: 4.34 | Val loss: 5.26\n",
      "Epoch 7350 | Train loss: 4.39 | Val loss: 5.21\n",
      "Epoch 7360 | Train loss: 4.42 | Val loss: 5.27\n",
      "Epoch 7370 | Train loss: 4.33 | Val loss: 5.26\n",
      "Epoch 7380 | Train loss: 4.40 | Val loss: 5.31\n",
      "Epoch 7390 | Train loss: 4.41 | Val loss: 5.57\n",
      "Epoch 7400 | Train loss: 4.37 | Val loss: 5.24\n",
      "Epoch 7410 | Train loss: 4.38 | Val loss: 5.25\n",
      "Epoch 7420 | Train loss: 4.42 | Val loss: 5.38\n",
      "Epoch 7430 | Train loss: 4.38 | Val loss: 5.20\n",
      "Epoch 7440 | Train loss: 4.46 | Val loss: 5.36\n",
      "Epoch 7450 | Train loss: 4.44 | Val loss: 5.30\n",
      "Epoch 7460 | Train loss: 4.40 | Val loss: 5.26\n",
      "Epoch 7470 | Train loss: 4.30 | Val loss: 5.30\n",
      "Epoch 7480 | Train loss: 4.33 | Val loss: 5.38\n",
      "Epoch 7490 | Train loss: 4.32 | Val loss: 5.25\n",
      "Epoch 7500 | Train loss: 4.37 | Val loss: 5.36\n",
      "Epoch 7510 | Train loss: 4.42 | Val loss: 5.40\n",
      "Epoch 7520 | Train loss: 4.37 | Val loss: 5.30\n",
      "Epoch 7530 | Train loss: 4.38 | Val loss: 5.22\n",
      "Epoch 7540 | Train loss: 4.36 | Val loss: 5.22\n",
      "Epoch 7550 | Train loss: 4.32 | Val loss: 5.19\n",
      "Epoch 7560 | Train loss: 4.39 | Val loss: 5.36\n",
      "Epoch 7570 | Train loss: 4.39 | Val loss: 5.33\n",
      "Epoch 7580 | Train loss: 4.36 | Val loss: 5.26\n",
      "Epoch 7590 | Train loss: 4.34 | Val loss: 5.46\n",
      "Epoch 7600 | Train loss: 4.36 | Val loss: 5.27\n",
      "Epoch 7610 | Train loss: 4.35 | Val loss: 5.32\n",
      "Epoch 7620 | Train loss: 4.33 | Val loss: 5.41\n",
      "Epoch 7630 | Train loss: 4.36 | Val loss: 5.26\n",
      "Epoch 7640 | Train loss: 4.36 | Val loss: 5.44\n",
      "Epoch 7650 | Train loss: 4.32 | Val loss: 5.22\n",
      "Epoch 7660 | Train loss: 4.35 | Val loss: 5.31\n",
      "Epoch 7670 | Train loss: 4.40 | Val loss: 5.28\n",
      "Epoch 7680 | Train loss: 4.40 | Val loss: 5.43\n",
      "Epoch 7690 | Train loss: 4.39 | Val loss: 5.40\n",
      "Epoch 7700 | Train loss: 4.35 | Val loss: 5.32\n",
      "Epoch 7710 | Train loss: 4.35 | Val loss: 5.44\n",
      "Epoch 7720 | Train loss: 4.31 | Val loss: 5.22\n",
      "Epoch 7730 | Train loss: 4.33 | Val loss: 5.30\n",
      "Epoch 7740 | Train loss: 4.35 | Val loss: 5.26\n",
      "Epoch 7750 | Train loss: 4.35 | Val loss: 5.30\n",
      "Epoch 7760 | Train loss: 4.34 | Val loss: 5.16\n",
      "Epoch 7770 | Train loss: 4.40 | Val loss: 5.30\n",
      "Epoch 7780 | Train loss: 4.32 | Val loss: 5.23\n",
      "Epoch 7790 | Train loss: 4.39 | Val loss: 5.28\n",
      "Epoch 7800 | Train loss: 4.34 | Val loss: 5.17\n",
      "Epoch 7810 | Train loss: 4.37 | Val loss: 5.40\n",
      "Epoch 7820 | Train loss: 4.38 | Val loss: 5.29\n",
      "Epoch 7830 | Train loss: 4.34 | Val loss: 5.43\n",
      "Epoch 7840 | Train loss: 4.36 | Val loss: 5.33\n",
      "Epoch 7850 | Train loss: 4.36 | Val loss: 5.29\n",
      "Epoch 7860 | Train loss: 4.39 | Val loss: 5.30\n",
      "Epoch 7870 | Train loss: 4.34 | Val loss: 5.12\n",
      "Epoch 7880 | Train loss: 4.34 | Val loss: 5.23\n",
      "Epoch 7890 | Train loss: 4.39 | Val loss: 5.36\n",
      "Epoch 7900 | Train loss: 4.35 | Val loss: 5.12\n",
      "Epoch 7910 | Train loss: 4.34 | Val loss: 5.24\n",
      "Epoch 7920 | Train loss: 4.35 | Val loss: 5.39\n",
      "Epoch 7930 | Train loss: 4.37 | Val loss: 5.35\n",
      "New best model saved with val loss: 5.06\n",
      "Epoch 7940 | Train loss: 4.34 | Val loss: 5.16\n",
      "Epoch 7950 | Train loss: 4.35 | Val loss: 5.26\n",
      "Epoch 7960 | Train loss: 4.34 | Val loss: 5.28\n",
      "Epoch 7970 | Train loss: 4.35 | Val loss: 5.35\n",
      "Epoch 7980 | Train loss: 4.35 | Val loss: 5.31\n",
      "Epoch 7990 | Train loss: 4.32 | Val loss: 5.30\n",
      "Epoch 8000 | Train loss: 4.37 | Val loss: 5.48\n",
      "Epoch 8010 | Train loss: 4.40 | Val loss: 5.17\n",
      "Epoch 8020 | Train loss: 4.37 | Val loss: 5.28\n",
      "Epoch 8030 | Train loss: 4.33 | Val loss: 5.40\n",
      "Epoch 8040 | Train loss: 4.40 | Val loss: 5.39\n",
      "Epoch 8050 | Train loss: 4.33 | Val loss: 5.51\n",
      "Epoch 8060 | Train loss: 4.36 | Val loss: 5.22\n",
      "Epoch 8070 | Train loss: 4.36 | Val loss: 5.38\n",
      "Epoch 8080 | Train loss: 4.42 | Val loss: 5.35\n",
      "Epoch 8090 | Train loss: 4.30 | Val loss: 5.24\n",
      "New best model saved with val loss: 5.06\n",
      "Epoch 8100 | Train loss: 4.35 | Val loss: 5.25\n",
      "Epoch 8110 | Train loss: 4.34 | Val loss: 5.37\n",
      "Epoch 8120 | Train loss: 4.36 | Val loss: 5.24\n",
      "Epoch 8130 | Train loss: 4.33 | Val loss: 5.28\n",
      "Epoch 8140 | Train loss: 4.32 | Val loss: 5.38\n",
      "Epoch 8150 | Train loss: 4.36 | Val loss: 5.23\n",
      "Epoch 8160 | Train loss: 4.35 | Val loss: 5.17\n",
      "Epoch 8170 | Train loss: 4.34 | Val loss: 5.36\n",
      "Epoch 8180 | Train loss: 4.37 | Val loss: 5.16\n",
      "Epoch 8190 | Train loss: 4.34 | Val loss: 5.33\n",
      "Epoch 8200 | Train loss: 4.34 | Val loss: 5.37\n",
      "Epoch 8210 | Train loss: 4.35 | Val loss: 5.20\n",
      "Epoch 8220 | Train loss: 4.35 | Val loss: 5.26\n",
      "Epoch 8230 | Train loss: 4.36 | Val loss: 5.29\n",
      "Epoch 8240 | Train loss: 4.37 | Val loss: 5.20\n",
      "Epoch 8250 | Train loss: 4.36 | Val loss: 5.36\n",
      "Epoch 8260 | Train loss: 4.35 | Val loss: 5.36\n",
      "Epoch 8270 | Train loss: 4.36 | Val loss: 5.30\n",
      "Epoch 8280 | Train loss: 4.37 | Val loss: 5.30\n",
      "Epoch 8290 | Train loss: 4.35 | Val loss: 5.20\n",
      "New best model saved with val loss: 5.03\n",
      "Epoch 8300 | Train loss: 4.38 | Val loss: 5.30\n",
      "Epoch 8310 | Train loss: 4.36 | Val loss: 5.33\n",
      "Epoch 8320 | Train loss: 4.35 | Val loss: 5.26\n",
      "Epoch 8330 | Train loss: 4.31 | Val loss: 5.35\n",
      "Epoch 8340 | Train loss: 4.40 | Val loss: 5.35\n",
      "Epoch 8350 | Train loss: 4.40 | Val loss: 5.25\n",
      "Epoch 8360 | Train loss: 4.34 | Val loss: 5.29\n",
      "Epoch 8370 | Train loss: 4.32 | Val loss: 5.20\n",
      "Epoch 8380 | Train loss: 4.36 | Val loss: 5.37\n",
      "Epoch 8390 | Train loss: 4.33 | Val loss: 5.21\n",
      "Epoch 8400 | Train loss: 4.34 | Val loss: 5.23\n",
      "Epoch 8410 | Train loss: 4.39 | Val loss: 5.28\n",
      "Epoch 8420 | Train loss: 4.41 | Val loss: 5.18\n",
      "Epoch 8430 | Train loss: 4.36 | Val loss: 5.36\n",
      "Epoch 8440 | Train loss: 4.37 | Val loss: 5.33\n",
      "Epoch 8450 | Train loss: 4.42 | Val loss: 5.19\n",
      "Epoch 8460 | Train loss: 4.30 | Val loss: 5.34\n",
      "Epoch 8470 | Train loss: 4.32 | Val loss: 5.25\n",
      "Epoch 8480 | Train loss: 4.35 | Val loss: 5.30\n",
      "Epoch 8490 | Train loss: 4.33 | Val loss: 5.38\n",
      "Epoch 8500 | Train loss: 4.33 | Val loss: 5.46\n",
      "Epoch 8510 | Train loss: 4.38 | Val loss: 5.38\n",
      "Epoch 8520 | Train loss: 4.32 | Val loss: 5.31\n",
      "Epoch 8530 | Train loss: 4.35 | Val loss: 5.31\n",
      "Epoch 8540 | Train loss: 4.34 | Val loss: 5.29\n",
      "Epoch 8550 | Train loss: 4.37 | Val loss: 5.27\n",
      "Epoch 8560 | Train loss: 4.33 | Val loss: 5.32\n",
      "Epoch 8570 | Train loss: 4.27 | Val loss: 5.24\n",
      "Epoch 8580 | Train loss: 4.38 | Val loss: 5.30\n",
      "Epoch 8590 | Train loss: 4.40 | Val loss: 5.26\n",
      "Epoch 8600 | Train loss: 4.31 | Val loss: 5.29\n",
      "Epoch 8610 | Train loss: 4.31 | Val loss: 5.36\n",
      "Epoch 8620 | Train loss: 4.35 | Val loss: 5.35\n",
      "Epoch 8630 | Train loss: 4.36 | Val loss: 5.39\n",
      "Epoch 8640 | Train loss: 4.34 | Val loss: 5.15\n",
      "Epoch 8650 | Train loss: 4.33 | Val loss: 5.18\n",
      "Epoch 8660 | Train loss: 4.37 | Val loss: 5.26\n",
      "Epoch 8670 | Train loss: 4.30 | Val loss: 5.18\n",
      "Epoch 8680 | Train loss: 4.38 | Val loss: 5.21\n",
      "Epoch 8690 | Train loss: 4.37 | Val loss: 5.49\n",
      "Epoch 8700 | Train loss: 4.32 | Val loss: 5.29\n",
      "Epoch 8710 | Train loss: 4.31 | Val loss: 5.10\n",
      "Epoch 8720 | Train loss: 4.41 | Val loss: 5.34\n",
      "Epoch 8730 | Train loss: 4.32 | Val loss: 5.38\n",
      "Epoch 8740 | Train loss: 4.35 | Val loss: 5.19\n",
      "Epoch 8750 | Train loss: 4.31 | Val loss: 5.42\n",
      "Epoch 8760 | Train loss: 4.37 | Val loss: 5.25\n",
      "Epoch 8770 | Train loss: 4.41 | Val loss: 5.31\n",
      "Epoch 8780 | Train loss: 4.35 | Val loss: 5.33\n",
      "Epoch 8790 | Train loss: 4.37 | Val loss: 5.35\n",
      "Epoch 8800 | Train loss: 4.43 | Val loss: 5.26\n",
      "Epoch 8810 | Train loss: 4.34 | Val loss: 5.27\n",
      "Epoch 8820 | Train loss: 4.32 | Val loss: 5.24\n",
      "Epoch 8830 | Train loss: 4.37 | Val loss: 5.27\n",
      "Epoch 8840 | Train loss: 4.39 | Val loss: 5.25\n",
      "Epoch 8850 | Train loss: 4.37 | Val loss: 5.34\n",
      "Epoch 8860 | Train loss: 4.38 | Val loss: 5.37\n",
      "Epoch 8870 | Train loss: 4.31 | Val loss: 5.26\n",
      "Epoch 8880 | Train loss: 4.36 | Val loss: 5.46\n",
      "Epoch 8890 | Train loss: 4.36 | Val loss: 5.15\n",
      "Epoch 8900 | Train loss: 4.38 | Val loss: 5.22\n",
      "Epoch 8910 | Train loss: 4.36 | Val loss: 5.33\n",
      "Epoch 8920 | Train loss: 4.39 | Val loss: 5.29\n",
      "Epoch 8930 | Train loss: 4.34 | Val loss: 5.30\n",
      "Epoch 8940 | Train loss: 4.36 | Val loss: 5.29\n",
      "Epoch 8950 | Train loss: 4.33 | Val loss: 5.27\n",
      "Epoch 8960 | Train loss: 4.36 | Val loss: 5.32\n",
      "Epoch 8970 | Train loss: 4.30 | Val loss: 5.25\n",
      "Epoch 8980 | Train loss: 4.31 | Val loss: 5.20\n",
      "Epoch 8990 | Train loss: 4.33 | Val loss: 5.40\n",
      "Epoch 9000 | Train loss: 4.34 | Val loss: 5.21\n",
      "Epoch 9010 | Train loss: 4.37 | Val loss: 5.24\n",
      "Epoch 9020 | Train loss: 4.35 | Val loss: 5.20\n",
      "Epoch 9030 | Train loss: 4.40 | Val loss: 5.16\n",
      "Epoch 9040 | Train loss: 4.36 | Val loss: 5.22\n",
      "Epoch 9050 | Train loss: 4.32 | Val loss: 5.27\n",
      "Epoch 9060 | Train loss: 4.32 | Val loss: 5.18\n",
      "Epoch 9070 | Train loss: 4.36 | Val loss: 5.28\n",
      "Epoch 9080 | Train loss: 4.38 | Val loss: 5.34\n",
      "Epoch 9090 | Train loss: 4.36 | Val loss: 5.18\n",
      "Epoch 9100 | Train loss: 4.34 | Val loss: 5.18\n",
      "Epoch 9110 | Train loss: 4.36 | Val loss: 5.32\n",
      "Epoch 9120 | Train loss: 4.37 | Val loss: 5.29\n",
      "Epoch 9130 | Train loss: 4.39 | Val loss: 5.25\n",
      "Epoch 9140 | Train loss: 4.38 | Val loss: 5.26\n",
      "Epoch 9150 | Train loss: 4.39 | Val loss: 5.29\n",
      "Epoch 9160 | Train loss: 4.35 | Val loss: 5.14\n",
      "Epoch 9170 | Train loss: 4.33 | Val loss: 5.33\n",
      "Epoch 9180 | Train loss: 4.32 | Val loss: 5.38\n",
      "Epoch 9190 | Train loss: 4.38 | Val loss: 5.24\n",
      "Epoch 9200 | Train loss: 4.34 | Val loss: 5.28\n",
      "Epoch 9210 | Train loss: 4.42 | Val loss: 5.18\n",
      "Epoch 9220 | Train loss: 4.36 | Val loss: 5.30\n",
      "Epoch 9230 | Train loss: 4.34 | Val loss: 5.38\n",
      "Epoch 9240 | Train loss: 4.38 | Val loss: 5.28\n",
      "Epoch 9250 | Train loss: 4.33 | Val loss: 5.29\n",
      "Epoch 9260 | Train loss: 4.34 | Val loss: 5.35\n",
      "Epoch 9270 | Train loss: 4.34 | Val loss: 5.30\n",
      "Epoch 9280 | Train loss: 4.35 | Val loss: 5.22\n",
      "Epoch 9290 | Train loss: 4.36 | Val loss: 5.19\n",
      "Epoch 9300 | Train loss: 4.32 | Val loss: 5.25\n",
      "Epoch 9310 | Train loss: 4.36 | Val loss: 5.29\n",
      "Epoch 9320 | Train loss: 4.33 | Val loss: 5.27\n",
      "Epoch 9330 | Train loss: 4.41 | Val loss: 5.31\n",
      "Epoch 9340 | Train loss: 4.38 | Val loss: 5.36\n",
      "Epoch 9350 | Train loss: 4.33 | Val loss: 5.25\n",
      "Epoch 9360 | Train loss: 4.38 | Val loss: 5.48\n",
      "Epoch 9370 | Train loss: 4.30 | Val loss: 5.23\n",
      "Epoch 9380 | Train loss: 4.36 | Val loss: 5.40\n",
      "Epoch 9390 | Train loss: 4.38 | Val loss: 5.21\n",
      "Epoch 9400 | Train loss: 4.35 | Val loss: 5.25\n",
      "Epoch 9410 | Train loss: 4.37 | Val loss: 5.43\n",
      "Epoch 9420 | Train loss: 4.31 | Val loss: 5.32\n",
      "Epoch 9430 | Train loss: 4.35 | Val loss: 5.22\n",
      "Epoch 9440 | Train loss: 4.38 | Val loss: 5.42\n",
      "Epoch 9450 | Train loss: 4.37 | Val loss: 5.37\n",
      "Epoch 9460 | Train loss: 4.31 | Val loss: 5.44\n",
      "Epoch 9470 | Train loss: 4.33 | Val loss: 5.28\n",
      "Epoch 9480 | Train loss: 4.36 | Val loss: 5.26\n",
      "Epoch 9490 | Train loss: 4.32 | Val loss: 5.35\n",
      "Epoch 9500 | Train loss: 4.32 | Val loss: 5.34\n",
      "Epoch 9510 | Train loss: 4.38 | Val loss: 5.43\n",
      "Epoch 9520 | Train loss: 4.31 | Val loss: 5.19\n",
      "Epoch 9530 | Train loss: 4.31 | Val loss: 5.19\n",
      "Epoch 9540 | Train loss: 4.36 | Val loss: 5.43\n",
      "Epoch 9550 | Train loss: 4.39 | Val loss: 5.44\n",
      "Epoch 9560 | Train loss: 4.31 | Val loss: 5.39\n",
      "Epoch 9570 | Train loss: 4.37 | Val loss: 5.30\n",
      "Epoch 9580 | Train loss: 4.40 | Val loss: 5.34\n",
      "Epoch 9590 | Train loss: 4.38 | Val loss: 5.19\n",
      "Epoch 9600 | Train loss: 4.35 | Val loss: 5.21\n",
      "Epoch 9610 | Train loss: 4.32 | Val loss: 5.29\n",
      "Epoch 9620 | Train loss: 4.34 | Val loss: 5.16\n",
      "Epoch 9630 | Train loss: 4.37 | Val loss: 5.34\n",
      "Epoch 9640 | Train loss: 4.36 | Val loss: 5.28\n",
      "Epoch 9650 | Train loss: 4.36 | Val loss: 5.35\n",
      "Epoch 9660 | Train loss: 4.34 | Val loss: 5.19\n",
      "Epoch 9670 | Train loss: 4.38 | Val loss: 5.27\n",
      "Epoch 9680 | Train loss: 4.32 | Val loss: 5.25\n",
      "Epoch 9690 | Train loss: 4.30 | Val loss: 5.24\n",
      "Epoch 9700 | Train loss: 4.33 | Val loss: 5.14\n",
      "Epoch 9710 | Train loss: 4.42 | Val loss: 5.27\n",
      "Epoch 9720 | Train loss: 4.31 | Val loss: 5.17\n",
      "Epoch 9730 | Train loss: 4.41 | Val loss: 5.30\n",
      "Epoch 9740 | Train loss: 4.33 | Val loss: 5.23\n",
      "Epoch 9750 | Train loss: 4.31 | Val loss: 5.26\n",
      "Epoch 9760 | Train loss: 4.38 | Val loss: 5.21\n",
      "Epoch 9770 | Train loss: 4.36 | Val loss: 5.40\n",
      "Epoch 9780 | Train loss: 4.33 | Val loss: 5.38\n",
      "Epoch 9790 | Train loss: 4.34 | Val loss: 5.22\n",
      "Epoch 9800 | Train loss: 4.35 | Val loss: 5.26\n",
      "Epoch 9810 | Train loss: 4.34 | Val loss: 5.15\n",
      "Epoch 9820 | Train loss: 4.34 | Val loss: 5.40\n",
      "Epoch 9830 | Train loss: 4.30 | Val loss: 5.18\n",
      "Epoch 9840 | Train loss: 4.33 | Val loss: 5.31\n",
      "Epoch 9850 | Train loss: 4.36 | Val loss: 5.17\n",
      "Epoch 9860 | Train loss: 4.36 | Val loss: 5.31\n",
      "Epoch 9870 | Train loss: 4.29 | Val loss: 5.47\n",
      "Epoch 9880 | Train loss: 4.34 | Val loss: 5.17\n",
      "Epoch 9890 | Train loss: 4.39 | Val loss: 5.37\n",
      "Epoch 9900 | Train loss: 4.37 | Val loss: 5.40\n",
      "Epoch 9910 | Train loss: 4.30 | Val loss: 5.27\n",
      "Epoch 9920 | Train loss: 4.33 | Val loss: 5.26\n",
      "Epoch 9930 | Train loss: 4.31 | Val loss: 5.24\n",
      "Epoch 9940 | Train loss: 4.35 | Val loss: 5.28\n",
      "Epoch 9950 | Train loss: 4.40 | Val loss: 5.44\n",
      "Epoch 9960 | Train loss: 4.36 | Val loss: 5.39\n",
      "Epoch 9970 | Train loss: 4.37 | Val loss: 5.21\n",
      "Epoch 9980 | Train loss: 4.37 | Val loss: 5.28\n",
      "Epoch 9990 | Train loss: 4.30 | Val loss: 5.41\n",
      "Best val loss: 5.03\n"
     ]
    }
   ],
   "source": [
    "from lr_scheduler import TrainingScheduler\n",
    "\n",
    "train_val_split_idx = int(len(tokenized_lotr) * 0.8)\n",
    "dataset = LotrDataset(torch.tensor(tokenized_lotr), context_length, length=1)\n",
    "\n",
    "model = model.cuda()\n",
    "lr0 = 1e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr0)\n",
    "nb_epochs = 10000\n",
    "warmup_steps = 100\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, nb_epochs - warmup_steps\n",
    ")\n",
    "scheduler = TrainingScheduler(\n",
    "    optimizer=optimizer, warmup_iteration=warmup_steps, lr0=lr0, scheduler=lr_scheduler\n",
    ")\n",
    "\n",
    "best_val_loss = torch.inf\n",
    "for epoch in range(nb_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(len(dataset)):\n",
    "        random_idx = torch.randint(\n",
    "            low=0, high=train_val_split_idx - context_length, size=(batch_size,)\n",
    "        )\n",
    "        x, targets = dataset[random_idx]\n",
    "        x, targets = x.cuda(), targets.cuda()\n",
    "        batch_output = model(x, targets)\n",
    "        train_loss += batch_output[1].item()\n",
    "        optimizer.zero_grad()\n",
    "        batch_output[1].backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            random_idx = torch.randint(\n",
    "                low=train_val_split_idx + 1,\n",
    "                high=len(tokenized_lotr) - context_length,\n",
    "                size=(batch_size,),\n",
    "            )\n",
    "            x, targets = dataset[random_idx]\n",
    "            x, targets = x.cuda(), targets.cuda()\n",
    "            batch_output = model(x, targets)\n",
    "            val_loss += batch_output[1].item()\n",
    "        val_loss /= len(dataset)\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save(model.state_dict(), \"best.pt\")\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"New best model saved with val loss: {best_val_loss:.2f}\")\n",
    "        if epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch} | Train loss: {train_loss:.2f} | Val loss: {val_loss:.2f}\"\n",
    "            )\n",
    "    scheduler.step(epoch)\n",
    "print(f\"Best val loss: {best_val_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Générer du Tolkien avec TolkienGPT ✍️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici la phrase générée : \n",
      " the hobbits stood now on the brink of a tall cliff , bare and bleak , its feet wrapped in mist ; and behind them rose the broken highlands crowned with drifting cloud . a chill wind blew from the east . there was a dark gate , and the water was still , and the wind was still in the mountains . the moon was still silent , but it was heavy , and the black sun was not very further . the trees were still in the wind , and they were gone . the mountains were clad in the gate , and the mountains were great and level . the sun was now in the path , and the sun was descended into the stream . the dark shadow was filled with the east . the moon was still and the light of the river , and the great river grew in the forest . the hobbits had found that the great river was still in the\n"
     ]
    }
   ],
   "source": [
    "model = TolkienGPT(len(tokenizer.unique_words), embeddind_dim, context_length, nb_blocks=nb_blocks, nb_heads=nb_heads)\n",
    "model.load_state_dict(torch.load(\"best.pt\"))\n",
    "model.cuda()\n",
    "model.eval()\n",
    "phrase = \"the hobbits stood now on the brink of a tall cliff, bare and bleak, its feet wrapped in mist; and behind them rose the broken highlands crowned with drifting cloud. a chill wind blew from the east.\"\n",
    "tokenized_phrase = torch.tensor(tokenizer.tokenize_a_sentence(phrase)).view(1, -1)\n",
    "output = model.generate(tokenized_phrase.cuda(), length=128)\n",
    "print(f\"Voici la phrase générée : \\n {' '.join([tokenizer.idx2word[token] for token in output.tolist()[0]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 🤩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: \n",
      " torch.Size([1, 1, 13451]) (batch_size, context_length, embedding_dim)\n",
      "Temps moyen de génération d'une phrase : 0.0070 secondes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tokenized_phrases = torch.stack([tokenized_phrase.view(-1) for _ in range(1)])\n",
    "\n",
    "times = []\n",
    "for i in range(100):\n",
    "    start = time.perf_counter()\n",
    "    output = model(tokenized_phrases.cuda(), None)\n",
    "    if i == 0:\n",
    "        print(f\"Output size: \\n {output[0].shape} (batch_size, context_length, embedding_dim)\")\n",
    "    end = time.perf_counter()\n",
    "    times.append(end-start)\n",
    "    \n",
    "times = torch.tensor(times)\n",
    "    \n",
    "print(f\"Temps moyen de génération d'une phrase : {times[1:].mean():.4f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 19, 768])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\"gandalf\", \"aragorn\", \"frodo\", \"sam\", \"sauron\", \"bilbo\", \"legolas\", \"gimli\", \"saruman\", \"pippin\", \"merry\", \"boromir\", \"faramir\", \"gollum\", \"elrond\", \"galadriel\", \"denethor\", \"eomer\", \"eowyn\"]\n",
    "# labels = [\"gandalf\", \"sauron\"]\n",
    "tokenized_labels = torch.tensor(tokenizer.tokenize_a_sentence(\" \".join(labels))).view(1, -1)\n",
    "model = model.cpu()\n",
    "embeddings = model.embedding_table(tokenized_labels)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(embeddings.squeeze().detach().numpy())\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "for i, label in enumerate(labels):\n",
    "    plt.scatter(X_pca[i, 0], X_pca[i, 1], label=label)\n",
    "    plt.text(X_pca[i, 0], X_pca[i, 1], label)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
