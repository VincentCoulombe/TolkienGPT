{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprendre l'architecture Transformer avec TolkienGPT üìö\n",
    "\n",
    "Bienvenue üòÄ \n",
    "\n",
    "Ce notebook n'a qu'un seul objectif. VOUS apprendre quelque chose de nouveau. Plus particuli√®rement:  \n",
    "\n",
    "- Comprendre l'architecture Transformer\n",
    "- Impl√©menter un Transformer decoder (l'algorithme derri√®re ChatGPT) avec seulement Pytorch\n",
    "- Entra√Æner un Transformer decoder √† g√©n√©rer du texte\n",
    "\n",
    "## About TolkienGPT üßô‚Äç‚ôÇÔ∏è\n",
    "\n",
    "TolkienGPT est un mod√®le de langue qu'on va coder et entra√Æner dans le but d'imiter l'√©criture de mon auteur pr√©f√©r√©, J.R.R. Tolkien (Le seigneur des anneaux, Le hobbit, Le silmarillon...). Ce mod√®le est bas√© sur l'architecture Transformer introduite dans la publication üìÑ [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf). Aussi, la p√©dagogie üìñ est inspir√©e du [tutoriel par Andrej Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY) de OpenAI ü¶æ. \n",
    "\n",
    "## Les pr√©requis üîç\n",
    "\n",
    "Pour √™tre √† l'aise, une compr√©hension de Python, Pytorch et des concepts de Deep Learning sont n√©cessaires. Je fais de mon mieux pour vulgariser, mais je ne peux pas faire un vid√©o de 3 heures non plus.\n",
    "\n",
    "\n",
    "Si vous √™tes pr√™ts, je vos propose de commencer notre aventure vers les secrets du Transformer Decoder üìú et la magie de TolkienGPT ü™Ñ!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger les donn√©es üöö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import re\n",
    "import os\n",
    "\n",
    "pdf_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"Tolkien-J.-The-lord-of-the-rings-HarperCollins-ebooks-2010.pdf\")\n",
    "pdf_reader = PdfReader(pdf_path)\n",
    "lotr = \"\"\n",
    "for i, page in enumerate(pdf_reader.pages):\n",
    "    if i >= 24 and i < 1163:\n",
    "        page_str = \"\".join(page.extract_text())\n",
    "        nlp_page = nlp(page_str)\n",
    "        for sentence in nlp_page.sents:\n",
    "            sentence_text = sentence.text.replace(\"\\n\", \" \").strip()\n",
    "            if re.search(r\"^[A-Z]\", sentence_text):\n",
    "                # print(sentence_text)\n",
    "                # print(\"-----\")\n",
    "                lotr += f\"{sentence_text.lower()}\\n\"\n",
    "  \n",
    "\n",
    "replacements = {\"-\", \"  \"}\n",
    "pattern = \"|\".join(map(re.escape, replacements))\n",
    "lotr = re.sub(pattern, \" \", lotr)       \n",
    "lotr = re.sub(r\"Ô¨Å\", \"fi\", lotr) \n",
    "lotr = re.sub(r\"Ô¨Ç\", \"fl\", lotr) \n",
    "lotr = re.sub(r\"Ô¨Ä\", \"ff\", lotr)\n",
    "lotr = re.sub(r\"√¶\", \"ae\", lotr)\n",
    "lotr = re.sub(r\"‚Äô\", \"'\", lotr)\n",
    "with open(\"lotr.txt\", \"w\") as f:\n",
    "    f.write(lotr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorer les donn√©es üïµÔ∏è‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 2191110 lettres dans le livre\n",
      "Il y a 417877 mots dans le livre\n",
      "Il y a 26276 phrases dans le livre\n"
     ]
    }
   ],
   "source": [
    "print(f\"Il y a {len(lotr)} lettres dans le livre\")\n",
    "print(f\"Il y a {len(lotr.split())} mots dans le livre\")\n",
    "print(f\"Il y a {len(lotr.splitlines())} phrases dans le livre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici la premi√®re phrase du livre:\n",
      " prologue  1 concerning hobbits  this book is largely concerned with hobbits, and from its pages a reader may discover much of their character and a little of their  history.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Voici la premi√®re phrase du livre:\\n {lotr.splitlines()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer les donn√©es üè≠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spoiler Alert ‚ö†Ô∏è les ordinateurs ne travaillent jamais directement sur les lettres (ils ne savent pas c'est quoi...). Donc, absolument toutes les applications encode les textes d'une mani√®re ou d'une autre.\n",
    "\n",
    "En Deep Learning, on les tokenize. C'est-√†-dire qu'on donne un ID ü™™ soit √† chaque lettre, √† chaque bout de mot ou bien directement √† chaque mot. \n",
    "\n",
    "Ici, on va tokenizer les mots directement (ce n'est pas un tutoriel sur la tokenization), mais les vrais mod√®les de langue tokenize sur des bouts de mots avec des algorithmes comme [SentencePiece](https://github.com/google/sentencepiece) de Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le mot 'gandalf' est le 1270√®me token du corpus\n",
      "\n",
      "Voici une phrase al√©atoire : \n",
      " great orcs, who also bore the white hand of isengard: that kind is stronger and more fell than all others.\n",
      "\n",
      "Voici les tokens de cette phrase : \n",
      " [115, 762, 22, 607, 63, 2303, 67, 769, 178, 55, 6672, 23, 71, 233, 42, 3286, 46, 125, 478, 153, 114, 511, 14]\n",
      "\n",
      "Voici la phrase reconstruite : \n",
      " great orcs , who also bore the white hand of isengard : that kind is stronger and more fell than all others .\n",
      "\n",
      "Le seigneur des anneaux contient 489427 tokens\n",
      "\n",
      "Le seigneur des anneaux contient 13451 tokens diff√©rents\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class lotrWordTokenizer:\n",
    "    def __init__(self, lotr):\n",
    "        self.special_chars = re.findall(r\"[^a-z\\s]\", \"\".join(set(lotr)))\n",
    "        self.unique_words = [] + self.special_chars\n",
    "        self.word2idx = {w: i for i, w in enumerate(self.unique_words)}\n",
    "        self.idx2word = {i: w for i, w in enumerate(self.unique_words)}\n",
    "        self.pattern = '|'.join(re.escape(special_char) for special_char in self.special_chars) + r\"|\\s\"\n",
    "\n",
    "    def tokenize_a_sentence(self, sentence):\n",
    "        matches = {match.start(): match.group() for match in re.finditer(self.pattern, sentence)}\n",
    "        matches\n",
    "        running_word = \"\"\n",
    "        tokens = []\n",
    "        for i, char in enumerate(sentence):\n",
    "            match = matches.get(i)\n",
    "            if match is not None:\n",
    "                if running_word != \"\":\n",
    "                    running_word = self.process_running_word(running_word, tokens)\n",
    "                if match != \" \":\n",
    "                    tokens.append(self.word2idx[match])\n",
    "            else:\n",
    "                running_word += char\n",
    "\n",
    "        if running_word != \"\":\n",
    "            running_word = self.process_running_word(running_word, tokens)\n",
    "        return tokens\n",
    "    \n",
    "    def process_running_word(self, running_word, tokens):\n",
    "        if running_word not in self.unique_words:\n",
    "            self.add_to_lexicon(running_word)\n",
    "        tokens.append(self.word2idx[running_word])\n",
    "        return \"\"\n",
    "\n",
    "    def add_to_lexicon(self, word):\n",
    "        self.unique_words.append(word)\n",
    "        self.word2idx[word] = len(self.unique_words) - 1\n",
    "        self.idx2word[len(self.unique_words) - 1] = word\n",
    "        \n",
    "tokenizer = lotrWordTokenizer(lotr)\n",
    "tokenized_lotr = []\n",
    "for sentence in lotr.splitlines():\n",
    "    tokenized_lotr.extend(tokenizer.tokenize_a_sentence(sentence))\n",
    "    \n",
    "    \n",
    "print(f\"Le mot 'gandalf' est le {tokenizer.word2idx['gandalf']}√®me token du corpus\\n\")\n",
    "random_sentence = \" \".join(lotr.splitlines()[9860:9861])\n",
    "print(f\"Voici une phrase al√©atoire : \\n {random_sentence}\")\n",
    "tokens = tokenizer.tokenize_a_sentence(random_sentence)\n",
    "print(f\"\\nVoici les tokens de cette phrase : \\n {tokens}\")\n",
    "print(f\"\\nVoici la phrase reconstruite : \\n {' '.join([tokenizer.idx2word[token] for token in tokens])}\\n\")\n",
    "\n",
    "tokenized_lotr = torch.tensor(tokenized_lotr)\n",
    "print(f\"Le seigneur des anneaux contient {len(tokenized_lotr)} tokens\")\n",
    "\n",
    "print(f\"\\nLe seigneur des anneaux contient {len(tokenizer.unique_words)} tokens diff√©rents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_words = \"\"\n",
    "for word in tokenizer.unique_words:\n",
    "    lotr_words += f\"{word}\\n\"\n",
    "    \n",
    "with open(\"lotr_words.txt\", \"w\") as f:\n",
    "    f.write(lotr_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Important** ‚ö†Ô∏è\n",
    "\n",
    "On  un mod√®le qui √©crit du Tolkien. C'est-√†-dire qui est capable d'aligner des mots, un apr√®s l'autre, comme Tolkien. Donc, il nous faut un mod√®le qui est capable de pr√©dire le prochain mot √† √©crire en fonction de ce qu'il a d√©j√† √©crit.\n",
    "\n",
    "Voici ce que je veux dire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TolkienGPT devra apprendre √† √©crire le mot 'great' dans un contexte o√π il a d√©j√† √©crit : \n",
      "\n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'orcs,' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'who' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'also' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'bore' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who also \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'the' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who also bore \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'white' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who also bore the \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'hand' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who also bore the white \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'of' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who also bore the white hand \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'isengard:' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who also bore the white hand of \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'that' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who also bore the white hand of isengard: \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'kind' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who also bore the white hand of isengard: that \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'is' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who also bore the white hand of isengard: that kind \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'stronger' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who also bore the white hand of isengard: that kind is \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'and' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who also bore the white hand of isengard: that kind is stronger \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'more' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who also bore the white hand of isengard: that kind is stronger and \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'fell' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who also bore the white hand of isengard: that kind is stronger and more \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'than' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who also bore the white hand of isengard: that kind is stronger and more fell \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'all' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who also bore the white hand of isengard: that kind is stronger and more fell than \n",
      "\n",
      "TolkienGPT devra apprendre √† √©crire le mot 'others.' dans un contexte o√π il a d√©j√† √©crit : \n",
      "great orcs, who also bore the white hand of isengard: that kind is stronger and more fell than all \n",
      "\n"
     ]
    }
   ],
   "source": [
    "wrote_words = \"\"\n",
    "for word in random_sentence.split():\n",
    "    print(f\"TolkienGPT devra apprendre √† √©crire le mot '{word}' dans un contexte o√π il a d√©j√† √©crit : \\n{wrote_words}\\n\")\n",
    "    wrote_words += f\"{word} \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, comment proc√©der dans un contexte matriciel ü§î?\n",
    "\n",
    "Je propose d'utiliser la m√©thode suivante:\n",
    "- D√©finir jusqu'√† combien de mots en arri√®re √ßa vaut la peine de regarder pour √©crire le prochain (context_length) \n",
    "- Piger al√©atoirement un mot dans le text (idx)\n",
    "- Tronquer les (context_length) mots suivants (idx)\n",
    "- Garder aussi en m√©moire le (context_length + 1)e mot apr√®s (idx).\n",
    "- Apprendre au mod√®le √† pr√©dire chacunes des combinaisons comprises dans l'exemple d'en haut ‚¨ÜÔ∏è:\n",
    "    - Le (idx + 1)e mot est celui qui devrait suivre le (idx)e mot\n",
    "    - Le (idx + 2)e mot est celui qui devrait suivre les (idx)e et (idx + 1)e mot\n",
    "    - ...\n",
    "    - Le (idx + context_length + 1)e mot est celui qui devrait suivre tous les mots du contexte\n",
    "\n",
    "Bref, ont initialise deux matrices. L'une contient le contexte et l'autre contient les mots √† √©crire. Soit:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici la matrice des inputs : \n",
      " ['all', 'that', 'was', 'necessary', ')']\n",
      "Voici la matrice des labels : \n",
      " ['that', 'was', 'necessary', ')', ',']\n"
     ]
    }
   ],
   "source": [
    "context_length = 5\n",
    "idx = 12346\n",
    "inputs = [tokenizer.idx2word[tokenized_lotr[i].item()] for i in range(idx, idx + context_length)]\n",
    "print(f\"Voici la matrice des inputs : \\n {inputs}\")\n",
    "\n",
    "labels = [tokenizer.idx2word[tokenized_lotr[i].item()] for i in range(idx + 1, idx + context_length + 1)]\n",
    "print(f\"Voici la matrice des labels : \\n {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cr√©er un [Dataset Pytorch](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) pour pouvoir facilement loader les phrases ainsi que le prochain mot √† pr√©dire.\n",
    "\n",
    "‚ö†Ô∏è **Important** ‚ö†Ô∏è\n",
    "\n",
    "Les batches sont trait√©es de mani√®re **ind√©pendantes**. Elles sont utiles parce que le GPU peut les trait√©es en parrall√®le (elles acc√©l√®re l'entrainement). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LotrDataset(Dataset):\n",
    "    def __init__(self, data, context_length, length):\n",
    "        self.data = data\n",
    "        self.context_length = context_length\n",
    "        self.length = length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, batch_idx):\n",
    "        input_tensor = torch.stack([self.data[idx:idx + self.context_length] for idx in batch_idx])\n",
    "        labels = torch.stack([self.data[idx+1:idx + self.context_length+1] for idx in batch_idx])\n",
    "        return input_tensor, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 32]), torch.Size([3, 32]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 32\n",
    "dataset = LotrDataset(tokenized_lotr, context_length, 1)\n",
    "batch = dataset[torch.tensor([1, 1234, 12345])]\n",
    "batch[0].shape, batch[1].shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construire TolkienGPT üèóÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le m√©cansime d'attention**\n",
    "\n",
    "En th√©orie: [Blogue sur le m√©cansime d'attention](https://www.syntell.com/blogue/nlp-3e-partie-chatgpt/)\n",
    "\n",
    "En pratique: [Impl√©mentation Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n",
    "\n",
    "Travaillons ensemble pour comprendre what's going on üïµÔ∏è‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment masquer l'attention via calculs matriciels ü§î? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5666,  1.1510, -0.8756,  1.1321],\n",
       "        [ 1.0021, -0.1516, -1.4235, -1.5469],\n",
       "        [ 0.1155, -0.0129,  0.6754,  0.4297],\n",
       "        [-1.6479, -0.2052,  0.1547, -0.4327]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_table = nn.Embedding(len(tokenizer.unique_words), 4)\n",
    "x = embedding_table(torch.tensor([tokenizer.word2idx[x] for x in [\"gandalf\", \"the\", \"grey\", \"is\"]]))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones((4, 4)))\n",
    "mask /= torch.sum(mask, dim=1, keepdim=True)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5666,  1.1510, -0.8756,  1.1321],\n",
       "        [-0.2822,  0.4997, -1.1496, -0.2074],\n",
       "        [-0.1497,  0.3289, -0.5412,  0.0050],\n",
       "        [-0.5242,  0.1954, -0.3673, -0.1044]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = mask @ x\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_bias = torch.zeros((4, 4))\n",
    "mas = torch.tril(torch.ones((4, 4)))\n",
    "attn_bias.masked_fill_(mas == 0, float(\"-inf\"))\n",
    "# Communication entre les mots\n",
    "# attn_bias = torch.softmax(attn_bias, dim=1)\n",
    "attn_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment masquer les embeddings font pour se parler ü§î? \n",
    "\n",
    "- Query = Voici ce que je cherche comme information\n",
    "- Key = Voici ce que je peux te donner comme information (marketing)\n",
    "- value = Voici l'information que je te donne r√©ellement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communication = nn.Linear(4, 4*3)\n",
    "key, query, value = communication(x).chunk(3, dim=-1)\n",
    "attn_scores = query @ key.transpose(-1, -2) / math.sqrt(query.size(-1))\n",
    "attn_scores += attn_bias\n",
    "attn_scores = torch.softmax(attn_scores, dim=-1)\n",
    "attn_scores = attn_scores @ value\n",
    "attn_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(key, query, value):\n",
    "    B,_, T, E = key.size()\n",
    "    attn_bias = torch.zeros((T, T), device=key.device)\n",
    "    mask = torch.tril(torch.ones((T, T), device=key.device))\n",
    "    attn_bias.masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "    attn_scores = query @ key.transpose(-1, -2) / math.sqrt(query.size(-1))\n",
    "    attn_scores += attn_bias\n",
    "    attn_scores = torch.softmax(attn_scores, dim=-1)\n",
    "    attn_scores = attn_scores @ value\n",
    "    return attn_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, nb_heads, embedding_size, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.communication = nn.Linear(embedding_size, embedding_size*3, bias=False)\n",
    "        self.nb_heads = nb_heads\n",
    "        self.projection = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, E = x.shape\n",
    "        key, query, value = self.communication(x).chunk(3, dim=-1)\n",
    "        key = key.view(B, T, self.nb_heads, E // self.nb_heads).transpose(1, 2) # shape: (batch size, nb heads, sequence length , embedding size per head)\n",
    "        query = query.view(B, T, self.nb_heads, E // self.nb_heads).transpose(1, 2)\n",
    "        value = value.view(B, T, self.nb_heads, E // self.nb_heads).transpose(1, 2)\n",
    "        attn_scores = scaled_dot_product_attention(key, query, value)\n",
    "        return self.dropout(self.projection(attn_scores.transpose(1, 2).contiguous().view(B, T, E)))\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embedding_size, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embedding_size, embedding_size)\n",
    "        self.fc2 = nn.Linear(embedding_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.fc2(F.gelu(self.fc1(x))))\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, nb_heads, embedding_size) -> None:\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(nb_heads, embedding_size, 0.2)\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_size)\n",
    "        self.mlp = MLP(embedding_size, 0.2)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.multi_head_attention(self.layer_norm1(x))\n",
    "        x = x + self.mlp(self.layer_norm2(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L'architecture Transformer**\n",
    "\n",
    "Impl√©mentation selon le paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TolkienGPT(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, embedding_dim, context_length, nb_blocks=6, nb_heads=8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_embeddings = nn.Embedding(context_length, embedding_dim)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(nb_heads, embedding_dim) for _ in range(nb_blocks)]\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_length = context_length\n",
    "        self.nb_blocks = nb_blocks\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "        embeddings = self.embedding_table(x) + self.positional_embeddings(\n",
    "            torch.arange(T, device=x.device)\n",
    "        )\n",
    "        E = embeddings.shape[-1]\n",
    "\n",
    "        x = self.dropout(embeddings)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        logits = self.linear(self.ln(x))\n",
    "\n",
    "        logits = logits.view(B * T, self.vocab_size)  # Concat√©ner les batchs\n",
    "        if targets is not None:\n",
    "            targets = targets.view(B * T)\n",
    "            loss = self.loss_function(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits.view(B, T, self.vocab_size), loss\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(\n",
    "                module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.nb_blocks)\n",
    "            )\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def generate(self, idx, length):\n",
    "        with torch.no_grad():\n",
    "            for _ in range(length):\n",
    "                # On prend les derniers context_length embeddings comme contexte d'inf√©rence\n",
    "                context = idx[:, -self.context_length :]\n",
    "                x_emb, _ = self.forward(context, None)\n",
    "\n",
    "                # On utilise les derniers logits pour obtenir notre distribution de probabilit√© pour le prochain mot\n",
    "                x_emb = x_emb[:, -1, :] / 0.5\n",
    "                normalized_x_emb = F.softmax(x_emb, dim=-1)\n",
    "\n",
    "                # On pige un mot suivant cette distribution de probabilit√©\n",
    "                next_idx = torch.multinomial(normalized_x_emb, num_samples=1)\n",
    "                idx = torch.cat([idx, next_idx], dim=-1)\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entra√Æner TolkienGPT üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 64\n",
    "batch_size = 128\n",
    "embeddind_dim = 768\n",
    "nb_heads = 16\n",
    "nb_blocks = 16\n",
    "model = TolkienGPT(len(tokenizer.unique_words), embeddind_dim, context_length, nb_blocks=nb_blocks, nb_heads=nb_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le mod√®le a 77421707 param√®tres\n"
     ]
    }
   ],
   "source": [
    "print(f\"Le mod√®le a {sum(p.numel() for p in model.parameters() if p.requires_grad)} param√®tres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12889/4194449351.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = LotrDataset(torch.tensor(tokenized_lotr), context_length, length=1)\n"
     ]
    }
   ],
   "source": [
    "train_val_split_idx = int(len(tokenized_lotr) * 0.8)\n",
    "dataset = LotrDataset(torch.tensor(tokenized_lotr), context_length, length=1)\n",
    "random_idx = torch.randint(low=0, high=train_val_split_idx-context_length, size=(batch_size,))\n",
    "x, targets = dataset[random_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: \n",
      " torch.Size([128, 64]) (batch_size, context_length) \n",
      "\n",
      "Output size: \n",
      " torch.Size([128, 64, 13451]) (batch_size, context_length, embedding_dim)\n",
      "Th√©oriquement, une loss initialis√©e random devrait √™tre de 9.51, la loss est de 9.51.\n"
     ]
    }
   ],
   "source": [
    "batch_output = model(x, targets)\n",
    "print(f\"Input size: \\n {x.shape} (batch_size, context_length) \\n\\nOutput size: \\n {batch_output[0].shape} (batch_size, context_length, embedding_dim)\")\n",
    "print(f\"Th√©oriquement, une loss initialis√©e random devrait √™tre de {-math.log(1/batch_output[0].size(-1)):.2f}, la loss est de {batch_output[1]:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12889/1252902887.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = LotrDataset(torch.tensor(tokenized_lotr), context_length, length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with val loss: 9.51\n",
      "Epoch 0 | Train loss: 9.51 | Val loss: 9.51\n",
      "New best model saved with val loss: 9.51\n",
      "New best model saved with val loss: 9.51\n",
      "New best model saved with val loss: 9.51\n",
      "New best model saved with val loss: 9.51\n",
      "New best model saved with val loss: 9.50\n",
      "New best model saved with val loss: 9.50\n",
      "New best model saved with val loss: 9.49\n",
      "New best model saved with val loss: 9.49\n",
      "New best model saved with val loss: 9.48\n",
      "Epoch 10 | Train loss: 9.49 | Val loss: 9.48\n",
      "New best model saved with val loss: 9.47\n",
      "New best model saved with val loss: 9.47\n",
      "New best model saved with val loss: 9.46\n",
      "New best model saved with val loss: 9.45\n",
      "New best model saved with val loss: 9.44\n",
      "New best model saved with val loss: 9.42\n",
      "New best model saved with val loss: 9.41\n",
      "New best model saved with val loss: 9.40\n",
      "New best model saved with val loss: 9.39\n",
      "New best model saved with val loss: 9.37\n",
      "Epoch 20 | Train loss: 9.42 | Val loss: 9.37\n",
      "New best model saved with val loss: 9.36\n",
      "New best model saved with val loss: 9.35\n",
      "New best model saved with val loss: 9.33\n",
      "New best model saved with val loss: 9.32\n",
      "New best model saved with val loss: 9.31\n",
      "New best model saved with val loss: 9.30\n",
      "New best model saved with val loss: 9.29\n",
      "New best model saved with val loss: 9.27\n",
      "New best model saved with val loss: 9.26\n",
      "New best model saved with val loss: 9.26\n",
      "Epoch 30 | Train loss: 9.29 | Val loss: 9.26\n",
      "New best model saved with val loss: 9.25\n",
      "New best model saved with val loss: 9.24\n",
      "New best model saved with val loss: 9.22\n",
      "New best model saved with val loss: 9.21\n",
      "New best model saved with val loss: 9.21\n",
      "New best model saved with val loss: 9.20\n",
      "New best model saved with val loss: 9.19\n",
      "New best model saved with val loss: 9.19\n",
      "New best model saved with val loss: 9.18\n",
      "New best model saved with val loss: 9.18\n",
      "Epoch 40 | Train loss: 9.20 | Val loss: 9.18\n",
      "New best model saved with val loss: 9.17\n",
      "New best model saved with val loss: 9.15\n",
      "New best model saved with val loss: 9.14\n",
      "New best model saved with val loss: 9.13\n",
      "New best model saved with val loss: 9.12\n",
      "New best model saved with val loss: 9.11\n",
      "New best model saved with val loss: 9.11\n",
      "Epoch 50 | Train loss: 9.13 | Val loss: 9.11\n",
      "New best model saved with val loss: 9.11\n",
      "New best model saved with val loss: 9.10\n",
      "New best model saved with val loss: 9.09\n",
      "New best model saved with val loss: 9.09\n",
      "New best model saved with val loss: 9.08\n",
      "New best model saved with val loss: 9.07\n",
      "New best model saved with val loss: 9.05\n",
      "New best model saved with val loss: 9.05\n",
      "New best model saved with val loss: 9.05\n",
      "Epoch 60 | Train loss: 9.06 | Val loss: 9.05\n",
      "New best model saved with val loss: 9.03\n",
      "New best model saved with val loss: 9.03\n",
      "New best model saved with val loss: 9.02\n",
      "New best model saved with val loss: 9.01\n",
      "New best model saved with val loss: 8.99\n",
      "New best model saved with val loss: 8.98\n",
      "New best model saved with val loss: 8.97\n",
      "Epoch 70 | Train loss: 8.98 | Val loss: 8.97\n",
      "New best model saved with val loss: 8.97\n",
      "New best model saved with val loss: 8.96\n",
      "New best model saved with val loss: 8.96\n",
      "New best model saved with val loss: 8.94\n",
      "New best model saved with val loss: 8.93\n",
      "New best model saved with val loss: 8.92\n",
      "New best model saved with val loss: 8.91\n",
      "New best model saved with val loss: 8.90\n",
      "New best model saved with val loss: 8.89\n",
      "Epoch 80 | Train loss: 8.88 | Val loss: 8.89\n",
      "New best model saved with val loss: 8.86\n",
      "New best model saved with val loss: 8.85\n",
      "New best model saved with val loss: 8.84\n",
      "New best model saved with val loss: 8.83\n",
      "New best model saved with val loss: 8.81\n",
      "New best model saved with val loss: 8.79\n",
      "Epoch 90 | Train loss: 8.79 | Val loss: 8.79\n",
      "New best model saved with val loss: 8.79\n",
      "New best model saved with val loss: 8.78\n",
      "New best model saved with val loss: 8.78\n",
      "New best model saved with val loss: 8.77\n",
      "New best model saved with val loss: 8.74\n",
      "New best model saved with val loss: 8.73\n",
      "New best model saved with val loss: 8.71\n",
      "New best model saved with val loss: 8.69\n",
      "Epoch 100 | Train loss: 8.68 | Val loss: 8.70\n",
      "New best model saved with val loss: 8.69\n",
      "New best model saved with val loss: 8.67\n",
      "New best model saved with val loss: 8.66\n",
      "New best model saved with val loss: 8.66\n",
      "New best model saved with val loss: 8.62\n",
      "New best model saved with val loss: 8.62\n",
      "New best model saved with val loss: 8.61\n",
      "New best model saved with val loss: 8.59\n",
      "New best model saved with val loss: 8.59\n",
      "Epoch 110 | Train loss: 8.57 | Val loss: 8.59\n",
      "New best model saved with val loss: 8.58\n",
      "New best model saved with val loss: 8.56\n",
      "New best model saved with val loss: 8.54\n",
      "New best model saved with val loss: 8.54\n",
      "New best model saved with val loss: 8.52\n",
      "New best model saved with val loss: 8.49\n",
      "Epoch 120 | Train loss: 8.46 | Val loss: 8.51\n",
      "New best model saved with val loss: 8.47\n",
      "New best model saved with val loss: 8.45\n",
      "New best model saved with val loss: 8.44\n",
      "New best model saved with val loss: 8.43\n",
      "New best model saved with val loss: 8.42\n",
      "New best model saved with val loss: 8.41\n",
      "New best model saved with val loss: 8.38\n",
      "New best model saved with val loss: 8.37\n",
      "Epoch 130 | Train loss: 8.34 | Val loss: 8.37\n",
      "New best model saved with val loss: 8.36\n",
      "New best model saved with val loss: 8.36\n",
      "New best model saved with val loss: 8.35\n",
      "New best model saved with val loss: 8.33\n",
      "New best model saved with val loss: 8.32\n",
      "New best model saved with val loss: 8.30\n",
      "New best model saved with val loss: 8.28\n",
      "Epoch 140 | Train loss: 8.23 | Val loss: 8.28\n",
      "New best model saved with val loss: 8.27\n",
      "New best model saved with val loss: 8.25\n",
      "New best model saved with val loss: 8.24\n",
      "New best model saved with val loss: 8.21\n",
      "New best model saved with val loss: 8.20\n",
      "Epoch 150 | Train loss: 8.13 | Val loss: 8.21\n",
      "New best model saved with val loss: 8.19\n",
      "New best model saved with val loss: 8.15\n",
      "New best model saved with val loss: 8.13\n",
      "New best model saved with val loss: 8.12\n",
      "New best model saved with val loss: 8.08\n",
      "Epoch 160 | Train loss: 8.02 | Val loss: 8.09\n",
      "New best model saved with val loss: 8.06\n",
      "New best model saved with val loss: 8.05\n",
      "New best model saved with val loss: 8.02\n",
      "New best model saved with val loss: 8.01\n",
      "New best model saved with val loss: 8.01\n",
      "New best model saved with val loss: 8.00\n",
      "New best model saved with val loss: 8.00\n",
      "Epoch 170 | Train loss: 7.93 | Val loss: 8.00\n",
      "New best model saved with val loss: 7.99\n",
      "New best model saved with val loss: 7.98\n",
      "New best model saved with val loss: 7.94\n",
      "New best model saved with val loss: 7.94\n",
      "New best model saved with val loss: 7.90\n",
      "New best model saved with val loss: 7.90\n",
      "Epoch 180 | Train loss: 7.83 | Val loss: 7.90\n",
      "New best model saved with val loss: 7.89\n",
      "New best model saved with val loss: 7.87\n",
      "New best model saved with val loss: 7.86\n",
      "New best model saved with val loss: 7.84\n",
      "New best model saved with val loss: 7.83\n",
      "New best model saved with val loss: 7.81\n",
      "Epoch 190 | Train loss: 7.73 | Val loss: 7.81\n",
      "New best model saved with val loss: 7.78\n",
      "New best model saved with val loss: 7.76\n",
      "New best model saved with val loss: 7.74\n",
      "Epoch 200 | Train loss: 7.64 | Val loss: 7.74\n",
      "New best model saved with val loss: 7.71\n",
      "New best model saved with val loss: 7.70\n",
      "New best model saved with val loss: 7.68\n",
      "New best model saved with val loss: 7.66\n",
      "Epoch 210 | Train loss: 7.57 | Val loss: 7.69\n",
      "New best model saved with val loss: 7.61\n",
      "New best model saved with val loss: 7.61\n",
      "New best model saved with val loss: 7.57\n",
      "Epoch 220 | Train loss: 7.48 | Val loss: 7.57\n",
      "New best model saved with val loss: 7.56\n",
      "New best model saved with val loss: 7.56\n",
      "New best model saved with val loss: 7.54\n",
      "New best model saved with val loss: 7.52\n",
      "Epoch 230 | Train loss: 7.40 | Val loss: 7.53\n",
      "New best model saved with val loss: 7.49\n",
      "New best model saved with val loss: 7.47\n",
      "New best model saved with val loss: 7.45\n",
      "Epoch 240 | Train loss: 7.35 | Val loss: 7.45\n",
      "New best model saved with val loss: 7.43\n",
      "New best model saved with val loss: 7.39\n",
      "New best model saved with val loss: 7.37\n",
      "Epoch 250 | Train loss: 7.26 | Val loss: 7.40\n",
      "New best model saved with val loss: 7.35\n",
      "New best model saved with val loss: 7.34\n",
      "New best model saved with val loss: 7.34\n",
      "New best model saved with val loss: 7.31\n",
      "Epoch 260 | Train loss: 7.18 | Val loss: 7.31\n",
      "New best model saved with val loss: 7.30\n",
      "New best model saved with val loss: 7.28\n",
      "New best model saved with val loss: 7.27\n",
      "New best model saved with val loss: 7.25\n",
      "New best model saved with val loss: 7.24\n",
      "Epoch 270 | Train loss: 7.11 | Val loss: 7.24\n",
      "New best model saved with val loss: 7.18\n",
      "Epoch 280 | Train loss: 7.09 | Val loss: 7.20\n",
      "New best model saved with val loss: 7.18\n",
      "New best model saved with val loss: 7.17\n",
      "Epoch 290 | Train loss: 7.02 | Val loss: 7.18\n",
      "New best model saved with val loss: 7.14\n",
      "New best model saved with val loss: 7.14\n",
      "New best model saved with val loss: 7.13\n",
      "New best model saved with val loss: 7.13\n",
      "New best model saved with val loss: 7.10\n",
      "Epoch 300 | Train loss: 6.96 | Val loss: 7.10\n",
      "New best model saved with val loss: 7.08\n",
      "Epoch 310 | Train loss: 6.90 | Val loss: 7.14\n",
      "New best model saved with val loss: 7.04\n",
      "New best model saved with val loss: 7.01\n",
      "New best model saved with val loss: 7.01\n",
      "Epoch 320 | Train loss: 6.85 | Val loss: 7.05\n",
      "New best model saved with val loss: 7.00\n",
      "New best model saved with val loss: 6.99\n",
      "New best model saved with val loss: 6.99\n",
      "New best model saved with val loss: 6.96\n",
      "Epoch 330 | Train loss: 6.80 | Val loss: 6.96\n",
      "New best model saved with val loss: 6.94\n",
      "New best model saved with val loss: 6.93\n",
      "Epoch 340 | Train loss: 6.76 | Val loss: 6.96\n",
      "New best model saved with val loss: 6.91\n",
      "New best model saved with val loss: 6.89\n",
      "Epoch 350 | Train loss: 6.73 | Val loss: 6.90\n",
      "New best model saved with val loss: 6.88\n",
      "New best model saved with val loss: 6.85\n",
      "New best model saved with val loss: 6.83\n",
      "Epoch 360 | Train loss: 6.71 | Val loss: 6.86\n",
      "New best model saved with val loss: 6.82\n",
      "Epoch 370 | Train loss: 6.65 | Val loss: 6.86\n",
      "New best model saved with val loss: 6.78\n",
      "New best model saved with val loss: 6.76\n",
      "Epoch 380 | Train loss: 6.61 | Val loss: 6.89\n",
      "New best model saved with val loss: 6.74\n",
      "Epoch 390 | Train loss: 6.59 | Val loss: 6.78\n",
      "New best model saved with val loss: 6.74\n",
      "New best model saved with val loss: 6.73\n",
      "New best model saved with val loss: 6.71\n",
      "Epoch 400 | Train loss: 6.57 | Val loss: 6.76\n",
      "New best model saved with val loss: 6.69\n",
      "Epoch 410 | Train loss: 6.51 | Val loss: 6.75\n",
      "New best model saved with val loss: 6.67\n",
      "New best model saved with val loss: 6.64\n",
      "Epoch 420 | Train loss: 6.49 | Val loss: 6.75\n",
      "New best model saved with val loss: 6.59\n",
      "Epoch 430 | Train loss: 6.47 | Val loss: 6.70\n",
      "New best model saved with val loss: 6.55\n",
      "Epoch 440 | Train loss: 6.45 | Val loss: 6.70\n",
      "Epoch 450 | Train loss: 6.41 | Val loss: 6.65\n",
      "Epoch 460 | Train loss: 6.37 | Val loss: 6.64\n",
      "Epoch 470 | Train loss: 6.38 | Val loss: 6.57\n",
      "New best model saved with val loss: 6.51\n",
      "New best model saved with val loss: 6.50\n",
      "Epoch 480 | Train loss: 6.34 | Val loss: 6.50\n",
      "New best model saved with val loss: 6.50\n",
      "Epoch 490 | Train loss: 6.33 | Val loss: 6.50\n",
      "New best model saved with val loss: 6.46\n",
      "New best model saved with val loss: 6.45\n",
      "New best model saved with val loss: 6.44\n",
      "Epoch 500 | Train loss: 6.28 | Val loss: 6.59\n",
      "New best model saved with val loss: 6.41\n",
      "Epoch 510 | Train loss: 6.26 | Val loss: 6.50\n",
      "Epoch 520 | Train loss: 6.24 | Val loss: 6.41\n",
      "Epoch 530 | Train loss: 6.29 | Val loss: 6.48\n",
      "New best model saved with val loss: 6.40\n",
      "New best model saved with val loss: 6.35\n",
      "Epoch 540 | Train loss: 6.25 | Val loss: 6.35\n",
      "Epoch 550 | Train loss: 6.18 | Val loss: 6.46\n",
      "Epoch 560 | Train loss: 6.18 | Val loss: 6.44\n",
      "Epoch 570 | Train loss: 6.20 | Val loss: 6.36\n",
      "New best model saved with val loss: 6.34\n",
      "New best model saved with val loss: 6.30\n",
      "Epoch 580 | Train loss: 6.17 | Val loss: 6.40\n",
      "New best model saved with val loss: 6.29\n",
      "Epoch 590 | Train loss: 6.13 | Val loss: 6.35\n",
      "New best model saved with val loss: 6.29\n",
      "Epoch 600 | Train loss: 6.15 | Val loss: 6.42\n",
      "Epoch 610 | Train loss: 6.11 | Val loss: 6.32\n",
      "New best model saved with val loss: 6.29\n",
      "Epoch 620 | Train loss: 6.07 | Val loss: 6.29\n",
      "New best model saved with val loss: 6.28\n",
      "Epoch 630 | Train loss: 6.08 | Val loss: 6.34\n",
      "New best model saved with val loss: 6.26\n",
      "New best model saved with val loss: 6.25\n",
      "Epoch 640 | Train loss: 6.00 | Val loss: 6.33\n",
      "Epoch 650 | Train loss: 6.06 | Val loss: 6.30\n",
      "New best model saved with val loss: 6.19\n",
      "Epoch 660 | Train loss: 6.05 | Val loss: 6.26\n",
      "Epoch 670 | Train loss: 5.99 | Val loss: 6.19\n",
      "Epoch 680 | Train loss: 6.00 | Val loss: 6.30\n",
      "Epoch 690 | Train loss: 5.96 | Val loss: 6.26\n",
      "Epoch 700 | Train loss: 5.99 | Val loss: 6.28\n",
      "Epoch 710 | Train loss: 5.97 | Val loss: 6.26\n",
      "New best model saved with val loss: 6.16\n",
      "Epoch 720 | Train loss: 5.96 | Val loss: 6.16\n",
      "Epoch 730 | Train loss: 5.93 | Val loss: 6.20\n",
      "New best model saved with val loss: 6.12\n",
      "Epoch 740 | Train loss: 5.92 | Val loss: 6.31\n",
      "Epoch 750 | Train loss: 5.90 | Val loss: 6.24\n",
      "New best model saved with val loss: 6.10\n",
      "New best model saved with val loss: 6.04\n",
      "Epoch 760 | Train loss: 5.90 | Val loss: 6.04\n",
      "Epoch 770 | Train loss: 5.89 | Val loss: 6.14\n",
      "Epoch 780 | Train loss: 5.88 | Val loss: 6.25\n",
      "Epoch 790 | Train loss: 5.83 | Val loss: 6.25\n",
      "Epoch 800 | Train loss: 5.85 | Val loss: 6.09\n",
      "Epoch 810 | Train loss: 5.81 | Val loss: 6.21\n",
      "New best model saved with val loss: 6.02\n",
      "Epoch 820 | Train loss: 5.86 | Val loss: 6.07\n",
      "Epoch 830 | Train loss: 5.86 | Val loss: 6.19\n",
      "Epoch 840 | Train loss: 5.88 | Val loss: 6.15\n",
      "New best model saved with val loss: 5.96\n",
      "Epoch 850 | Train loss: 5.81 | Val loss: 5.96\n",
      "Epoch 860 | Train loss: 5.83 | Val loss: 6.12\n",
      "Epoch 870 | Train loss: 5.78 | Val loss: 6.03\n",
      "Epoch 880 | Train loss: 5.75 | Val loss: 6.05\n",
      "Epoch 890 | Train loss: 5.74 | Val loss: 6.00\n",
      "Epoch 900 | Train loss: 5.76 | Val loss: 6.09\n",
      "Epoch 910 | Train loss: 5.75 | Val loss: 6.01\n",
      "New best model saved with val loss: 5.94\n",
      "Epoch 920 | Train loss: 5.75 | Val loss: 6.07\n",
      "New best model saved with val loss: 5.94\n",
      "Epoch 930 | Train loss: 5.72 | Val loss: 6.04\n",
      "New best model saved with val loss: 5.93\n",
      "Epoch 940 | Train loss: 5.72 | Val loss: 6.14\n",
      "Epoch 950 | Train loss: 5.72 | Val loss: 6.12\n",
      "Epoch 960 | Train loss: 5.69 | Val loss: 6.08\n",
      "New best model saved with val loss: 5.93\n",
      "Epoch 970 | Train loss: 5.64 | Val loss: 6.06\n",
      "Epoch 980 | Train loss: 5.64 | Val loss: 5.95\n",
      "New best model saved with val loss: 5.92\n",
      "Epoch 990 | Train loss: 5.65 | Val loss: 6.01\n",
      "New best model saved with val loss: 5.88\n",
      "Epoch 1000 | Train loss: 5.63 | Val loss: 5.95\n",
      "Epoch 1010 | Train loss: 5.62 | Val loss: 6.03\n",
      "Epoch 1020 | Train loss: 5.67 | Val loss: 6.03\n",
      "Epoch 1030 | Train loss: 5.63 | Val loss: 5.97\n",
      "Epoch 1040 | Train loss: 5.64 | Val loss: 6.05\n",
      "Epoch 1050 | Train loss: 5.61 | Val loss: 5.90\n",
      "Epoch 1060 | Train loss: 5.55 | Val loss: 6.08\n",
      "Epoch 1070 | Train loss: 5.56 | Val loss: 5.94\n",
      "New best model saved with val loss: 5.86\n",
      "Epoch 1080 | Train loss: 5.56 | Val loss: 5.94\n",
      "New best model saved with val loss: 5.83\n",
      "Epoch 1090 | Train loss: 5.57 | Val loss: 5.91\n",
      "Epoch 1100 | Train loss: 5.52 | Val loss: 5.90\n",
      "Epoch 1110 | Train loss: 5.56 | Val loss: 5.95\n",
      "Epoch 1120 | Train loss: 5.53 | Val loss: 5.87\n",
      "Epoch 1130 | Train loss: 5.53 | Val loss: 6.06\n",
      "Epoch 1140 | Train loss: 5.53 | Val loss: 5.95\n",
      "Epoch 1150 | Train loss: 5.49 | Val loss: 5.94\n",
      "New best model saved with val loss: 5.83\n",
      "Epoch 1160 | Train loss: 5.62 | Val loss: 5.83\n",
      "New best model saved with val loss: 5.80\n",
      "Epoch 1170 | Train loss: 5.47 | Val loss: 5.87\n",
      "New best model saved with val loss: 5.77\n",
      "Epoch 1180 | Train loss: 5.50 | Val loss: 5.79\n",
      "New best model saved with val loss: 5.71\n",
      "Epoch 1190 | Train loss: 5.47 | Val loss: 5.88\n",
      "Epoch 1200 | Train loss: 5.47 | Val loss: 5.97\n",
      "Epoch 1210 | Train loss: 5.48 | Val loss: 5.81\n",
      "Epoch 1220 | Train loss: 5.51 | Val loss: 5.80\n",
      "Epoch 1230 | Train loss: 5.50 | Val loss: 5.76\n",
      "Epoch 1240 | Train loss: 5.47 | Val loss: 5.81\n",
      "Epoch 1250 | Train loss: 5.46 | Val loss: 5.96\n",
      "Epoch 1260 | Train loss: 5.45 | Val loss: 5.91\n",
      "Epoch 1270 | Train loss: 5.43 | Val loss: 5.83\n",
      "Epoch 1280 | Train loss: 5.35 | Val loss: 5.88\n",
      "Epoch 1290 | Train loss: 5.39 | Val loss: 5.93\n",
      "Epoch 1300 | Train loss: 5.43 | Val loss: 5.80\n",
      "New best model saved with val loss: 5.71\n",
      "Epoch 1310 | Train loss: 5.43 | Val loss: 5.93\n",
      "Epoch 1320 | Train loss: 5.46 | Val loss: 5.75\n",
      "New best model saved with val loss: 5.69\n",
      "Epoch 1330 | Train loss: 5.34 | Val loss: 5.73\n",
      "Epoch 1340 | Train loss: 5.38 | Val loss: 5.74\n",
      "Epoch 1350 | Train loss: 5.39 | Val loss: 5.77\n",
      "Epoch 1360 | Train loss: 5.39 | Val loss: 5.85\n",
      "Epoch 1370 | Train loss: 5.33 | Val loss: 5.80\n",
      "Epoch 1380 | Train loss: 5.38 | Val loss: 5.72\n",
      "Epoch 1390 | Train loss: 5.38 | Val loss: 5.86\n",
      "New best model saved with val loss: 5.68\n",
      "Epoch 1400 | Train loss: 5.38 | Val loss: 5.75\n",
      "New best model saved with val loss: 5.68\n",
      "Epoch 1410 | Train loss: 5.36 | Val loss: 5.87\n",
      "New best model saved with val loss: 5.64\n",
      "Epoch 1420 | Train loss: 5.32 | Val loss: 5.79\n",
      "New best model saved with val loss: 5.63\n",
      "Epoch 1430 | Train loss: 5.30 | Val loss: 5.78\n",
      "New best model saved with val loss: 5.62\n",
      "Epoch 1440 | Train loss: 5.33 | Val loss: 5.62\n",
      "New best model saved with val loss: 5.61\n",
      "Epoch 1450 | Train loss: 5.32 | Val loss: 5.65\n",
      "Epoch 1460 | Train loss: 5.28 | Val loss: 5.72\n",
      "Epoch 1470 | Train loss: 5.30 | Val loss: 5.74\n",
      "New best model saved with val loss: 5.60\n",
      "Epoch 1480 | Train loss: 5.28 | Val loss: 5.70\n",
      "Epoch 1490 | Train loss: 5.28 | Val loss: 5.83\n",
      "Epoch 1500 | Train loss: 5.23 | Val loss: 5.77\n",
      "New best model saved with val loss: 5.60\n",
      "New best model saved with val loss: 5.57\n",
      "Epoch 1510 | Train loss: 5.26 | Val loss: 5.69\n",
      "Epoch 1520 | Train loss: 5.25 | Val loss: 5.66\n",
      "Epoch 1530 | Train loss: 5.33 | Val loss: 5.72\n",
      "Epoch 1540 | Train loss: 5.25 | Val loss: 5.79\n",
      "New best model saved with val loss: 5.55\n",
      "Epoch 1550 | Train loss: 5.29 | Val loss: 5.55\n",
      "Epoch 1560 | Train loss: 5.28 | Val loss: 5.64\n",
      "Epoch 1570 | Train loss: 5.27 | Val loss: 5.75\n",
      "Epoch 1580 | Train loss: 5.21 | Val loss: 5.64\n",
      "Epoch 1590 | Train loss: 5.27 | Val loss: 5.71\n",
      "Epoch 1600 | Train loss: 5.24 | Val loss: 5.68\n",
      "Epoch 1610 | Train loss: 5.23 | Val loss: 5.68\n",
      "Epoch 1620 | Train loss: 5.28 | Val loss: 5.80\n",
      "Epoch 1630 | Train loss: 5.29 | Val loss: 5.74\n",
      "Epoch 1640 | Train loss: 5.25 | Val loss: 5.73\n",
      "Epoch 1650 | Train loss: 5.24 | Val loss: 5.68\n",
      "New best model saved with val loss: 5.55\n",
      "New best model saved with val loss: 5.54\n",
      "Epoch 1660 | Train loss: 5.23 | Val loss: 5.65\n",
      "Epoch 1670 | Train loss: 5.23 | Val loss: 5.63\n",
      "Epoch 1680 | Train loss: 5.18 | Val loss: 5.66\n",
      "Epoch 1690 | Train loss: 5.19 | Val loss: 5.61\n",
      "Epoch 1700 | Train loss: 5.23 | Val loss: 5.62\n",
      "Epoch 1710 | Train loss: 5.21 | Val loss: 5.57\n",
      "Epoch 1720 | Train loss: 5.14 | Val loss: 5.61\n",
      "Epoch 1730 | Train loss: 5.19 | Val loss: 5.75\n",
      "Epoch 1740 | Train loss: 5.14 | Val loss: 5.64\n",
      "Epoch 1750 | Train loss: 5.19 | Val loss: 5.70\n",
      "Epoch 1760 | Train loss: 5.17 | Val loss: 5.68\n",
      "New best model saved with val loss: 5.54\n",
      "Epoch 1770 | Train loss: 5.14 | Val loss: 5.58\n",
      "Epoch 1780 | Train loss: 5.11 | Val loss: 5.72\n",
      "Epoch 1790 | Train loss: 5.12 | Val loss: 5.66\n",
      "New best model saved with val loss: 5.49\n",
      "Epoch 1800 | Train loss: 5.13 | Val loss: 5.63\n",
      "Epoch 1810 | Train loss: 5.17 | Val loss: 5.66\n",
      "Epoch 1820 | Train loss: 5.13 | Val loss: 5.62\n",
      "Epoch 1830 | Train loss: 5.13 | Val loss: 5.65\n",
      "Epoch 1840 | Train loss: 5.10 | Val loss: 5.51\n",
      "Epoch 1850 | Train loss: 5.12 | Val loss: 5.59\n",
      "Epoch 1860 | Train loss: 5.12 | Val loss: 5.63\n",
      "Epoch 1870 | Train loss: 5.12 | Val loss: 5.51\n",
      "Epoch 1880 | Train loss: 5.14 | Val loss: 5.65\n",
      "Epoch 1890 | Train loss: 5.06 | Val loss: 5.68\n",
      "Epoch 1900 | Train loss: 5.06 | Val loss: 5.67\n",
      "Epoch 1910 | Train loss: 5.09 | Val loss: 5.50\n",
      "Epoch 1920 | Train loss: 5.10 | Val loss: 5.74\n",
      "Epoch 1930 | Train loss: 5.11 | Val loss: 5.58\n",
      "Epoch 1940 | Train loss: 5.04 | Val loss: 5.56\n",
      "Epoch 1950 | Train loss: 5.08 | Val loss: 5.56\n",
      "New best model saved with val loss: 5.47\n",
      "Epoch 1960 | Train loss: 5.11 | Val loss: 5.58\n",
      "Epoch 1970 | Train loss: 5.09 | Val loss: 5.60\n",
      "Epoch 1980 | Train loss: 5.08 | Val loss: 5.59\n",
      "Epoch 1990 | Train loss: 5.07 | Val loss: 5.60\n",
      "Epoch 2000 | Train loss: 5.08 | Val loss: 5.61\n",
      "Epoch 2010 | Train loss: 5.06 | Val loss: 5.72\n",
      "Epoch 2020 | Train loss: 5.07 | Val loss: 5.55\n",
      "Epoch 2030 | Train loss: 5.07 | Val loss: 5.65\n",
      "Epoch 2040 | Train loss: 5.07 | Val loss: 5.65\n",
      "Epoch 2050 | Train loss: 5.03 | Val loss: 5.58\n",
      "Epoch 2060 | Train loss: 5.04 | Val loss: 5.52\n",
      "Epoch 2070 | Train loss: 5.01 | Val loss: 5.75\n",
      "Epoch 2080 | Train loss: 4.99 | Val loss: 5.67\n",
      "Epoch 2090 | Train loss: 5.02 | Val loss: 5.59\n",
      "Epoch 2100 | Train loss: 5.06 | Val loss: 5.61\n",
      "Epoch 2110 | Train loss: 5.01 | Val loss: 5.59\n",
      "Epoch 2120 | Train loss: 5.04 | Val loss: 5.56\n",
      "New best model saved with val loss: 5.45\n",
      "Epoch 2130 | Train loss: 5.03 | Val loss: 5.54\n",
      "New best model saved with val loss: 5.44\n",
      "Epoch 2140 | Train loss: 5.01 | Val loss: 5.58\n",
      "Epoch 2150 | Train loss: 5.03 | Val loss: 5.62\n",
      "New best model saved with val loss: 5.40\n",
      "Epoch 2160 | Train loss: 4.98 | Val loss: 5.59\n",
      "Epoch 2170 | Train loss: 5.04 | Val loss: 5.64\n",
      "Epoch 2180 | Train loss: 4.99 | Val loss: 5.53\n",
      "Epoch 2190 | Train loss: 5.03 | Val loss: 5.56\n",
      "Epoch 2200 | Train loss: 4.98 | Val loss: 5.56\n",
      "Epoch 2210 | Train loss: 5.05 | Val loss: 5.58\n",
      "Epoch 2220 | Train loss: 4.91 | Val loss: 5.46\n",
      "Epoch 2230 | Train loss: 4.97 | Val loss: 5.73\n",
      "Epoch 2240 | Train loss: 5.00 | Val loss: 5.53\n",
      "Epoch 2250 | Train loss: 5.02 | Val loss: 5.64\n",
      "Epoch 2260 | Train loss: 4.95 | Val loss: 5.50\n",
      "Epoch 2270 | Train loss: 4.94 | Val loss: 5.51\n",
      "Epoch 2280 | Train loss: 4.96 | Val loss: 5.59\n",
      "New best model saved with val loss: 5.37\n",
      "Epoch 2290 | Train loss: 4.98 | Val loss: 5.53\n",
      "Epoch 2300 | Train loss: 4.92 | Val loss: 5.47\n",
      "Epoch 2310 | Train loss: 4.97 | Val loss: 5.77\n",
      "Epoch 2320 | Train loss: 4.88 | Val loss: 5.47\n",
      "Epoch 2330 | Train loss: 5.01 | Val loss: 5.50\n",
      "Epoch 2340 | Train loss: 4.97 | Val loss: 5.42\n",
      "Epoch 2350 | Train loss: 4.95 | Val loss: 5.62\n",
      "Epoch 2360 | Train loss: 4.94 | Val loss: 5.58\n",
      "Epoch 2370 | Train loss: 4.97 | Val loss: 5.53\n",
      "Epoch 2380 | Train loss: 4.95 | Val loss: 5.63\n",
      "Epoch 2390 | Train loss: 4.93 | Val loss: 5.52\n",
      "Epoch 2400 | Train loss: 4.92 | Val loss: 5.56\n",
      "Epoch 2410 | Train loss: 4.99 | Val loss: 5.38\n",
      "Epoch 2420 | Train loss: 4.96 | Val loss: 5.48\n",
      "Epoch 2430 | Train loss: 4.87 | Val loss: 5.53\n",
      "Epoch 2440 | Train loss: 4.90 | Val loss: 5.51\n",
      "Epoch 2450 | Train loss: 4.96 | Val loss: 5.51\n",
      "New best model saved with val loss: 5.35\n",
      "Epoch 2460 | Train loss: 4.97 | Val loss: 5.41\n",
      "Epoch 2470 | Train loss: 4.93 | Val loss: 5.53\n",
      "Epoch 2480 | Train loss: 4.98 | Val loss: 5.42\n",
      "Epoch 2490 | Train loss: 4.93 | Val loss: 5.44\n",
      "Epoch 2500 | Train loss: 4.93 | Val loss: 5.44\n",
      "Epoch 2510 | Train loss: 4.88 | Val loss: 5.52\n",
      "Epoch 2520 | Train loss: 4.91 | Val loss: 5.38\n",
      "New best model saved with val loss: 5.31\n",
      "Epoch 2530 | Train loss: 4.92 | Val loss: 5.36\n",
      "Epoch 2540 | Train loss: 4.91 | Val loss: 5.50\n",
      "Epoch 2550 | Train loss: 4.92 | Val loss: 5.55\n",
      "Epoch 2560 | Train loss: 4.87 | Val loss: 5.50\n",
      "Epoch 2570 | Train loss: 4.87 | Val loss: 5.45\n",
      "Epoch 2580 | Train loss: 4.94 | Val loss: 5.52\n",
      "Epoch 2590 | Train loss: 4.88 | Val loss: 5.53\n",
      "Epoch 2600 | Train loss: 4.88 | Val loss: 5.60\n",
      "Epoch 2610 | Train loss: 4.90 | Val loss: 5.48\n",
      "Epoch 2620 | Train loss: 4.95 | Val loss: 5.38\n",
      "New best model saved with val loss: 5.26\n",
      "Epoch 2630 | Train loss: 4.88 | Val loss: 5.47\n",
      "Epoch 2640 | Train loss: 4.84 | Val loss: 5.40\n",
      "Epoch 2650 | Train loss: 4.89 | Val loss: 5.57\n",
      "Epoch 2660 | Train loss: 4.87 | Val loss: 5.45\n",
      "Epoch 2670 | Train loss: 4.95 | Val loss: 5.51\n",
      "Epoch 2680 | Train loss: 4.86 | Val loss: 5.47\n",
      "Epoch 2690 | Train loss: 4.86 | Val loss: 5.59\n",
      "Epoch 2700 | Train loss: 4.92 | Val loss: 5.41\n",
      "Epoch 2710 | Train loss: 4.84 | Val loss: 5.58\n",
      "Epoch 2720 | Train loss: 4.90 | Val loss: 5.51\n",
      "Epoch 2730 | Train loss: 4.84 | Val loss: 5.52\n",
      "Epoch 2740 | Train loss: 4.87 | Val loss: 5.53\n",
      "Epoch 2750 | Train loss: 4.80 | Val loss: 5.44\n",
      "Epoch 2760 | Train loss: 4.86 | Val loss: 5.38\n",
      "Epoch 2770 | Train loss: 4.84 | Val loss: 5.48\n",
      "Epoch 2780 | Train loss: 4.85 | Val loss: 5.40\n",
      "Epoch 2790 | Train loss: 4.78 | Val loss: 5.43\n",
      "Epoch 2800 | Train loss: 4.85 | Val loss: 5.35\n",
      "Epoch 2810 | Train loss: 4.79 | Val loss: 5.43\n",
      "Epoch 2820 | Train loss: 4.82 | Val loss: 5.45\n",
      "Epoch 2830 | Train loss: 4.85 | Val loss: 5.52\n",
      "Epoch 2840 | Train loss: 4.92 | Val loss: 5.40\n",
      "Epoch 2850 | Train loss: 4.83 | Val loss: 5.49\n",
      "Epoch 2860 | Train loss: 4.82 | Val loss: 5.48\n",
      "Epoch 2870 | Train loss: 4.85 | Val loss: 5.42\n",
      "Epoch 2880 | Train loss: 4.76 | Val loss: 5.60\n",
      "Epoch 2890 | Train loss: 4.83 | Val loss: 5.36\n",
      "Epoch 2900 | Train loss: 4.80 | Val loss: 5.50\n",
      "Epoch 2910 | Train loss: 4.85 | Val loss: 5.43\n",
      "Epoch 2920 | Train loss: 4.82 | Val loss: 5.50\n",
      "Epoch 2930 | Train loss: 4.78 | Val loss: 5.59\n",
      "Epoch 2940 | Train loss: 4.85 | Val loss: 5.43\n",
      "Epoch 2950 | Train loss: 4.82 | Val loss: 5.39\n",
      "Epoch 2960 | Train loss: 4.81 | Val loss: 5.38\n",
      "Epoch 2970 | Train loss: 4.78 | Val loss: 5.45\n",
      "Epoch 2980 | Train loss: 4.89 | Val loss: 5.43\n",
      "Epoch 2990 | Train loss: 4.83 | Val loss: 5.49\n",
      "Epoch 3000 | Train loss: 4.77 | Val loss: 5.39\n",
      "Epoch 3010 | Train loss: 4.84 | Val loss: 5.46\n",
      "Epoch 3020 | Train loss: 4.89 | Val loss: 5.41\n",
      "Epoch 3030 | Train loss: 4.75 | Val loss: 5.43\n",
      "Epoch 3040 | Train loss: 4.89 | Val loss: 5.42\n",
      "Epoch 3050 | Train loss: 4.79 | Val loss: 5.38\n",
      "Epoch 3060 | Train loss: 4.79 | Val loss: 5.35\n",
      "Epoch 3070 | Train loss: 4.80 | Val loss: 5.41\n",
      "Epoch 3080 | Train loss: 4.80 | Val loss: 5.49\n",
      "Epoch 3090 | Train loss: 4.80 | Val loss: 5.48\n",
      "Epoch 3100 | Train loss: 4.75 | Val loss: 5.44\n",
      "Epoch 3110 | Train loss: 4.84 | Val loss: 5.56\n",
      "Epoch 3120 | Train loss: 4.79 | Val loss: 5.38\n",
      "Epoch 3130 | Train loss: 4.84 | Val loss: 5.37\n",
      "New best model saved with val loss: 5.24\n",
      "Epoch 3140 | Train loss: 4.76 | Val loss: 5.40\n",
      "Epoch 3150 | Train loss: 4.79 | Val loss: 5.32\n",
      "Epoch 3160 | Train loss: 4.77 | Val loss: 5.40\n",
      "Epoch 3170 | Train loss: 4.86 | Val loss: 5.39\n",
      "New best model saved with val loss: 5.22\n",
      "Epoch 3180 | Train loss: 4.78 | Val loss: 5.46\n",
      "Epoch 3190 | Train loss: 4.74 | Val loss: 5.40\n",
      "Epoch 3200 | Train loss: 4.78 | Val loss: 5.53\n",
      "Epoch 3210 | Train loss: 4.74 | Val loss: 5.52\n",
      "Epoch 3220 | Train loss: 4.77 | Val loss: 5.42\n",
      "Epoch 3230 | Train loss: 4.76 | Val loss: 5.42\n",
      "Epoch 3240 | Train loss: 4.79 | Val loss: 5.48\n",
      "Epoch 3250 | Train loss: 4.77 | Val loss: 5.37\n",
      "Epoch 3260 | Train loss: 4.74 | Val loss: 5.52\n",
      "Epoch 3270 | Train loss: 4.72 | Val loss: 5.38\n",
      "Epoch 3280 | Train loss: 4.73 | Val loss: 5.45\n",
      "Epoch 3290 | Train loss: 4.79 | Val loss: 5.57\n",
      "Epoch 3300 | Train loss: 4.74 | Val loss: 5.50\n",
      "Epoch 3310 | Train loss: 4.72 | Val loss: 5.37\n",
      "New best model saved with val loss: 5.21\n",
      "Epoch 3320 | Train loss: 4.75 | Val loss: 5.21\n",
      "Epoch 3330 | Train loss: 4.75 | Val loss: 5.38\n",
      "Epoch 3340 | Train loss: 4.75 | Val loss: 5.32\n",
      "Epoch 3350 | Train loss: 4.74 | Val loss: 5.48\n",
      "Epoch 3360 | Train loss: 4.71 | Val loss: 5.38\n",
      "Epoch 3370 | Train loss: 4.71 | Val loss: 5.41\n",
      "Epoch 3380 | Train loss: 4.74 | Val loss: 5.43\n",
      "Epoch 3390 | Train loss: 4.79 | Val loss: 5.48\n",
      "Epoch 3400 | Train loss: 4.68 | Val loss: 5.34\n",
      "Epoch 3410 | Train loss: 4.73 | Val loss: 5.48\n",
      "Epoch 3420 | Train loss: 4.73 | Val loss: 5.43\n",
      "Epoch 3430 | Train loss: 4.72 | Val loss: 5.35\n",
      "Epoch 3440 | Train loss: 4.73 | Val loss: 5.36\n",
      "New best model saved with val loss: 5.20\n",
      "Epoch 3450 | Train loss: 4.69 | Val loss: 5.36\n",
      "Epoch 3460 | Train loss: 4.69 | Val loss: 5.34\n",
      "Epoch 3470 | Train loss: 4.77 | Val loss: 5.49\n",
      "Epoch 3480 | Train loss: 4.71 | Val loss: 5.54\n",
      "Epoch 3490 | Train loss: 4.71 | Val loss: 5.46\n",
      "Epoch 3500 | Train loss: 4.69 | Val loss: 5.35\n",
      "Epoch 3510 | Train loss: 4.68 | Val loss: 5.38\n",
      "Epoch 3520 | Train loss: 4.73 | Val loss: 5.41\n",
      "Epoch 3530 | Train loss: 4.72 | Val loss: 5.35\n",
      "Epoch 3540 | Train loss: 4.74 | Val loss: 5.46\n",
      "Epoch 3550 | Train loss: 4.73 | Val loss: 5.32\n",
      "New best model saved with val loss: 5.17\n",
      "Epoch 3560 | Train loss: 4.78 | Val loss: 5.33\n",
      "Epoch 3570 | Train loss: 4.72 | Val loss: 5.48\n",
      "Epoch 3580 | Train loss: 4.74 | Val loss: 5.38\n",
      "Epoch 3590 | Train loss: 4.66 | Val loss: 5.45\n",
      "Epoch 3600 | Train loss: 4.64 | Val loss: 5.32\n",
      "Epoch 3610 | Train loss: 4.76 | Val loss: 5.27\n",
      "Epoch 3620 | Train loss: 4.62 | Val loss: 5.46\n",
      "Epoch 3630 | Train loss: 4.68 | Val loss: 5.44\n",
      "Epoch 3640 | Train loss: 4.71 | Val loss: 5.25\n",
      "Epoch 3650 | Train loss: 4.65 | Val loss: 5.37\n",
      "Epoch 3660 | Train loss: 4.70 | Val loss: 5.48\n",
      "Epoch 3670 | Train loss: 4.68 | Val loss: 5.33\n",
      "Epoch 3680 | Train loss: 4.70 | Val loss: 5.43\n",
      "Epoch 3690 | Train loss: 4.68 | Val loss: 5.45\n",
      "Epoch 3700 | Train loss: 4.75 | Val loss: 5.33\n",
      "Epoch 3710 | Train loss: 4.59 | Val loss: 5.41\n",
      "Epoch 3720 | Train loss: 4.64 | Val loss: 5.36\n",
      "Epoch 3730 | Train loss: 4.62 | Val loss: 5.37\n",
      "Epoch 3740 | Train loss: 4.67 | Val loss: 5.38\n",
      "Epoch 3750 | Train loss: 4.67 | Val loss: 5.43\n",
      "Epoch 3760 | Train loss: 4.64 | Val loss: 5.31\n",
      "Epoch 3770 | Train loss: 4.65 | Val loss: 5.45\n",
      "Epoch 3780 | Train loss: 4.67 | Val loss: 5.33\n",
      "Epoch 3790 | Train loss: 4.65 | Val loss: 5.56\n",
      "Epoch 3800 | Train loss: 4.66 | Val loss: 5.41\n",
      "New best model saved with val loss: 5.15\n",
      "Epoch 3810 | Train loss: 4.67 | Val loss: 5.15\n",
      "Epoch 3820 | Train loss: 4.68 | Val loss: 5.25\n",
      "Epoch 3830 | Train loss: 4.66 | Val loss: 5.40\n",
      "Epoch 3840 | Train loss: 4.66 | Val loss: 5.42\n",
      "Epoch 3850 | Train loss: 4.65 | Val loss: 5.50\n",
      "Epoch 3860 | Train loss: 4.65 | Val loss: 5.42\n",
      "Epoch 3870 | Train loss: 4.66 | Val loss: 5.42\n",
      "Epoch 3880 | Train loss: 4.63 | Val loss: 5.37\n",
      "Epoch 3890 | Train loss: 4.61 | Val loss: 5.29\n",
      "Epoch 3900 | Train loss: 4.64 | Val loss: 5.46\n",
      "Epoch 3910 | Train loss: 4.64 | Val loss: 5.39\n",
      "Epoch 3920 | Train loss: 4.63 | Val loss: 5.37\n",
      "Epoch 3930 | Train loss: 4.66 | Val loss: 5.34\n",
      "Epoch 3940 | Train loss: 4.60 | Val loss: 5.33\n",
      "Epoch 3950 | Train loss: 4.67 | Val loss: 5.35\n",
      "Epoch 3960 | Train loss: 4.63 | Val loss: 5.37\n",
      "Epoch 3970 | Train loss: 4.66 | Val loss: 5.27\n",
      "Epoch 3980 | Train loss: 4.66 | Val loss: 5.32\n",
      "Epoch 3990 | Train loss: 4.61 | Val loss: 5.34\n",
      "Epoch 4000 | Train loss: 4.62 | Val loss: 5.44\n",
      "Epoch 4010 | Train loss: 4.61 | Val loss: 5.33\n",
      "Epoch 4020 | Train loss: 4.68 | Val loss: 5.38\n",
      "Epoch 4030 | Train loss: 4.60 | Val loss: 5.40\n",
      "Epoch 4040 | Train loss: 4.66 | Val loss: 5.40\n",
      "Epoch 4050 | Train loss: 4.56 | Val loss: 5.24\n",
      "Epoch 4060 | Train loss: 4.69 | Val loss: 5.43\n",
      "Epoch 4070 | Train loss: 4.62 | Val loss: 5.31\n",
      "Epoch 4080 | Train loss: 4.62 | Val loss: 5.47\n",
      "Epoch 4090 | Train loss: 4.59 | Val loss: 5.42\n",
      "Epoch 4100 | Train loss: 4.60 | Val loss: 5.51\n",
      "Epoch 4110 | Train loss: 4.63 | Val loss: 5.36\n",
      "Epoch 4120 | Train loss: 4.59 | Val loss: 5.39\n",
      "Epoch 4130 | Train loss: 4.63 | Val loss: 5.36\n",
      "Epoch 4140 | Train loss: 4.61 | Val loss: 5.41\n",
      "Epoch 4150 | Train loss: 4.63 | Val loss: 5.46\n",
      "Epoch 4160 | Train loss: 4.63 | Val loss: 5.38\n",
      "Epoch 4170 | Train loss: 4.63 | Val loss: 5.33\n",
      "Epoch 4180 | Train loss: 4.64 | Val loss: 5.26\n",
      "Epoch 4190 | Train loss: 4.67 | Val loss: 5.35\n",
      "Epoch 4200 | Train loss: 4.65 | Val loss: 5.21\n",
      "Epoch 4210 | Train loss: 4.62 | Val loss: 5.44\n",
      "Epoch 4220 | Train loss: 4.58 | Val loss: 5.34\n",
      "Epoch 4230 | Train loss: 4.61 | Val loss: 5.38\n",
      "Epoch 4240 | Train loss: 4.61 | Val loss: 5.42\n",
      "Epoch 4250 | Train loss: 4.62 | Val loss: 5.36\n",
      "Epoch 4260 | Train loss: 4.59 | Val loss: 5.38\n",
      "Epoch 4270 | Train loss: 4.62 | Val loss: 5.40\n",
      "Epoch 4280 | Train loss: 4.67 | Val loss: 5.48\n",
      "Epoch 4290 | Train loss: 4.61 | Val loss: 5.33\n",
      "Epoch 4300 | Train loss: 4.58 | Val loss: 5.36\n",
      "Epoch 4310 | Train loss: 4.56 | Val loss: 5.42\n",
      "Epoch 4320 | Train loss: 4.59 | Val loss: 5.35\n",
      "Epoch 4330 | Train loss: 4.57 | Val loss: 5.53\n",
      "Epoch 4340 | Train loss: 4.64 | Val loss: 5.34\n",
      "Epoch 4350 | Train loss: 4.61 | Val loss: 5.31\n",
      "Epoch 4360 | Train loss: 4.53 | Val loss: 5.41\n",
      "Epoch 4370 | Train loss: 4.57 | Val loss: 5.43\n",
      "New best model saved with val loss: 5.14\n",
      "Epoch 4380 | Train loss: 4.61 | Val loss: 5.31\n",
      "Epoch 4390 | Train loss: 4.57 | Val loss: 5.37\n",
      "Epoch 4400 | Train loss: 4.56 | Val loss: 5.39\n",
      "Epoch 4410 | Train loss: 4.58 | Val loss: 5.33\n",
      "Epoch 4420 | Train loss: 4.58 | Val loss: 5.44\n",
      "Epoch 4430 | Train loss: 4.58 | Val loss: 5.30\n",
      "Epoch 4440 | Train loss: 4.58 | Val loss: 5.32\n",
      "Epoch 4450 | Train loss: 4.52 | Val loss: 5.27\n",
      "Epoch 4460 | Train loss: 4.55 | Val loss: 5.32\n",
      "Epoch 4470 | Train loss: 4.56 | Val loss: 5.25\n",
      "New best model saved with val loss: 5.12\n",
      "Epoch 4480 | Train loss: 4.54 | Val loss: 5.22\n",
      "Epoch 4490 | Train loss: 4.60 | Val loss: 5.34\n",
      "Epoch 4500 | Train loss: 4.63 | Val loss: 5.37\n",
      "Epoch 4510 | Train loss: 4.56 | Val loss: 5.33\n",
      "Epoch 4520 | Train loss: 4.55 | Val loss: 5.33\n",
      "Epoch 4530 | Train loss: 4.65 | Val loss: 5.35\n",
      "Epoch 4540 | Train loss: 4.58 | Val loss: 5.38\n",
      "Epoch 4550 | Train loss: 4.62 | Val loss: 5.29\n",
      "Epoch 4560 | Train loss: 4.57 | Val loss: 5.27\n",
      "Epoch 4570 | Train loss: 4.57 | Val loss: 5.37\n",
      "Epoch 4580 | Train loss: 4.50 | Val loss: 5.32\n",
      "Epoch 4590 | Train loss: 4.57 | Val loss: 5.35\n",
      "Epoch 4600 | Train loss: 4.57 | Val loss: 5.38\n",
      "Epoch 4610 | Train loss: 4.55 | Val loss: 5.14\n",
      "Epoch 4620 | Train loss: 4.57 | Val loss: 5.29\n",
      "Epoch 4630 | Train loss: 4.54 | Val loss: 5.32\n",
      "Epoch 4640 | Train loss: 4.57 | Val loss: 5.49\n",
      "Epoch 4650 | Train loss: 4.55 | Val loss: 5.35\n",
      "Epoch 4660 | Train loss: 4.56 | Val loss: 5.31\n",
      "Epoch 4670 | Train loss: 4.54 | Val loss: 5.39\n",
      "Epoch 4680 | Train loss: 4.56 | Val loss: 5.48\n",
      "Epoch 4690 | Train loss: 4.59 | Val loss: 5.21\n",
      "Epoch 4700 | Train loss: 4.53 | Val loss: 5.35\n",
      "Epoch 4710 | Train loss: 4.55 | Val loss: 5.27\n",
      "Epoch 4720 | Train loss: 4.56 | Val loss: 5.24\n",
      "Epoch 4730 | Train loss: 4.50 | Val loss: 5.39\n",
      "Epoch 4740 | Train loss: 4.55 | Val loss: 5.24\n",
      "Epoch 4750 | Train loss: 4.53 | Val loss: 5.30\n",
      "Epoch 4760 | Train loss: 4.51 | Val loss: 5.32\n",
      "Epoch 4770 | Train loss: 4.55 | Val loss: 5.40\n",
      "Epoch 4780 | Train loss: 4.53 | Val loss: 5.31\n",
      "Epoch 4790 | Train loss: 4.57 | Val loss: 5.29\n",
      "Epoch 4800 | Train loss: 4.53 | Val loss: 5.29\n",
      "Epoch 4810 | Train loss: 4.52 | Val loss: 5.36\n",
      "Epoch 4820 | Train loss: 4.56 | Val loss: 5.28\n",
      "Epoch 4830 | Train loss: 4.52 | Val loss: 5.28\n",
      "Epoch 4840 | Train loss: 4.45 | Val loss: 5.37\n",
      "Epoch 4850 | Train loss: 4.52 | Val loss: 5.30\n",
      "Epoch 4860 | Train loss: 4.52 | Val loss: 5.26\n",
      "Epoch 4870 | Train loss: 4.52 | Val loss: 5.21\n",
      "Epoch 4880 | Train loss: 4.50 | Val loss: 5.21\n",
      "Epoch 4890 | Train loss: 4.52 | Val loss: 5.18\n",
      "Epoch 4900 | Train loss: 4.57 | Val loss: 5.35\n",
      "Epoch 4910 | Train loss: 4.54 | Val loss: 5.49\n",
      "Epoch 4920 | Train loss: 4.62 | Val loss: 5.38\n",
      "Epoch 4930 | Train loss: 4.51 | Val loss: 5.36\n",
      "Epoch 4940 | Train loss: 4.55 | Val loss: 5.39\n",
      "Epoch 4950 | Train loss: 4.54 | Val loss: 5.32\n",
      "Epoch 4960 | Train loss: 4.53 | Val loss: 5.31\n",
      "Epoch 4970 | Train loss: 4.53 | Val loss: 5.39\n",
      "Epoch 4980 | Train loss: 4.55 | Val loss: 5.41\n",
      "Epoch 4990 | Train loss: 4.56 | Val loss: 5.24\n",
      "Epoch 5000 | Train loss: 4.51 | Val loss: 5.21\n",
      "Epoch 5010 | Train loss: 4.55 | Val loss: 5.32\n",
      "New best model saved with val loss: 5.11\n",
      "Epoch 5020 | Train loss: 4.49 | Val loss: 5.33\n",
      "Epoch 5030 | Train loss: 4.56 | Val loss: 5.35\n",
      "Epoch 5040 | Train loss: 4.51 | Val loss: 5.35\n",
      "Epoch 5050 | Train loss: 4.50 | Val loss: 5.31\n",
      "Epoch 5060 | Train loss: 4.50 | Val loss: 5.30\n",
      "Epoch 5070 | Train loss: 4.59 | Val loss: 5.37\n",
      "Epoch 5080 | Train loss: 4.53 | Val loss: 5.23\n",
      "New best model saved with val loss: 5.08\n",
      "Epoch 5090 | Train loss: 4.44 | Val loss: 5.32\n",
      "Epoch 5100 | Train loss: 4.48 | Val loss: 5.26\n",
      "Epoch 5110 | Train loss: 4.49 | Val loss: 5.32\n",
      "Epoch 5120 | Train loss: 4.51 | Val loss: 5.47\n",
      "Epoch 5130 | Train loss: 4.49 | Val loss: 5.52\n",
      "Epoch 5140 | Train loss: 4.56 | Val loss: 5.18\n",
      "Epoch 5150 | Train loss: 4.50 | Val loss: 5.35\n",
      "Epoch 5160 | Train loss: 4.53 | Val loss: 5.36\n",
      "Epoch 5170 | Train loss: 4.52 | Val loss: 5.17\n",
      "Epoch 5180 | Train loss: 4.46 | Val loss: 5.43\n",
      "Epoch 5190 | Train loss: 4.51 | Val loss: 5.32\n",
      "Epoch 5200 | Train loss: 4.54 | Val loss: 5.28\n",
      "Epoch 5210 | Train loss: 4.41 | Val loss: 5.29\n",
      "Epoch 5220 | Train loss: 4.49 | Val loss: 5.24\n",
      "Epoch 5230 | Train loss: 4.48 | Val loss: 5.29\n",
      "Epoch 5240 | Train loss: 4.54 | Val loss: 5.31\n",
      "Epoch 5250 | Train loss: 4.51 | Val loss: 5.30\n",
      "Epoch 5260 | Train loss: 4.52 | Val loss: 5.17\n",
      "Epoch 5270 | Train loss: 4.48 | Val loss: 5.32\n",
      "Epoch 5280 | Train loss: 4.48 | Val loss: 5.39\n",
      "Epoch 5290 | Train loss: 4.51 | Val loss: 5.24\n",
      "Epoch 5300 | Train loss: 4.52 | Val loss: 5.38\n",
      "Epoch 5310 | Train loss: 4.50 | Val loss: 5.22\n",
      "Epoch 5320 | Train loss: 4.48 | Val loss: 5.33\n",
      "Epoch 5330 | Train loss: 4.52 | Val loss: 5.38\n",
      "Epoch 5340 | Train loss: 4.55 | Val loss: 5.32\n",
      "Epoch 5350 | Train loss: 4.45 | Val loss: 5.53\n",
      "Epoch 5360 | Train loss: 4.47 | Val loss: 5.41\n",
      "Epoch 5370 | Train loss: 4.49 | Val loss: 5.31\n",
      "Epoch 5380 | Train loss: 4.51 | Val loss: 5.29\n",
      "Epoch 5390 | Train loss: 4.49 | Val loss: 5.37\n",
      "Epoch 5400 | Train loss: 4.48 | Val loss: 5.27\n",
      "Epoch 5410 | Train loss: 4.44 | Val loss: 5.32\n",
      "Epoch 5420 | Train loss: 4.55 | Val loss: 5.35\n",
      "Epoch 5430 | Train loss: 4.45 | Val loss: 5.36\n",
      "Epoch 5440 | Train loss: 4.49 | Val loss: 5.23\n",
      "Epoch 5450 | Train loss: 4.45 | Val loss: 5.21\n",
      "Epoch 5460 | Train loss: 4.49 | Val loss: 5.18\n",
      "Epoch 5470 | Train loss: 4.43 | Val loss: 5.31\n",
      "Epoch 5480 | Train loss: 4.53 | Val loss: 5.41\n",
      "Epoch 5490 | Train loss: 4.50 | Val loss: 5.30\n",
      "Epoch 5500 | Train loss: 4.46 | Val loss: 5.35\n",
      "Epoch 5510 | Train loss: 4.50 | Val loss: 5.28\n",
      "Epoch 5520 | Train loss: 4.50 | Val loss: 5.37\n",
      "Epoch 5530 | Train loss: 4.38 | Val loss: 5.33\n",
      "Epoch 5540 | Train loss: 4.46 | Val loss: 5.26\n",
      "Epoch 5550 | Train loss: 4.45 | Val loss: 5.38\n",
      "Epoch 5560 | Train loss: 4.43 | Val loss: 5.29\n",
      "Epoch 5570 | Train loss: 4.45 | Val loss: 5.24\n",
      "Epoch 5580 | Train loss: 4.46 | Val loss: 5.31\n",
      "Epoch 5590 | Train loss: 4.44 | Val loss: 5.29\n",
      "Epoch 5600 | Train loss: 4.48 | Val loss: 5.31\n",
      "Epoch 5610 | Train loss: 4.44 | Val loss: 5.29\n",
      "Epoch 5620 | Train loss: 4.47 | Val loss: 5.32\n",
      "Epoch 5630 | Train loss: 4.54 | Val loss: 5.24\n",
      "Epoch 5640 | Train loss: 4.46 | Val loss: 5.28\n",
      "Epoch 5650 | Train loss: 4.50 | Val loss: 5.17\n",
      "Epoch 5660 | Train loss: 4.46 | Val loss: 5.27\n",
      "Epoch 5670 | Train loss: 4.50 | Val loss: 5.36\n",
      "Epoch 5680 | Train loss: 4.42 | Val loss: 5.36\n",
      "Epoch 5690 | Train loss: 4.42 | Val loss: 5.31\n",
      "Epoch 5700 | Train loss: 4.52 | Val loss: 5.34\n",
      "Epoch 5710 | Train loss: 4.42 | Val loss: 5.23\n",
      "Epoch 5720 | Train loss: 4.45 | Val loss: 5.28\n",
      "Epoch 5730 | Train loss: 4.47 | Val loss: 5.33\n",
      "Epoch 5740 | Train loss: 4.46 | Val loss: 5.29\n",
      "Epoch 5750 | Train loss: 4.42 | Val loss: 5.19\n",
      "Epoch 5760 | Train loss: 4.49 | Val loss: 5.38\n",
      "Epoch 5770 | Train loss: 4.50 | Val loss: 5.25\n",
      "Epoch 5780 | Train loss: 4.41 | Val loss: 5.31\n",
      "Epoch 5790 | Train loss: 4.44 | Val loss: 5.37\n",
      "Epoch 5800 | Train loss: 4.50 | Val loss: 5.29\n",
      "Epoch 5810 | Train loss: 4.46 | Val loss: 5.29\n",
      "Epoch 5820 | Train loss: 4.49 | Val loss: 5.35\n",
      "Epoch 5830 | Train loss: 4.44 | Val loss: 5.32\n",
      "Epoch 5840 | Train loss: 4.47 | Val loss: 5.33\n",
      "Epoch 5850 | Train loss: 4.45 | Val loss: 5.23\n",
      "Epoch 5860 | Train loss: 4.43 | Val loss: 5.35\n",
      "Epoch 5870 | Train loss: 4.45 | Val loss: 5.30\n",
      "Epoch 5880 | Train loss: 4.45 | Val loss: 5.24\n",
      "Epoch 5890 | Train loss: 4.45 | Val loss: 5.34\n",
      "Epoch 5900 | Train loss: 4.42 | Val loss: 5.32\n",
      "Epoch 5910 | Train loss: 4.40 | Val loss: 5.19\n",
      "Epoch 5920 | Train loss: 4.44 | Val loss: 5.20\n",
      "Epoch 5930 | Train loss: 4.45 | Val loss: 5.42\n",
      "Epoch 5940 | Train loss: 4.49 | Val loss: 5.39\n",
      "Epoch 5950 | Train loss: 4.48 | Val loss: 5.32\n",
      "Epoch 5960 | Train loss: 4.43 | Val loss: 5.38\n",
      "Epoch 5970 | Train loss: 4.47 | Val loss: 5.40\n",
      "Epoch 5980 | Train loss: 4.48 | Val loss: 5.29\n",
      "Epoch 5990 | Train loss: 4.50 | Val loss: 5.26\n",
      "Epoch 6000 | Train loss: 4.43 | Val loss: 5.25\n",
      "Epoch 6010 | Train loss: 4.47 | Val loss: 5.27\n",
      "Epoch 6020 | Train loss: 4.45 | Val loss: 5.42\n",
      "Epoch 6030 | Train loss: 4.47 | Val loss: 5.25\n",
      "Epoch 6040 | Train loss: 4.49 | Val loss: 5.29\n",
      "Epoch 6050 | Train loss: 4.43 | Val loss: 5.21\n",
      "Epoch 6060 | Train loss: 4.42 | Val loss: 5.30\n",
      "Epoch 6070 | Train loss: 4.43 | Val loss: 5.31\n",
      "Epoch 6080 | Train loss: 4.42 | Val loss: 5.30\n",
      "Epoch 6090 | Train loss: 4.42 | Val loss: 5.22\n",
      "Epoch 6100 | Train loss: 4.47 | Val loss: 5.36\n",
      "Epoch 6110 | Train loss: 4.42 | Val loss: 5.11\n",
      "Epoch 6120 | Train loss: 4.47 | Val loss: 5.28\n",
      "Epoch 6130 | Train loss: 4.41 | Val loss: 5.13\n",
      "Epoch 6140 | Train loss: 4.43 | Val loss: 5.43\n",
      "Epoch 6150 | Train loss: 4.37 | Val loss: 5.28\n",
      "Epoch 6160 | Train loss: 4.46 | Val loss: 5.41\n",
      "Epoch 6170 | Train loss: 4.43 | Val loss: 5.16\n",
      "Epoch 6180 | Train loss: 4.45 | Val loss: 5.42\n",
      "Epoch 6190 | Train loss: 4.41 | Val loss: 5.23\n",
      "Epoch 6200 | Train loss: 4.37 | Val loss: 5.34\n",
      "Epoch 6210 | Train loss: 4.44 | Val loss: 5.40\n",
      "Epoch 6220 | Train loss: 4.40 | Val loss: 5.32\n",
      "Epoch 6230 | Train loss: 4.44 | Val loss: 5.34\n",
      "Epoch 6240 | Train loss: 4.41 | Val loss: 5.29\n",
      "Epoch 6250 | Train loss: 4.43 | Val loss: 5.28\n",
      "Epoch 6260 | Train loss: 4.42 | Val loss: 5.23\n",
      "Epoch 6270 | Train loss: 4.49 | Val loss: 5.41\n",
      "Epoch 6280 | Train loss: 4.37 | Val loss: 5.22\n",
      "Epoch 6290 | Train loss: 4.47 | Val loss: 5.26\n",
      "Epoch 6300 | Train loss: 4.41 | Val loss: 5.31\n",
      "Epoch 6310 | Train loss: 4.43 | Val loss: 5.42\n",
      "Epoch 6320 | Train loss: 4.45 | Val loss: 5.37\n",
      "Epoch 6330 | Train loss: 4.42 | Val loss: 5.32\n",
      "Epoch 6340 | Train loss: 4.41 | Val loss: 5.40\n",
      "Epoch 6350 | Train loss: 4.42 | Val loss: 5.28\n",
      "Epoch 6360 | Train loss: 4.43 | Val loss: 5.10\n",
      "Epoch 6370 | Train loss: 4.36 | Val loss: 5.32\n",
      "Epoch 6380 | Train loss: 4.41 | Val loss: 5.27\n",
      "Epoch 6390 | Train loss: 4.41 | Val loss: 5.28\n",
      "Epoch 6400 | Train loss: 4.47 | Val loss: 5.19\n",
      "Epoch 6410 | Train loss: 4.45 | Val loss: 5.30\n",
      "New best model saved with val loss: 5.06\n",
      "Epoch 6420 | Train loss: 4.37 | Val loss: 5.20\n",
      "Epoch 6430 | Train loss: 4.43 | Val loss: 5.32\n",
      "Epoch 6440 | Train loss: 4.41 | Val loss: 5.16\n",
      "Epoch 6450 | Train loss: 4.42 | Val loss: 5.33\n",
      "Epoch 6460 | Train loss: 4.44 | Val loss: 5.35\n",
      "Epoch 6470 | Train loss: 4.45 | Val loss: 5.27\n",
      "Epoch 6480 | Train loss: 4.40 | Val loss: 5.33\n",
      "Epoch 6490 | Train loss: 4.45 | Val loss: 5.39\n",
      "Epoch 6500 | Train loss: 4.42 | Val loss: 5.40\n",
      "Epoch 6510 | Train loss: 4.36 | Val loss: 5.33\n",
      "Epoch 6520 | Train loss: 4.46 | Val loss: 5.31\n",
      "Epoch 6530 | Train loss: 4.41 | Val loss: 5.32\n",
      "Epoch 6540 | Train loss: 4.42 | Val loss: 5.35\n",
      "Epoch 6550 | Train loss: 4.39 | Val loss: 5.36\n",
      "Epoch 6560 | Train loss: 4.42 | Val loss: 5.30\n",
      "Epoch 6570 | Train loss: 4.42 | Val loss: 5.38\n",
      "Epoch 6580 | Train loss: 4.39 | Val loss: 5.41\n",
      "Epoch 6590 | Train loss: 4.37 | Val loss: 5.38\n",
      "Epoch 6600 | Train loss: 4.43 | Val loss: 5.15\n",
      "Epoch 6610 | Train loss: 4.39 | Val loss: 5.17\n",
      "Epoch 6620 | Train loss: 4.41 | Val loss: 5.39\n",
      "Epoch 6630 | Train loss: 4.41 | Val loss: 5.45\n",
      "Epoch 6640 | Train loss: 4.41 | Val loss: 5.35\n",
      "Epoch 6650 | Train loss: 4.38 | Val loss: 5.28\n",
      "Epoch 6660 | Train loss: 4.35 | Val loss: 5.31\n",
      "Epoch 6670 | Train loss: 4.40 | Val loss: 5.27\n",
      "Epoch 6680 | Train loss: 4.41 | Val loss: 5.34\n",
      "Epoch 6690 | Train loss: 4.32 | Val loss: 5.42\n",
      "Epoch 6700 | Train loss: 4.38 | Val loss: 5.33\n",
      "Epoch 6710 | Train loss: 4.43 | Val loss: 5.35\n",
      "Epoch 6720 | Train loss: 4.36 | Val loss: 5.26\n",
      "Epoch 6730 | Train loss: 4.41 | Val loss: 5.20\n",
      "Epoch 6740 | Train loss: 4.41 | Val loss: 5.29\n",
      "Epoch 6750 | Train loss: 4.37 | Val loss: 5.32\n",
      "Epoch 6760 | Train loss: 4.33 | Val loss: 5.20\n",
      "Epoch 6770 | Train loss: 4.42 | Val loss: 5.23\n",
      "Epoch 6780 | Train loss: 4.39 | Val loss: 5.47\n",
      "Epoch 6790 | Train loss: 4.37 | Val loss: 5.33\n",
      "Epoch 6800 | Train loss: 4.37 | Val loss: 5.30\n",
      "Epoch 6810 | Train loss: 4.37 | Val loss: 5.39\n",
      "Epoch 6820 | Train loss: 4.36 | Val loss: 5.34\n",
      "Epoch 6830 | Train loss: 4.42 | Val loss: 5.26\n",
      "Epoch 6840 | Train loss: 4.39 | Val loss: 5.23\n",
      "Epoch 6850 | Train loss: 4.37 | Val loss: 5.41\n",
      "Epoch 6860 | Train loss: 4.40 | Val loss: 5.19\n",
      "Epoch 6870 | Train loss: 4.39 | Val loss: 5.17\n",
      "Epoch 6880 | Train loss: 4.40 | Val loss: 5.35\n",
      "Epoch 6890 | Train loss: 4.45 | Val loss: 5.22\n",
      "Epoch 6900 | Train loss: 4.42 | Val loss: 5.31\n",
      "Epoch 6910 | Train loss: 4.36 | Val loss: 5.34\n",
      "Epoch 6920 | Train loss: 4.39 | Val loss: 5.35\n",
      "Epoch 6930 | Train loss: 4.44 | Val loss: 5.41\n",
      "Epoch 6940 | Train loss: 4.41 | Val loss: 5.25\n",
      "Epoch 6950 | Train loss: 4.41 | Val loss: 5.28\n",
      "Epoch 6960 | Train loss: 4.33 | Val loss: 5.36\n",
      "Epoch 6970 | Train loss: 4.38 | Val loss: 5.27\n",
      "Epoch 6980 | Train loss: 4.42 | Val loss: 5.26\n",
      "Epoch 6990 | Train loss: 4.42 | Val loss: 5.29\n",
      "Epoch 7000 | Train loss: 4.43 | Val loss: 5.31\n",
      "Epoch 7010 | Train loss: 4.39 | Val loss: 5.30\n",
      "Epoch 7020 | Train loss: 4.44 | Val loss: 5.24\n",
      "Epoch 7030 | Train loss: 4.38 | Val loss: 5.24\n",
      "Epoch 7040 | Train loss: 4.40 | Val loss: 5.28\n",
      "Epoch 7050 | Train loss: 4.34 | Val loss: 5.31\n",
      "Epoch 7060 | Train loss: 4.36 | Val loss: 5.36\n",
      "Epoch 7070 | Train loss: 4.37 | Val loss: 5.32\n",
      "Epoch 7080 | Train loss: 4.38 | Val loss: 5.32\n",
      "Epoch 7090 | Train loss: 4.35 | Val loss: 5.42\n",
      "Epoch 7100 | Train loss: 4.43 | Val loss: 5.39\n",
      "Epoch 7110 | Train loss: 4.39 | Val loss: 5.26\n",
      "Epoch 7120 | Train loss: 4.41 | Val loss: 5.30\n",
      "Epoch 7130 | Train loss: 4.38 | Val loss: 5.32\n",
      "Epoch 7140 | Train loss: 4.39 | Val loss: 5.44\n",
      "Epoch 7150 | Train loss: 4.38 | Val loss: 5.25\n",
      "Epoch 7160 | Train loss: 4.36 | Val loss: 5.32\n",
      "Epoch 7170 | Train loss: 4.39 | Val loss: 5.11\n",
      "Epoch 7180 | Train loss: 4.40 | Val loss: 5.41\n",
      "Epoch 7190 | Train loss: 4.37 | Val loss: 5.36\n",
      "Epoch 7200 | Train loss: 4.39 | Val loss: 5.43\n",
      "Epoch 7210 | Train loss: 4.38 | Val loss: 5.24\n",
      "Epoch 7220 | Train loss: 4.40 | Val loss: 5.21\n",
      "Epoch 7230 | Train loss: 4.38 | Val loss: 5.30\n",
      "Epoch 7240 | Train loss: 4.39 | Val loss: 5.28\n",
      "Epoch 7250 | Train loss: 4.38 | Val loss: 5.35\n",
      "Epoch 7260 | Train loss: 4.37 | Val loss: 5.20\n",
      "Epoch 7270 | Train loss: 4.38 | Val loss: 5.34\n",
      "Epoch 7280 | Train loss: 4.39 | Val loss: 5.31\n",
      "Epoch 7290 | Train loss: 4.39 | Val loss: 5.38\n",
      "Epoch 7300 | Train loss: 4.38 | Val loss: 5.21\n",
      "Epoch 7310 | Train loss: 4.42 | Val loss: 5.27\n",
      "Epoch 7320 | Train loss: 4.41 | Val loss: 5.32\n",
      "Epoch 7330 | Train loss: 4.42 | Val loss: 5.28\n",
      "Epoch 7340 | Train loss: 4.34 | Val loss: 5.26\n",
      "Epoch 7350 | Train loss: 4.39 | Val loss: 5.21\n",
      "Epoch 7360 | Train loss: 4.42 | Val loss: 5.27\n",
      "Epoch 7370 | Train loss: 4.33 | Val loss: 5.26\n",
      "Epoch 7380 | Train loss: 4.40 | Val loss: 5.31\n",
      "Epoch 7390 | Train loss: 4.41 | Val loss: 5.57\n",
      "Epoch 7400 | Train loss: 4.37 | Val loss: 5.24\n",
      "Epoch 7410 | Train loss: 4.38 | Val loss: 5.25\n",
      "Epoch 7420 | Train loss: 4.42 | Val loss: 5.38\n",
      "Epoch 7430 | Train loss: 4.38 | Val loss: 5.20\n",
      "Epoch 7440 | Train loss: 4.46 | Val loss: 5.36\n",
      "Epoch 7450 | Train loss: 4.44 | Val loss: 5.30\n",
      "Epoch 7460 | Train loss: 4.40 | Val loss: 5.26\n",
      "Epoch 7470 | Train loss: 4.30 | Val loss: 5.30\n",
      "Epoch 7480 | Train loss: 4.33 | Val loss: 5.38\n",
      "Epoch 7490 | Train loss: 4.32 | Val loss: 5.25\n",
      "Epoch 7500 | Train loss: 4.37 | Val loss: 5.36\n",
      "Epoch 7510 | Train loss: 4.42 | Val loss: 5.40\n",
      "Epoch 7520 | Train loss: 4.37 | Val loss: 5.30\n",
      "Epoch 7530 | Train loss: 4.38 | Val loss: 5.22\n",
      "Epoch 7540 | Train loss: 4.36 | Val loss: 5.22\n",
      "Epoch 7550 | Train loss: 4.32 | Val loss: 5.19\n",
      "Epoch 7560 | Train loss: 4.39 | Val loss: 5.36\n",
      "Epoch 7570 | Train loss: 4.39 | Val loss: 5.33\n",
      "Epoch 7580 | Train loss: 4.36 | Val loss: 5.26\n",
      "Epoch 7590 | Train loss: 4.34 | Val loss: 5.46\n",
      "Epoch 7600 | Train loss: 4.36 | Val loss: 5.27\n",
      "Epoch 7610 | Train loss: 4.35 | Val loss: 5.32\n",
      "Epoch 7620 | Train loss: 4.33 | Val loss: 5.41\n",
      "Epoch 7630 | Train loss: 4.36 | Val loss: 5.26\n",
      "Epoch 7640 | Train loss: 4.36 | Val loss: 5.44\n",
      "Epoch 7650 | Train loss: 4.32 | Val loss: 5.22\n",
      "Epoch 7660 | Train loss: 4.35 | Val loss: 5.31\n",
      "Epoch 7670 | Train loss: 4.40 | Val loss: 5.28\n",
      "Epoch 7680 | Train loss: 4.40 | Val loss: 5.43\n",
      "Epoch 7690 | Train loss: 4.39 | Val loss: 5.40\n",
      "Epoch 7700 | Train loss: 4.35 | Val loss: 5.32\n",
      "Epoch 7710 | Train loss: 4.35 | Val loss: 5.44\n",
      "Epoch 7720 | Train loss: 4.31 | Val loss: 5.22\n",
      "Epoch 7730 | Train loss: 4.33 | Val loss: 5.30\n",
      "Epoch 7740 | Train loss: 4.35 | Val loss: 5.26\n",
      "Epoch 7750 | Train loss: 4.35 | Val loss: 5.30\n",
      "Epoch 7760 | Train loss: 4.34 | Val loss: 5.16\n",
      "Epoch 7770 | Train loss: 4.40 | Val loss: 5.30\n",
      "Epoch 7780 | Train loss: 4.32 | Val loss: 5.23\n",
      "Epoch 7790 | Train loss: 4.39 | Val loss: 5.28\n",
      "Epoch 7800 | Train loss: 4.34 | Val loss: 5.17\n",
      "Epoch 7810 | Train loss: 4.37 | Val loss: 5.40\n",
      "Epoch 7820 | Train loss: 4.38 | Val loss: 5.29\n",
      "Epoch 7830 | Train loss: 4.34 | Val loss: 5.43\n",
      "Epoch 7840 | Train loss: 4.36 | Val loss: 5.33\n",
      "Epoch 7850 | Train loss: 4.36 | Val loss: 5.29\n",
      "Epoch 7860 | Train loss: 4.39 | Val loss: 5.30\n",
      "Epoch 7870 | Train loss: 4.34 | Val loss: 5.12\n",
      "Epoch 7880 | Train loss: 4.34 | Val loss: 5.23\n",
      "Epoch 7890 | Train loss: 4.39 | Val loss: 5.36\n",
      "Epoch 7900 | Train loss: 4.35 | Val loss: 5.12\n",
      "Epoch 7910 | Train loss: 4.34 | Val loss: 5.24\n",
      "Epoch 7920 | Train loss: 4.35 | Val loss: 5.39\n",
      "Epoch 7930 | Train loss: 4.37 | Val loss: 5.35\n",
      "New best model saved with val loss: 5.06\n",
      "Epoch 7940 | Train loss: 4.34 | Val loss: 5.16\n",
      "Epoch 7950 | Train loss: 4.35 | Val loss: 5.26\n",
      "Epoch 7960 | Train loss: 4.34 | Val loss: 5.28\n",
      "Epoch 7970 | Train loss: 4.35 | Val loss: 5.35\n",
      "Epoch 7980 | Train loss: 4.35 | Val loss: 5.31\n",
      "Epoch 7990 | Train loss: 4.32 | Val loss: 5.30\n",
      "Epoch 8000 | Train loss: 4.37 | Val loss: 5.48\n",
      "Epoch 8010 | Train loss: 4.40 | Val loss: 5.17\n",
      "Epoch 8020 | Train loss: 4.37 | Val loss: 5.28\n",
      "Epoch 8030 | Train loss: 4.33 | Val loss: 5.40\n",
      "Epoch 8040 | Train loss: 4.40 | Val loss: 5.39\n",
      "Epoch 8050 | Train loss: 4.33 | Val loss: 5.51\n",
      "Epoch 8060 | Train loss: 4.36 | Val loss: 5.22\n",
      "Epoch 8070 | Train loss: 4.36 | Val loss: 5.38\n",
      "Epoch 8080 | Train loss: 4.42 | Val loss: 5.35\n",
      "Epoch 8090 | Train loss: 4.30 | Val loss: 5.24\n",
      "New best model saved with val loss: 5.06\n",
      "Epoch 8100 | Train loss: 4.35 | Val loss: 5.25\n",
      "Epoch 8110 | Train loss: 4.34 | Val loss: 5.37\n",
      "Epoch 8120 | Train loss: 4.36 | Val loss: 5.24\n",
      "Epoch 8130 | Train loss: 4.33 | Val loss: 5.28\n",
      "Epoch 8140 | Train loss: 4.32 | Val loss: 5.38\n",
      "Epoch 8150 | Train loss: 4.36 | Val loss: 5.23\n",
      "Epoch 8160 | Train loss: 4.35 | Val loss: 5.17\n",
      "Epoch 8170 | Train loss: 4.34 | Val loss: 5.36\n",
      "Epoch 8180 | Train loss: 4.37 | Val loss: 5.16\n",
      "Epoch 8190 | Train loss: 4.34 | Val loss: 5.33\n",
      "Epoch 8200 | Train loss: 4.34 | Val loss: 5.37\n",
      "Epoch 8210 | Train loss: 4.35 | Val loss: 5.20\n",
      "Epoch 8220 | Train loss: 4.35 | Val loss: 5.26\n",
      "Epoch 8230 | Train loss: 4.36 | Val loss: 5.29\n",
      "Epoch 8240 | Train loss: 4.37 | Val loss: 5.20\n",
      "Epoch 8250 | Train loss: 4.36 | Val loss: 5.36\n",
      "Epoch 8260 | Train loss: 4.35 | Val loss: 5.36\n",
      "Epoch 8270 | Train loss: 4.36 | Val loss: 5.30\n",
      "Epoch 8280 | Train loss: 4.37 | Val loss: 5.30\n",
      "Epoch 8290 | Train loss: 4.35 | Val loss: 5.20\n",
      "New best model saved with val loss: 5.03\n",
      "Epoch 8300 | Train loss: 4.38 | Val loss: 5.30\n",
      "Epoch 8310 | Train loss: 4.36 | Val loss: 5.33\n",
      "Epoch 8320 | Train loss: 4.35 | Val loss: 5.26\n",
      "Epoch 8330 | Train loss: 4.31 | Val loss: 5.35\n",
      "Epoch 8340 | Train loss: 4.40 | Val loss: 5.35\n",
      "Epoch 8350 | Train loss: 4.40 | Val loss: 5.25\n",
      "Epoch 8360 | Train loss: 4.34 | Val loss: 5.29\n",
      "Epoch 8370 | Train loss: 4.32 | Val loss: 5.20\n",
      "Epoch 8380 | Train loss: 4.36 | Val loss: 5.37\n",
      "Epoch 8390 | Train loss: 4.33 | Val loss: 5.21\n",
      "Epoch 8400 | Train loss: 4.34 | Val loss: 5.23\n",
      "Epoch 8410 | Train loss: 4.39 | Val loss: 5.28\n",
      "Epoch 8420 | Train loss: 4.41 | Val loss: 5.18\n",
      "Epoch 8430 | Train loss: 4.36 | Val loss: 5.36\n",
      "Epoch 8440 | Train loss: 4.37 | Val loss: 5.33\n",
      "Epoch 8450 | Train loss: 4.42 | Val loss: 5.19\n",
      "Epoch 8460 | Train loss: 4.30 | Val loss: 5.34\n",
      "Epoch 8470 | Train loss: 4.32 | Val loss: 5.25\n",
      "Epoch 8480 | Train loss: 4.35 | Val loss: 5.30\n",
      "Epoch 8490 | Train loss: 4.33 | Val loss: 5.38\n",
      "Epoch 8500 | Train loss: 4.33 | Val loss: 5.46\n",
      "Epoch 8510 | Train loss: 4.38 | Val loss: 5.38\n",
      "Epoch 8520 | Train loss: 4.32 | Val loss: 5.31\n",
      "Epoch 8530 | Train loss: 4.35 | Val loss: 5.31\n",
      "Epoch 8540 | Train loss: 4.34 | Val loss: 5.29\n",
      "Epoch 8550 | Train loss: 4.37 | Val loss: 5.27\n",
      "Epoch 8560 | Train loss: 4.33 | Val loss: 5.32\n",
      "Epoch 8570 | Train loss: 4.27 | Val loss: 5.24\n",
      "Epoch 8580 | Train loss: 4.38 | Val loss: 5.30\n",
      "Epoch 8590 | Train loss: 4.40 | Val loss: 5.26\n",
      "Epoch 8600 | Train loss: 4.31 | Val loss: 5.29\n",
      "Epoch 8610 | Train loss: 4.31 | Val loss: 5.36\n",
      "Epoch 8620 | Train loss: 4.35 | Val loss: 5.35\n",
      "Epoch 8630 | Train loss: 4.36 | Val loss: 5.39\n",
      "Epoch 8640 | Train loss: 4.34 | Val loss: 5.15\n",
      "Epoch 8650 | Train loss: 4.33 | Val loss: 5.18\n",
      "Epoch 8660 | Train loss: 4.37 | Val loss: 5.26\n",
      "Epoch 8670 | Train loss: 4.30 | Val loss: 5.18\n",
      "Epoch 8680 | Train loss: 4.38 | Val loss: 5.21\n",
      "Epoch 8690 | Train loss: 4.37 | Val loss: 5.49\n",
      "Epoch 8700 | Train loss: 4.32 | Val loss: 5.29\n",
      "Epoch 8710 | Train loss: 4.31 | Val loss: 5.10\n",
      "Epoch 8720 | Train loss: 4.41 | Val loss: 5.34\n",
      "Epoch 8730 | Train loss: 4.32 | Val loss: 5.38\n",
      "Epoch 8740 | Train loss: 4.35 | Val loss: 5.19\n",
      "Epoch 8750 | Train loss: 4.31 | Val loss: 5.42\n",
      "Epoch 8760 | Train loss: 4.37 | Val loss: 5.25\n",
      "Epoch 8770 | Train loss: 4.41 | Val loss: 5.31\n",
      "Epoch 8780 | Train loss: 4.35 | Val loss: 5.33\n",
      "Epoch 8790 | Train loss: 4.37 | Val loss: 5.35\n",
      "Epoch 8800 | Train loss: 4.43 | Val loss: 5.26\n",
      "Epoch 8810 | Train loss: 4.34 | Val loss: 5.27\n",
      "Epoch 8820 | Train loss: 4.32 | Val loss: 5.24\n",
      "Epoch 8830 | Train loss: 4.37 | Val loss: 5.27\n",
      "Epoch 8840 | Train loss: 4.39 | Val loss: 5.25\n",
      "Epoch 8850 | Train loss: 4.37 | Val loss: 5.34\n",
      "Epoch 8860 | Train loss: 4.38 | Val loss: 5.37\n",
      "Epoch 8870 | Train loss: 4.31 | Val loss: 5.26\n",
      "Epoch 8880 | Train loss: 4.36 | Val loss: 5.46\n",
      "Epoch 8890 | Train loss: 4.36 | Val loss: 5.15\n",
      "Epoch 8900 | Train loss: 4.38 | Val loss: 5.22\n",
      "Epoch 8910 | Train loss: 4.36 | Val loss: 5.33\n",
      "Epoch 8920 | Train loss: 4.39 | Val loss: 5.29\n",
      "Epoch 8930 | Train loss: 4.34 | Val loss: 5.30\n",
      "Epoch 8940 | Train loss: 4.36 | Val loss: 5.29\n",
      "Epoch 8950 | Train loss: 4.33 | Val loss: 5.27\n",
      "Epoch 8960 | Train loss: 4.36 | Val loss: 5.32\n",
      "Epoch 8970 | Train loss: 4.30 | Val loss: 5.25\n",
      "Epoch 8980 | Train loss: 4.31 | Val loss: 5.20\n",
      "Epoch 8990 | Train loss: 4.33 | Val loss: 5.40\n",
      "Epoch 9000 | Train loss: 4.34 | Val loss: 5.21\n",
      "Epoch 9010 | Train loss: 4.37 | Val loss: 5.24\n",
      "Epoch 9020 | Train loss: 4.35 | Val loss: 5.20\n",
      "Epoch 9030 | Train loss: 4.40 | Val loss: 5.16\n",
      "Epoch 9040 | Train loss: 4.36 | Val loss: 5.22\n",
      "Epoch 9050 | Train loss: 4.32 | Val loss: 5.27\n",
      "Epoch 9060 | Train loss: 4.32 | Val loss: 5.18\n",
      "Epoch 9070 | Train loss: 4.36 | Val loss: 5.28\n",
      "Epoch 9080 | Train loss: 4.38 | Val loss: 5.34\n",
      "Epoch 9090 | Train loss: 4.36 | Val loss: 5.18\n",
      "Epoch 9100 | Train loss: 4.34 | Val loss: 5.18\n",
      "Epoch 9110 | Train loss: 4.36 | Val loss: 5.32\n",
      "Epoch 9120 | Train loss: 4.37 | Val loss: 5.29\n",
      "Epoch 9130 | Train loss: 4.39 | Val loss: 5.25\n",
      "Epoch 9140 | Train loss: 4.38 | Val loss: 5.26\n",
      "Epoch 9150 | Train loss: 4.39 | Val loss: 5.29\n",
      "Epoch 9160 | Train loss: 4.35 | Val loss: 5.14\n",
      "Epoch 9170 | Train loss: 4.33 | Val loss: 5.33\n",
      "Epoch 9180 | Train loss: 4.32 | Val loss: 5.38\n",
      "Epoch 9190 | Train loss: 4.38 | Val loss: 5.24\n",
      "Epoch 9200 | Train loss: 4.34 | Val loss: 5.28\n",
      "Epoch 9210 | Train loss: 4.42 | Val loss: 5.18\n",
      "Epoch 9220 | Train loss: 4.36 | Val loss: 5.30\n",
      "Epoch 9230 | Train loss: 4.34 | Val loss: 5.38\n",
      "Epoch 9240 | Train loss: 4.38 | Val loss: 5.28\n",
      "Epoch 9250 | Train loss: 4.33 | Val loss: 5.29\n",
      "Epoch 9260 | Train loss: 4.34 | Val loss: 5.35\n",
      "Epoch 9270 | Train loss: 4.34 | Val loss: 5.30\n",
      "Epoch 9280 | Train loss: 4.35 | Val loss: 5.22\n",
      "Epoch 9290 | Train loss: 4.36 | Val loss: 5.19\n",
      "Epoch 9300 | Train loss: 4.32 | Val loss: 5.25\n",
      "Epoch 9310 | Train loss: 4.36 | Val loss: 5.29\n",
      "Epoch 9320 | Train loss: 4.33 | Val loss: 5.27\n",
      "Epoch 9330 | Train loss: 4.41 | Val loss: 5.31\n",
      "Epoch 9340 | Train loss: 4.38 | Val loss: 5.36\n",
      "Epoch 9350 | Train loss: 4.33 | Val loss: 5.25\n",
      "Epoch 9360 | Train loss: 4.38 | Val loss: 5.48\n",
      "Epoch 9370 | Train loss: 4.30 | Val loss: 5.23\n",
      "Epoch 9380 | Train loss: 4.36 | Val loss: 5.40\n",
      "Epoch 9390 | Train loss: 4.38 | Val loss: 5.21\n",
      "Epoch 9400 | Train loss: 4.35 | Val loss: 5.25\n",
      "Epoch 9410 | Train loss: 4.37 | Val loss: 5.43\n",
      "Epoch 9420 | Train loss: 4.31 | Val loss: 5.32\n",
      "Epoch 9430 | Train loss: 4.35 | Val loss: 5.22\n",
      "Epoch 9440 | Train loss: 4.38 | Val loss: 5.42\n",
      "Epoch 9450 | Train loss: 4.37 | Val loss: 5.37\n",
      "Epoch 9460 | Train loss: 4.31 | Val loss: 5.44\n",
      "Epoch 9470 | Train loss: 4.33 | Val loss: 5.28\n",
      "Epoch 9480 | Train loss: 4.36 | Val loss: 5.26\n",
      "Epoch 9490 | Train loss: 4.32 | Val loss: 5.35\n",
      "Epoch 9500 | Train loss: 4.32 | Val loss: 5.34\n",
      "Epoch 9510 | Train loss: 4.38 | Val loss: 5.43\n",
      "Epoch 9520 | Train loss: 4.31 | Val loss: 5.19\n",
      "Epoch 9530 | Train loss: 4.31 | Val loss: 5.19\n",
      "Epoch 9540 | Train loss: 4.36 | Val loss: 5.43\n",
      "Epoch 9550 | Train loss: 4.39 | Val loss: 5.44\n",
      "Epoch 9560 | Train loss: 4.31 | Val loss: 5.39\n",
      "Epoch 9570 | Train loss: 4.37 | Val loss: 5.30\n",
      "Epoch 9580 | Train loss: 4.40 | Val loss: 5.34\n",
      "Epoch 9590 | Train loss: 4.38 | Val loss: 5.19\n",
      "Epoch 9600 | Train loss: 4.35 | Val loss: 5.21\n",
      "Epoch 9610 | Train loss: 4.32 | Val loss: 5.29\n",
      "Epoch 9620 | Train loss: 4.34 | Val loss: 5.16\n",
      "Epoch 9630 | Train loss: 4.37 | Val loss: 5.34\n",
      "Epoch 9640 | Train loss: 4.36 | Val loss: 5.28\n",
      "Epoch 9650 | Train loss: 4.36 | Val loss: 5.35\n",
      "Epoch 9660 | Train loss: 4.34 | Val loss: 5.19\n",
      "Epoch 9670 | Train loss: 4.38 | Val loss: 5.27\n",
      "Epoch 9680 | Train loss: 4.32 | Val loss: 5.25\n",
      "Epoch 9690 | Train loss: 4.30 | Val loss: 5.24\n",
      "Epoch 9700 | Train loss: 4.33 | Val loss: 5.14\n",
      "Epoch 9710 | Train loss: 4.42 | Val loss: 5.27\n",
      "Epoch 9720 | Train loss: 4.31 | Val loss: 5.17\n",
      "Epoch 9730 | Train loss: 4.41 | Val loss: 5.30\n",
      "Epoch 9740 | Train loss: 4.33 | Val loss: 5.23\n",
      "Epoch 9750 | Train loss: 4.31 | Val loss: 5.26\n",
      "Epoch 9760 | Train loss: 4.38 | Val loss: 5.21\n",
      "Epoch 9770 | Train loss: 4.36 | Val loss: 5.40\n",
      "Epoch 9780 | Train loss: 4.33 | Val loss: 5.38\n",
      "Epoch 9790 | Train loss: 4.34 | Val loss: 5.22\n",
      "Epoch 9800 | Train loss: 4.35 | Val loss: 5.26\n",
      "Epoch 9810 | Train loss: 4.34 | Val loss: 5.15\n",
      "Epoch 9820 | Train loss: 4.34 | Val loss: 5.40\n",
      "Epoch 9830 | Train loss: 4.30 | Val loss: 5.18\n",
      "Epoch 9840 | Train loss: 4.33 | Val loss: 5.31\n",
      "Epoch 9850 | Train loss: 4.36 | Val loss: 5.17\n",
      "Epoch 9860 | Train loss: 4.36 | Val loss: 5.31\n",
      "Epoch 9870 | Train loss: 4.29 | Val loss: 5.47\n",
      "Epoch 9880 | Train loss: 4.34 | Val loss: 5.17\n",
      "Epoch 9890 | Train loss: 4.39 | Val loss: 5.37\n",
      "Epoch 9900 | Train loss: 4.37 | Val loss: 5.40\n",
      "Epoch 9910 | Train loss: 4.30 | Val loss: 5.27\n",
      "Epoch 9920 | Train loss: 4.33 | Val loss: 5.26\n",
      "Epoch 9930 | Train loss: 4.31 | Val loss: 5.24\n",
      "Epoch 9940 | Train loss: 4.35 | Val loss: 5.28\n",
      "Epoch 9950 | Train loss: 4.40 | Val loss: 5.44\n",
      "Epoch 9960 | Train loss: 4.36 | Val loss: 5.39\n",
      "Epoch 9970 | Train loss: 4.37 | Val loss: 5.21\n",
      "Epoch 9980 | Train loss: 4.37 | Val loss: 5.28\n",
      "Epoch 9990 | Train loss: 4.30 | Val loss: 5.41\n",
      "Best val loss: 5.03\n"
     ]
    }
   ],
   "source": [
    "from lr_scheduler import TrainingScheduler\n",
    "\n",
    "train_val_split_idx = int(len(tokenized_lotr) * 0.8)\n",
    "dataset = LotrDataset(torch.tensor(tokenized_lotr), context_length, length=1)\n",
    "\n",
    "model = model.cuda()\n",
    "lr0 = 1e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr0)\n",
    "nb_epochs = 10000\n",
    "warmup_steps = 100\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, nb_epochs - warmup_steps\n",
    ")\n",
    "scheduler = TrainingScheduler(\n",
    "    optimizer=optimizer, warmup_iteration=warmup_steps, lr0=lr0, scheduler=lr_scheduler\n",
    ")\n",
    "\n",
    "best_val_loss = torch.inf\n",
    "for epoch in range(nb_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(len(dataset)):\n",
    "        random_idx = torch.randint(\n",
    "            low=0, high=train_val_split_idx - context_length, size=(batch_size,)\n",
    "        )\n",
    "        x, targets = dataset[random_idx]\n",
    "        x, targets = x.cuda(), targets.cuda()\n",
    "        batch_output = model(x, targets)\n",
    "        train_loss += batch_output[1].item()\n",
    "        optimizer.zero_grad()\n",
    "        batch_output[1].backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            random_idx = torch.randint(\n",
    "                low=train_val_split_idx + 1,\n",
    "                high=len(tokenized_lotr) - context_length,\n",
    "                size=(batch_size,),\n",
    "            )\n",
    "            x, targets = dataset[random_idx]\n",
    "            x, targets = x.cuda(), targets.cuda()\n",
    "            batch_output = model(x, targets)\n",
    "            val_loss += batch_output[1].item()\n",
    "        val_loss /= len(dataset)\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save(model.state_dict(), \"best.pt\")\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"New best model saved with val loss: {best_val_loss:.2f}\")\n",
    "        if epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch} | Train loss: {train_loss:.2f} | Val loss: {val_loss:.2f}\"\n",
    "            )\n",
    "    scheduler.step(epoch)\n",
    "print(f\"Best val loss: {best_val_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G√©n√©rer du Tolkien avec TolkienGPT ‚úçÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici la phrase g√©n√©r√©e : \n",
      " the hobbits stood now on the brink of a tall cliff , bare and bleak , its feet wrapped in mist ; and behind them rose the broken highlands crowned with drifting cloud . a chill wind blew from the east . there was a dark gate , and the water was still , and the wind was still in the mountains . the moon was still silent , but it was heavy , and the black sun was not very further . the trees were still in the wind , and they were gone . the mountains were clad in the gate , and the mountains were great and level . the sun was now in the path , and the sun was descended into the stream . the dark shadow was filled with the east . the moon was still and the light of the river , and the great river grew in the forest . the hobbits had found that the great river was still in the\n"
     ]
    }
   ],
   "source": [
    "model = TolkienGPT(len(tokenizer.unique_words), embeddind_dim, context_length, nb_blocks=nb_blocks, nb_heads=nb_heads)\n",
    "model.load_state_dict(torch.load(\"best.pt\"))\n",
    "model.cuda()\n",
    "model.eval()\n",
    "phrase = \"the hobbits stood now on the brink of a tall cliff, bare and bleak, its feet wrapped in mist; and behind them rose the broken highlands crowned with drifting cloud. a chill wind blew from the east.\"\n",
    "tokenized_phrase = torch.tensor(tokenizer.tokenize_a_sentence(phrase)).view(1, -1)\n",
    "output = model.generate(tokenized_phrase.cuda(), length=128)\n",
    "print(f\"Voici la phrase g√©n√©r√©e : \\n {' '.join([tokenizer.idx2word[token] for token in output.tolist()[0]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus ü§©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: \n",
      " torch.Size([1, 1, 13451]) (batch_size, context_length, embedding_dim)\n",
      "Temps moyen de g√©n√©ration d'une phrase : 0.0070 secondes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tokenized_phrases = torch.stack([tokenized_phrase.view(-1) for _ in range(1)])\n",
    "\n",
    "times = []\n",
    "for i in range(100):\n",
    "    start = time.perf_counter()\n",
    "    output = model(tokenized_phrases.cuda(), None)\n",
    "    if i == 0:\n",
    "        print(f\"Output size: \\n {output[0].shape} (batch_size, context_length, embedding_dim)\")\n",
    "    end = time.perf_counter()\n",
    "    times.append(end-start)\n",
    "    \n",
    "times = torch.tensor(times)\n",
    "    \n",
    "print(f\"Temps moyen de g√©n√©ration d'une phrase : {times[1:].mean():.4f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 19, 768])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\"gandalf\", \"aragorn\", \"frodo\", \"sam\", \"sauron\", \"bilbo\", \"legolas\", \"gimli\", \"saruman\", \"pippin\", \"merry\", \"boromir\", \"faramir\", \"gollum\", \"elrond\", \"galadriel\", \"denethor\", \"eomer\", \"eowyn\"]\n",
    "# labels = [\"gandalf\", \"sauron\"]\n",
    "tokenized_labels = torch.tensor(tokenizer.tokenize_a_sentence(\" \".join(labels))).view(1, -1)\n",
    "model = model.cpu()\n",
    "embeddings = model.embedding_table(tokenized_labels)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(embeddings.squeeze().detach().numpy())\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "for i, label in enumerate(labels):\n",
    "    plt.scatter(X_pca[i, 0], X_pca[i, 1], label=label)\n",
    "    plt.text(X_pca[i, 0], X_pca[i, 1], label)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
